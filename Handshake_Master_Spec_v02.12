          .%:                                                                         .#@-          
        .:@@@@%:..                                                                ..#@@@@@+.        
       .:@@@@@@@@@-.                                                         ...:%@@@@@@@@@*..      
      .+@@@@@@@@@@@=....             ........       ...:-=++=-:....        ..=@::@@@@@@@@@@@#.      
     .*@@@@@@@@@@@:.=@@@@+:....:-+%@@@@@@@@@@@@=..:%@@@@@@@@@@@@@@@@@%##%@@@@@@@-.%@@@@@@@@@@@.     
   ..#@@@@@@@@@@@:.#@@@@@@@@@@@@@@@@@@@@@@@@#:.:*@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@-.#@@@@@@@@@@@:.   
  ..%@@@@@@@@@@%..%@@@@@@@@@@@@@@@@@@@@@@%-..=@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@+.*@@@@@@@@@@@:.  
 .:@@@@@@@@@@@# .%@@@@@@@@@@@@@@@@@@@@@= .=@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#.=@@@@@@@@@@@=. 
.:@@@@@@@@@@@#.:@@@@@@@@@@@@@@@@@@@@*..-%@@@@@@@@@@@@@%-...=@@@@@@@@@@@@@@@@@@@@@@@@#.-@@@@@@@@@@@+.
-@@@@@@@@@@@+.-@@@@@@@@@@@@@@@@@@@* .#@@@@@@@@@@@@@*...=@@%:.=@@@@@@@@@@@@@@@@@@@@@@@%.-@@@@@@@@@@@*
.*@@@@@@@@@-.-@@@@@@@@@@@@@@@@@@@@.:@@@@@@@@@@@%=..-#@@@@@@@%..=@@@@@@@@@@@@@@@@@@@@@@@:.%@@@@@@@#:.
  ..*@@@@@-.+@@@@@@@@@@@@@@@@@@@@@.:@@@@@@@@*...=@@@@@@@@@@@@@#. =@@@@@@@@@@@@@@@@@@@@@@:.#@@@#:.   
     ..+@:.*@@@@@@@@@@@@@@@@@@@@@@*..*@@#=..-*@@@@@@@@@@@@@@@@@@#..+@@@@@@@@@@@@@@@@@@@@@-.+:.      
          +@@@@@@@@@@@@@@@@@@@@@@@@@+-..:=%@@@@@@@@@@@@@@@@@@@@@@@#..+@@@@@@@@@@@@@@@@@@@=          
          ..%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#..+@@@@@@@@@@@@@@@+.           
            .:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@*..+@@@@@@@@@@@#.             
             ..=@@@*++*@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@*..+@@@@@@@%:               
                ...-++-.:%@@@%++#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@*..*@@@@-.                
                .+@@@@@@.:@-..=+:.:%@@@@@@@@@@@@@@@@@@@@@@@@@@@@%..+@@@@@@@@*..*=..                 
               .%@@@@@@@:..-%@@@@@.:@%++*@@@@@@@@@@@@@@@@@@@@@@@@@#..=@@@@@@@@*.                    
               .*@@@@@+..=@@@@@@@@:...=+:.:%@@@@@@@@@@@@@-.:%@@@@@@@#..=@@@@@@@@=.                  
                .:==-..=@@@@@@@@#:.:%@@@@@:.%@@@@@@@@@@@@@@-.:%@@@@@@@#..=@@@@@@%.                  
                      .@@@@@@@*..=@@@@@@@@-.%@@@@*..#@@@@@@@@:.:%@@@@@@@*..=@@@#..                  
                      ..%@@@-..*@@@@@@@@%:.+@@@@@@@+..#@@@@@@@%-.:@@@@@@@. .....                    
                         ....#@@@@@@@@*.....=@@@@@@@@+..#@@@@@@@@:.:%@@@-.                          
                           .-@@@@@@@-..*@@@%..=@@@@@@@@*..*@@@@@@*.......                           
                           ..#@@@@:..#@@@@@@* ..=@@@@@@@@=..*@@@*..                                 
                            ....... :@@@@@@@:  ...=@@@@@@%. .....                                   
                                    .+@@@@:.       .=@@@@:.                                         







# Project Handshake â€“ Master Specification

**Status:** Merged Specification (Diary Governance + Infrastructure)  
**Version:** v02.12  
**Date:** 2025-12-04  
**Purpose:** Complete reference combining product vision, Diary governance (Parts 1-2 runtime, Part 5 content integrity, Part 6 validation gates, Part 7 response behavior, Part 10 file integrity, Part 12 pre-commit gate), extraction pipeline (DES-001, IMG-001, SYM-001), mechanical integrations (Docling, ASR), COR-701 deterministic edit process, LOG-001 session logging, and AI job model.

---

## Table of Contents

- [1 Vision & Context](#1-vision-context)
  - [1.1 Executive Summary](#11-executive-summary)
  - [1.2 The Diary Origin Story](#12-the-diary-origin-story)
  - [1.3 The Four-Layer Architecture](#13-the-four-layer-architecture)
  - [1.4 LLM Reliability Hierarchy](#14-llm-reliability-hierarchy)
  - [1.5 What Gets Ported from the Diary](#15-what-gets-ported-from-the-diary)
  - [1.6 Design Philosophy: Self-Enforcing Governance](#16-design-philosophy-self-enforcing-governance)
  - [1.7 Success Criteria](#17-success-criteria)
  - [1.8 Introduction](#18-introduction)
- [2 System Architecture](#2-system-architecture)
  - [2.1 High-Level Architecture](#21-high-level-architecture)
  - [2.2 Data & Content Model](#22-data-content-model)
  - [2.3 Content Integrity (Diary Part 5: COR-700)](#23-content-integrity-diary-part-5-cor-700)
  - [2.4 Extraction Pipeline (The Product)](#24-extraction-pipeline-the-product)
  - [2.5 AI Interaction Patterns](#25-ai-interaction-patterns)
  - [2.6 Workflow & Automation Engine](#26-workflow-automation-engine)
  - [2.7 Response Behavior Contract (Diary ANS-001)](#27-response-behavior-contract-diary-ans-001)
  - [2.8 Governance Runtime (Diary Parts 1-2)](#28-governance-runtime-diary-parts-1-2)
  - [2.9 Deterministic Edit Process (COR-701)](#29-deterministic-edit-process-cor-701)
  - [2.10 Session Logging (LOG-001)](#210-session-logging-log-001)
- [3 Local-First Infrastructure](#3-local-first-infrastructure)
  - [3.1 Local-First Data Fundamentals](#31-local-first-data-fundamentals)
  - [3.2 CRDT Libraries Comparison](#32-crdt-libraries-comparison)
  - [3.3 Database & Sync Patterns](#33-database-sync-patterns)
  - [3.4 Conflict Resolution UX](#34-conflict-resolution-ux)
- [4 LLM Infrastructure](#4-llm-infrastructure)
  - [4.1 LLM Infrastructure](#41-llm-infrastructure)
  - [4.2 LLM Inference Runtimes](#42-llm-inference-runtimes)
  - [4.3 Model Selection & Roles](#43-model-selection-roles)
  - [4.4 Image Generation (Stable Diffusion)](#44-image-generation-stable-diffusion)
- [5 Security & Observability](#5-security-observability)
  - [5.1 Plugin Architecture](#51-plugin-architecture)
  - [5.2 Sandboxing & Security](#52-sandboxing-security)
  - [5.3 AI Observability](#53-ai-observability)
  - [5.4 Evaluation & Quality](#54-evaluation-quality)
    - [5.4.6 Governance Compliance Tests](#546-governance-compliance-tests)
  - [5.5 Benchmark Harness](#55-benchmark-harness)
- [6 Mechanical Integrations](#6-mechanical-integrations)
  - [6.1 Document Ingestion: Docling Subsystem](#61-document-ingestion-docling-subsystem)
  - [6.2 Speech Recognition: ASR Subsystem](#62-speech-recognition-asr-subsystem)
- [7 User Experience & Development](#7-user-experience-development)
  - [7.1 User Interface Components](#71-user-interface-components)
  - [7.2 Multi-Agent Orchestration](#72-multi-agent-orchestration)
  - [7.3 Collaboration and Sync](#73-collaboration-and-sync)
  - [7.4 Reference Application Analysis](#74-reference-application-analysis)
  - [7.5 Development Workflow](#75-development-workflow)
  - [7.6 Development Roadmap](#76-development-roadmap)
- [8 Reference](#8-reference)
  - [8.1 Risk Assessment](#81-risk-assessment)
  - [8.2 Technology Stack Summary](#82-technology-stack-summary)
  - [8.3 Gap Analysis & Open Questions](#83-gap-analysis-open-questions)
  - [8.4 Consolidated Glossary](#84-consolidated-glossary)
  - [8.5 Sources Referenced](#85-sources-referenced)
  - [8.6 Appendices](#86-appendices)

---


---

# 1. Vision & Context

## 1.1 Executive Summary

**Why**  
Provides high-level orientation for readers new to the specification. Establishes context before technical details.

**What**  
Quick-start overview of Project Handshake: what it is, who it's for, and how this document evolved from both infrastructure research AND three months of AI governance R&D (the Prompt Diaries project).

**Jargon**  
- **Local-first**: Data lives on your device; cloud is optional backup/sync.
- **AI-native**: AI integrated from inception, not bolted on.
- **Workspace**: Unified environment combining docs, canvases, tables.
- **Diary**: The Prompt Diaries project â€” 3 months of governance R&D that Handshake now implements.
- **RID**: Rule Identifier â€” a numbered, machine-checkable governance clause.

---

#### 1.1.1 TL;DR Box

> **Project Handshake** is a desktop application combining:
> - **Notion-like** document editing with databases
> - **Milanote-like** visual canvas/moodboards  
> - **Excel-like** spreadsheets with formulas
> - **Local AI models** for writing, coding, and image generation
> - **Descriptor extraction** for tracking taste and building searchable creative references
> 
> **Tech Stack Decision:** Tauri + React + TypeScript (frontend) + Python (AI backend)
> 
> **Key Insight:** Run AI models locally for privacy, speed, and cost savingsâ€”with cloud fallback when needed.
>
> **Governance Origin:** The Prompt Diaries project spent 3 months building ~1,232 governance clauses to make LLMs reliable. Handshake implements this governance in code.

---

#### 1.1.2 What We're Building

**Project Handshake is a "local AI cloud" on your desktop.** Instead of sending your documents, ideas, and data to cloud services like Notion or Google Docs, everything stays on your computer. AI assistants run locally too, meaning your sensitive information never leaves your machine.

The application combines three types of tools that creative professionals typically use separately:

| Tool Type | Inspiration | Use Case |
|-----------|-------------|----------|
| **Rich Documents** | Notion | Writing, planning, structured databases |
| **Visual Canvas** | Milanote | Mood boards, brainstorming, spatial organization |
| **Spreadsheets** | Excel | Data manipulation, calculations, analysis |

**What makes this different:** Local AI models collaborate to help you. One AI might plan your project, another writes the code, and a third generates imagesâ€”all coordinated automatically.

**The hidden layer:** A comprehensive governance system (ported from the Diary) ensures AI behavior is reliable, deterministic, and auditable.

---

#### 1.1.3 Key Architecture Decisions (From Research)

Based on extensive research across multiple documents, the following decisions have been validated:

| Decision | Choice | Why |
|----------|--------|-----|
| Desktop Shell | **Tauri** (not Electron) | 90% less memory usage; critical when running AI models |
| Frontend | **React + TypeScript** | Rich ecosystem, same code works in both shells |
| Backend | **Python** | Best AI/ML library support, orchestration frameworks |
| AI Orchestration | **AutoGen or LangGraph** | Mature multi-agent coordination |
| Data Sync | **CRDTs (Yjs)** | Offline-first, conflict-free collaboration |
| Storage | **File-tree based** | Human-readable, portable, git-friendly |
| Governance | **Code-enforced (from Diary)** | LLMs can't violate what code prevents |

---

#### 1.1.4 Why Local-First Matters

ğŸ“Œ **Key Point:** The entire architecture is designed around "local-first" principles:

1. **Privacy:** Your documents and AI conversations never leave your computer
2. **Speed:** No network latency for AI responses
3. **Cost:** After initial model download, AI usage is essentially free
4. **Reliability:** Works without internet, on airplanes, in poor connectivity
5. **Control:** You own your data in standard file formats

---

#### 1.1.5 Hardware Context

The target hardware for development and initial deployment:

| Component | Specification | Why It Matters |
|-----------|--------------|----------------|
| CPU | Ryzen 9 5950X (16 cores) | Handles multiple processes, CPU inference fallback |
| RAM | 128 GB | Multiple AI models can stay loaded in memory |
| GPU | RTX 3090 (24GB VRAM) | Runs large AI models, image generation |
| Storage | NVMe SSD | Fast model loading, responsive file operations |

âš ï¸ **Warning:** This hardware is above average. The app design must handle graceful degradation for users with less powerful systems, including cloud fallback options.

---

**Key Takeaways**
- Handshake is a local-first, AI-native desktop workspace
- Combines Notion-style docs, Miro-style canvases, and Excel-style tables
- Designed for power users with high-end hardware (RTX 3090, 128GB RAM)
- **Includes governance layer from Diary** â€” 3 months of R&D on making AI reliable
- This specification covers product vision, governance implementation, and mechanical integrations

---

## 1.2 The Diary Origin Story

**Why**  
Understanding where Handshake's governance comes from explains why it's built the way it is. The Diary was 3 months of R&D that discovered what it actually takes to make AI reliable.

**What**  
This section explains the creative goal that started everything, the problems LLMs caused, the governance solution that emerged, and how Handshake transforms that into code.

**Jargon**  
- **Diary / Prompt Diaries**: The R&D project that preceded Handshake. A plain-text governance system.
- **RID**: Rule Identifier â€” a numbered governance rule (e.g., DES-001, COR-701).
- **Clause**: A single, machine-checkable requirement within a RID.
- **Descriptor**: A structured record describing an image or creative reference.
- **CORPUS**: The accumulated collection of descriptors.
- **CONFIG**: Vocabulary and profile definitions that govern extraction.

---

### 1.2.1 The Goal

The Prompt Diaries project started with a creative goal:
- **Track taste** â€” build a personal aesthetic vocabulary
- **Describe images** â€” extract structured descriptors from visual content  
- **Build a corpus** â€” accumulate tagged, searchable creative references

This is what DES-001 (Descriptor Extraction), IMG-001 (Image Analysis), and SYM-001 (Symbolic Layers) are for. **These three RIDs are the actual product.**

---

### 1.2.2 The Problem

LLMs couldn't reliably do this work because:
- They **drift** â€” forget rules mid-conversation
- They **can't edit reliably** â€” surgical changes corrupt surrounding content
- They **don't know where they are** â€” lose track of document position
- They **guess** â€” fabricate content when uncertain instead of stopping

Every attempt to extract descriptors resulted in:
- Schema violations
- Content in wrong locations
- Silent modifications to existing data
- Inconsistent output formats

---

### 1.2.3 The Solution (That Became Its Own Project)

To make LLMs reliable, the project built a comprehensive governance system:
- **RIDs** â€” Rules with machine-checkable clauses
- **Layers** â€” L1 (immutable), L2 (promotion-only), L3 (writable)
- **Gates** â€” Validation checkpoints before any operation
- **Modes** â€” Explicit work contexts with different permissions
- **Lint rules** â€” Automated compliance checking
- **Answer governance** â€” Structured output formats

This governance layer grew to **~1,232 clauses across 14 RIDs** plus Bootloader and Execution Charter.

The governance infrastructure became so comprehensive that it overshadowed the creative extraction core it was built to enable. The Diary became known for its rules, not its purpose.

---

### 1.2.4 What Handshake Changes

Handshake moves enforcement from **rules in context** (unreliable) to **code enforcement** (reliable):

```
DIARY (Before):
  Rules live in text â†’ LLM reads them â†’ LLM may or may not follow them
  
HANDSHAKE (After):
  Rules become code â†’ Code enforces them â†’ LLM literally cannot violate them
```

**The Diary was R&D. Handshake is the product.**

---

**Key Takeaways**
- The real product is **descriptor extraction** (DES-001, IMG-001, SYM-001)
- Governance exists because LLMs couldn't do extraction reliably
- ~1,232 clauses were needed to make LLMs behave
- Handshake implements these clauses in code, not text
- Code enforcement is at the top of the reliability hierarchy; rules-in-context is near the bottom

---

## 1.3 The Four-Layer Architecture

**Why**  
Understanding the layers helps you know where each piece of functionality lives. When something goes wrong, you know which layer to debug.

**What**  
Handshake has four layers: LLM (decides what), Orchestrator (enforces rules), Mechanical (executes deterministically), and Validation (confirms correctness).

**Jargon**  
- **LLM Layer**: The AI model that reasons about what to do.
- **Orchestrator Layer**: The code that translates AI intent into safe operations.
- **Mechanical Layer**: Deterministic engines (Word, Excel, Docling) that execute operations.
- **Validation Layer**: Checks that confirm output matches expectations.

---

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM LAYER                                                      â”‚
â”‚  Decides WHAT to change                                         â”‚
â”‚  Outputs: structured instruction (not raw text)                 â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ORCHESTRATOR LAYER                                             â”‚
â”‚  Translates instruction â†’ API calls                             â”‚
â”‚  Enforces capability constraints                                â”‚
â”‚  Loads relevant rules (not all 1,232)                           â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MECHANICAL LAYER                                               â”‚
â”‚  Descriptor extraction / Document editing engine                â”‚
â”‚  Executes deterministically                                     â”‚
â”‚  LLM never touches data directly                                â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  VALIDATION LAYER                                               â”‚
â”‚  SHA: did input become expected output?                         â”‚
â”‚  Lint: did instruction make sense?                              â”‚
â”‚  Diff: is change within allowed scope?                          â”‚
â”‚  Failure is visible and recoverable                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key principle:** LLM steers, software executes, code validates.

---

### 1.3.1 How the Layers Work Together

1. **User request** arrives (e.g., "extract descriptors from this image")
2. **LLM Layer** reasons about the task, outputs structured instruction
3. **Orchestrator Layer** checks: Is this operation permitted? Which RIDs govern it?
4. **Mechanical Layer** executes deterministically (IMG-001 pipeline extracts descriptors)
5. **Validation Layer** confirms: Schema valid? Gates passed? SHA matches?
6. **Result** returns to user (or error with recovery path)

The LLM **never directly touches** document content or descriptor data. It only emits instructions that the mechanical layer executes.

---

**Key Takeaways**
- Four layers: LLM â†’ Orchestrator â†’ Mechanical â†’ Validation
- LLM decides WHAT, never HOW
- Mechanical layer is deterministic (same input â†’ same output)
- Validation catches failures before they reach the user
- This architecture makes AI behavior auditable and recoverable

---

## 1.4 LLM Reliability Hierarchy

**Why**  
This hierarchy explains why some AI behaviors are trustworthy and others aren't. It guides every design decision: push enforcement UP the hierarchy.

**What**  
A ranking from most reliable (code enforcement) to least reliable (hoping the model remembers). Handshake operates at the top; the Diary operated near the bottom.

**Jargon**  
- **Code enforcement**: Rules the LLM literally cannot violate (compile-time, type system).
- **Structured output**: JSON schema, grammar constraints that force valid format.
- **Verbatim markers**: Explicit tags that mechanical code processes (not the LLM).
- **Rules in context**: Instructions the LLM can read but may ignore.

---

```
MOST RELIABLE
     â”‚
     â”‚  1. Code enforcement (literally cannot violate)
     â”‚        â†’ Rust type system, compile-time checks
     â”‚        â†’ HANDSHAKE OPERATES HERE
     â”‚
     â”‚  2. Verbatim markers + mechanical execution
     â”‚        â†’ "Write descriptor to block X" (code does the writing)
     â”‚
     â”‚  3. Structured output + validation
     â”‚        â†’ JSON schema, grammar constraints, SHA verification
     â”‚
     â”‚  4. Explicit state passed every prompt
     â”‚        â†’ State machine, not memory
     â”‚
     â”‚  5. Rules in context
     â”‚        â†’ May be read, may not be applied
     â”‚        â†’ DIARY OPERATED HERE
     â”‚
     â”‚  6. Rules the model "should remember"
     â”‚        â†’ Will drift, will guess, will fail
     â”‚
LEAST RELIABLE
```

---

### 1.4.1 Why This Matters

The Diary spent 3 months writing ~1,232 clauses. These were **rules in context** (level 5). The LLM could read them but might not apply them. Every drift required more rules, which increased context load, which caused more drift.

Handshake breaks this cycle by implementing rules as **code** (level 1). The LLM can't violate a constraint that's enforced by the type system.

| Level | Diary Approach | Handshake Approach |
|-------|----------------|-------------------|
| 1 | â€” | Rust types, `LayerGuard`, immutable references |
| 2 | â€” | Mechanical pipelines (IMG-001, Docling) |
| 3 | â€” | JSON schema validation, Gate trait |
| 4 | Partial (state in prompts) | `StateSnapshot` in every request |
| 5 | **Primary** (RIDs in context) | Fallback only |
| 6 | Sometimes | Never |

---

**Key Takeaways**
- Design for the top of the hierarchy, never the bottom
- Code enforcement > structured output > rules in context
- The Diary's rules were level 5; Handshake implements them at level 1-3
- This is why Handshake will work where the Diary struggled

---

## 1.5 What Gets Ported from the Diary

**Why**  
Not everything from the Diary becomes Handshake code. Understanding the categories helps you know what to implement, what to configure, and what to skip.

**What**  
The ~1,232 Diary clauses fall into four categories: PORTED (becomes Rust types), TRANSFORMED (rules become code), PRESERVED (extraction core), and DEPRECATED (text-format specifics not needed).

**Jargon**  
- **PORTED**: Diary concepts that become Rust structs/enums directly.
- **TRANSFORMED**: Rules that were text now become code enforcement.
- **PRESERVED**: The extraction pipeline (the actual product).
- **DEPRECATED**: Text-format rules that Rust types make unnecessary.

---

### 1.5.1 PORTED: Concepts Become Rust Types

| Diary Concept | Handshake Implementation |
|---------------|--------------------------|
| Layers (L1/L2/L3) | `Layer` enum + `LayerGuard` |
| Work Modes | `WorkMode` enum + mode state machine |
| Gates (COR-701) | `Gate` trait + 11 implementations |
| PlannedOperation | `PlannedOperation` struct |
| DescriptorRow | `DescriptorRow` struct |
| SHOT_DNA | `ShotDna` struct with field enums |
| Flight Recorder | `FlightRecorder` append-only log |

---

### 1.5.2 TRANSFORMED: Rules Become Code Enforcement

| Diary Enforcement | Handshake Enforcement |
|-------------------|----------------------|
| "L1 is immutable" (text rule) | `&L1Content` (no `&mut`, compile-time) |
| "Must pass 11 gates" (text rule) | `GatePipeline::validate()` (runtime check) |
| "Use CONFIG vocab only" (text rule) | `Vocab::validate(value)` (type-checked) |
| RID lint rules | `ValidatorConfig` patterns |

---

### 1.5.3 PRESERVED: The Extraction Core (The Product)

| Extraction RID | What It Does | Status |
|----------------|--------------|--------|
| DES-001 | Descriptor schema + extraction rules | **Core product** |
| IMG-001 | Image â†’ Descriptor pipeline | **Core product** |
| SYM-001 | SHOT_DNA â†’ Layer scores | **Core product** |

These are not governance overhead. **These are the point.**

---

### 1.5.4 DEPRECATED: Text-Format Specifics

| Diary Feature | Why Not Needed in Handshake |
|---------------|----------------------------|
| Rail patterns (`====`) | Rust structs replace text delimiters |
| Topic markers (`[[SUB:X]]`) | Struct fields replace markers |
| File naming conventions | Handshake manages its own storage |
| Text lint patterns | Type system prevents invalid states |

---

**Key Takeaways**
- ~400 clauses become Rust code (PORTED + TRANSFORMED)
- ~200 clauses become validator configs
- ~300 clauses are reference documentation
- ~180 clauses are deferred for post-MVP
- The extraction core (DES-001, IMG-001, SYM-001) is the actual product

---

## 1.6 Design Philosophy: Self-Enforcing Governance

**Why**  
Understanding why the Diary embeds its own enforcement explains a key principle Handshake must preserve: rules and their validators must live together.

**What**  
Traditional document governance fails because rules and enforcement are separate. The Diary embeds lint rules and machine code alongside the RIDs they enforce. Handshake must preserve this pattern.

**Jargon**  
- **Self-governance loop**: Rules, validators, and helpers all version together.
- **Embedded enforcement**: Lint rules live in the same document as the rules they check.
- **Provenance**: The trail from clause ID to code to test.

---

### 1.6.1 The Problem: Governance Drift

Traditional document governance fails because rules and enforcement are separate:

```
Human writes rule â†’ Human remembers to follow it â†’ Human checks own work â†’ Drift happens
```

Over time:
- Rules get forgotten or misremembered
- External linters drift from rule intent
- Scripts produce non-compliant output
- Nobody notices until it's too late

---

### 1.6.2 The Solution: Embedded Enforcement

The Diary embeds its own immune system. Rules, validators, and automation live together:

```
Human writes RID â†’ Lint checks compliance â†’ Machine code automates â†’ Consistency enforced
```

This creates a **self-governance loop**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DIARY                          â”‚
â”‚                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     governs      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   RIDs   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Helpers    â”‚   â”‚
â”‚   â”‚  (LAW)   â”‚                  â”‚(MACHINE_CODE)â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â”‚                               â”‚           â”‚
â”‚        â”‚ defines                       â”‚ checked   â”‚
â”‚        â–¼                               â–¼           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     enforces     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Lint   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Output     â”‚   â”‚
â”‚   â”‚  Rules   â”‚                  â”‚  (CORPUS)    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1.6.3 What This Means for Handshake

Handshake must preserve the self-governance property:

| Diary Component | Handshake Equivalent | Preservation Requirement |
|-----------------|---------------------|--------------------------|
| RIDs (LAW) | Clause-attributed code | `#[clause("ID", "desc")]` links code to law |
| Lint rules | `ValidatorConfig` | Patterns loaded from same source as rules |
| Helpers | Service implementations | Services carry clause provenance |
| CORPUS | Output artifacts | All output validated by same validators |

**Key principle:** In Handshake, a clause should never exist without its validator, and a validator should never exist without its clause. They are born together, live together, and die together.

---

### 1.6.4 Clause Provenance Pattern

Every implemented clause uses the provenance attribute:

```rust
#[clause("BL-270", "Treat change as PATCH not rewrite")]
pub struct PlannedOperation { ... }
```

For validator configs, use comments:

```rust
// FMT-140-F140-23: Rail pattern must match exactly
pub const RAIL_PATTERN: &str = r"^={4,}.*={4,}$";
```

For deferred clauses, use TODO:

```rust
// TODO(IMG001-180): Implement InputCollection stage
// Deferred: Image analysis is complex; using stub
pub fn input_collection_stub() -> Vec<ImageSource> { vec![] }
```

This enables:
- Grep for clause ID finds all related code
- Clause changes â†’ easy to find what needs updating
- Audit trail from law to implementation

---

**Key Takeaways**
- Rules and enforcement must live together
- The Diary's self-governance loop must be preserved in Handshake
- Every clause gets a `#[clause("ID", "desc")]` attribute
- Grep for clause ID finds all related code, tests, validators
- This is how you maintain 1,232 clauses without drift

---

## 1.7 Success Criteria

**Why**  
Clear success criteria tell you when the implementation is working. Without these, you can't know if you're done.

**What**  
Six checkpoints that define a working Handshake implementation.

---

The implementation works when:

1. **LLM outputs structured instruction** â€” not raw text edits
2. **Orchestrator validates against capability profile** â€” derived from RIDs
3. **Mechanical layer executes deterministically** â€” DES-001/IMG-001/SYM-001
4. **Validator confirms output** â€” gates pass, schema valid
5. **On failure: visible error, recoverable state** â€” Flight Recorder tracks everything
6. **On success: provenance tracked** â€” clause IDs in code, audit trail complete

**The LLM never directly touches document/descriptor content.**

---

**Key Takeaways**
- Success = all six criteria pass
- The LLM steering / software executing pattern is non-negotiable
- If you can't recover from a failure, the implementation is incomplete

---

## 1.8 Introduction

**Why**  
This section establishes the foundational identity, target users, and design philosophy of Handshake. Without this grounding, subsequent technical decisions lack context and rationale.

**What**  
Defines Handshake as a local-first, AI-native desktop workspace that unifies document editing, visual canvases, and spreadsheets. Documents the specification's evolution and clarifies its relationship to the underlying infrastructure research.

**Jargon**  
- **Local-first**: Data lives primarily on the user's device; cloud sync is optional and never required.
- **AI-native**: AI models and agents are integrated into the core data model and workflows from inception, not bolted on later.
- **Raw/Derived/Display**: Three-layer content separation where user content (Raw) is never silently modified, AI output (Derived) is regenerable, and UI rendering (Display) applies policy/formatting.
- **Desktop-first**: Initial target is a powerful workstation, with laptop and mobile coming later.

---

### 1.8.1 Product Vision & Guiding Principles

#### 1.8.1.1 What Handshake Is

Handshake is a **local-first, AI-enhanced desktop workspace** that unifies three major modes of work:

- **Notion-style docs and databases**
- **Milanote / Miro / tldraw-style visual canvases and moodboards**
- **Excel-style tables, formulas, and data manipulation**

All of these views sit on top of a **single local workspace data graph** backed by a robust data layer. The app is:

- **Desktop-first**, initially targeting a powerful workstation (Ryzen + 128GB RAM + RTX-class GPU), with later paths to laptop and eventually mobile.
- **Local-first**, offline-capable by default, with optional sync and small-team collaboration later.
- **AI-native**, not AI-bolted-on â€“ models and agents are integrated into the data model, workflows, and UX from day one.

#### 1.8.1.2 Target Users

**Primary:**
- A single power user (you) running heavy local models, building workflows, and using the app as a personal production studio, research hub, and coding assistant.

**Longer-term:**
- Small creative / technical teams who need a private, sovereign workspace with powerful AI but without SaaS lock-in or cloud dependence.

#### 1.8.1.3 Guiding Principles

1. **Local-first, truly sovereign**
   - Data lives on the user's machine first. Sync is optional, encrypted, and never assumed.
   - Cloud models are **optional helpers**, not hard dependencies.

2. **Raw / Derived / Display separation**
   - **RawContent** is user-authored content and canonical external inputs. It is never silently changed by AI or filters.
   - **DerivedContent** is AI-generated metadata, summaries, plans, embeddings, layouts, taste descriptors, etc.
   - **DisplayContent** is what the UI shows and what gets exported, including safety filtering and formatting.
   - Censorship and policy enforcement apply **only at Display/Export**, never to Raw/Derived.

3. **AI as collaborator, not overlord**
   - AI is treated as a **co-editor/agent** with its own identity in the data/sync layer, not as magical hidden automation.
   - Every AI action is inspectable, revertible, and attributed.

4. **Composable, inspectable workflows**
   - Automations and agents operate through explicit, typed workflows, not opaque monolithic "magic" buttons.
   - Users can see, edit, disable, or delete anything the system automates for them.

5. **Safety through architecture, not just prompts**
   - Capability-limited tools, sandboxing, durable logs, and typed operations are the main safety tools.
   - Prompts and policy text are layered on top of a secure foundation, not a replacement for it.

6. **Progressive complexity**
   - MVP focuses on single-user workflows and a small set of high-value AI capabilities.
   - More complex multi-agent orchestration, collaboration, and marketplaces come later, on top of a stable core.

---

### 1.8.2 Specification Evolution

This document integrates multiple research sources:

- **GPT-4o Handshake research paper (v1.0)** â€” Original architecture and vision
- **Gemini COMBO synthesis** â€” Shadow Workspace, graph/relational data stack, taste engine implementation, capability tokens, Flight Recorder patterns
- **Claude Opus 4.5 research** â€” AI interaction patterns, workflow safety model, RAG/indexing patterns, doc/canvas behaviors, dev-tools and terminal/agent safety
- **Docs & Sheets AI Integration Protocol** â€” AI jobs over documents and tables with stable IDs and provenance
- **Prompt Diaries governance (v02.00)** â€” ~1,232 clauses of governance R&D, extraction pipeline, validation gates

**Major additions in this version:**

| Area | What Was Added |
|------|----------------|
| **Data & Indexing** | Shadow Workspace with incremental parsing, graph-relational knowledge graph, hybrid retrieval |
| **Collaboration** | CRDTs (Yjs) as core Humanâ€“AI concurrency fabric, AI as first-class CRDT site ID |
| **Implementation** | Rust coordinator + Tauri + React desktop shell, Model Runtime Layer, embedded local stores |
| **Security** | WASI-style capability model, capability contracts, scoped tokens |
| **Observability** | Flight Recorder for full trace logging and replay |
| **AI UX** | Command palette, structural editor, background agent patterns tied to Raw/Derived/Display |
| **Taste Engine** | CLIP embeddings, authorial LoRA adapters, JSON taste descriptors, DPO-style learning |
| **Workflows** | Typed node set, strong validation pipeline, durable local execution |
| **Governance** | Diary RIDs ported to code â€” layers, gates, modes, extraction pipeline |

---

### 1.8.3 Relationship to Base Research

This document defines **behaviours, UX patterns, and architectural constraints** for Handshake.
All underlying infrastructure choices for:

- Storage and sync (file-tree, CRDTs, databases)
- Inference runtimes and model hosting
- Plugin / extension patterns and sandboxing
- Observability and benchmarking

...are inherited from the broader research document `Project_Handshake_Research_merged_v2`.

Where this spec talks about graphs, runtimes, logging, or workflows, it is describing **how to use those base mechanisms** rather than introducing parallel infrastructure. If there is any ambiguity, the base research document is the reference for concrete tool/runtime/database selection; this spec is the reference for how those pieces should behave together.

---

**Key Takeaways**
- Handshake is a sovereign, offline-capable desktop workspace combining docs, canvases, and tables with deep AI integration
- The specification evolved through multi-model research synthesis (GPT-4o, Gemini, Claude), now including Diary governance
- Behavioral spec and infrastructure research are complementary views of one architecture
- Core principles: local-first sovereignty, Raw/Derived/Display separation, AI as attributed collaborator, safety through architecture

---


# 2. System Architecture

## 2.1 High-Level Architecture

**Why**  
Before diving into implementation details, you need a mental map of how all subsystems relate. This section provides that overview, enabling targeted deep-dives into specific layers.

**What**  
Enumerates and briefly describes the ten major architectural layers: Desktop Shell, Workspace Data Layer, Model Runtime Layer, Workflow Engine, Flight Recorder, Capability Layer, Connectors, AI UX, Taste Engine, and Dev Tools.

**Jargon**  
- **Tauri**: Rust-based framework for building lightweight desktop apps using system webview instead of bundled Chromium.
- **Rust Coordinator**: The central Rust process managing data, CRDT state, workflows, and service connections.
- **Model Runtime Layer**: Abstraction over local inference servers (Ollama, vLLM, TGI, llama.cpp, ComfyUI), accessed via HTTP/gRPC.
- **Flight Recorder**: Subsystem for logging prompts, model calls, tool invocations, and workflow steps for replay and debugging.
- **WASI-style Capability System**: Security model where each tool/agent receives explicit, scoped, time-limited permission tokens.
- **Shadow Workspace**: Background indexer that parses, chunks, embeds, and indexes workspace content for retrieval.
- **Taste Engine**: Subsystem capturing user style/preferences via embeddings and JSON descriptors.

---

At a high level, Handshake consists of the following major subsystems:

### 2.1.1 Desktop Shell & Coordinator

- **Tauri app** with a **React** front-end.
- **Rust coordinator** process managing local data, CRDT documents, workflows, and connections to local services (model runtimes, sync, etc.).

### 2.1.2 Workspace Data Layer

- **Local document store** (SQLite + CRDT data structures).
- **Knowledge graph** (embedded graph/relational engine â€“ e.g. CozoDB or KuzuDB, or DuckDB with graph extension).
- **Shadow Workspace** for parsing, chunking, embedding, and indexing.
- **CRDT engine (Yjs or equivalent)** for real-time document/canvas/table collaboration and AI participation.

### 2.1.3 Model Runtime Layer

- Encapsulates calls from the coordinator into the local model servers described in the runtime research (Ollama, vLLM/TGI, llama.cpp, SDXL/ComfyUI, etc.), over HTTP/gRPC.

### 2.1.4 Automation & Workflow Engine

- A **local workflow runtime** that executes typed node graphs:
  - Triggers (time, events, webhooks).
  - Workspace operations (read/write docs, canvases, tables).
  - AI nodes (LLM calls, embedding jobs, image generation).
  - Control flow (branching, loops, retries).
- Stores state and history in **SQLite**, with Temporal-inspired durable execution and resumability.

All AI workâ€”whether editing documents, transforming spreadsheets, transcribing audio, or ingesting filesâ€”executes as **AI jobs** under a unified model.

### 2.1.5 Observability & Flight Recorder

- A **Flight Recorder** subsystem using **DuckDB** (or similar) to log every significant event:
  - Prompts, model calls, tool calls, workflow steps.
  - Errors, timeouts, resource usage snapshots.
- A replay/debugger UI to explore and reproduce "timelines" of actions.

### 2.1.6 Capability & Security Layer

- A **WASI-style capability system** controlling what each tool, agent, workflow, or plugin can interact with:
  - File system scopes.
  - Network domains.
  - Workspace entities (docs, tables, canvases, tags).
- Capability tokens are scoped, time-limited, and auditable.
- All AI jobs MUST respect these capability scopes.

### 2.1.7 Connectors & External Data Layer

- Adapters for external systems:
  - Email via **JMAP**.
  - Calendar via **CalDAV**.
  - Generic HTTP / webhook connectors.
  - Emerging **Model Context Protocol (MCP)** tools for structured knowledge sources.
- External data is funneled into the **knowledge graph** and/or stored as RawContent with DerivedContent summarization and linking.

### 2.1.8 AI UX & Interaction Layer

- Unified AI entry points:
  - **Command Palette** (explicit tasks).
  - **Structural Editor** (contextual refactors and transformations).
  - **Background Agents** (ongoing suggestions, linking, clustering).
- All tied to the Raw/Derived/Display semantics and the capability system.

### 2.1.9 Taste Engine & Personalisation Layer

- Models and embeddings that capture the user's style, preferences, and "visual taste".
- Represented as a **JSON taste descriptor** injected into prompts and model configuration.
- **This is where DES-001, IMG-001, and SYM-001 from the Diary integrate** â€” the extraction pipeline feeds the Taste Engine.

### 2.1.10 Dev Tools & Extension Platform

- An integrated terminal and scripting interface.
- Extension/plugin APIs for scripts, custom views, and AI tools.
- Sandboxed execution environments for untrusted code.

---

### 2.1.11 Hardware Context: The RTX 3090 Setup

**Why this matters:** Understanding VRAM constraints is critical for model selection and concurrent execution planning.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   YOUR SETUP                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CPU:  AMD Ryzen 5950X (16 cores, 32 threads)          â”‚
â”‚  RAM:  128 GB DDR4                                      â”‚
â”‚  GPU:  NVIDIA RTX 3090 (24 GB VRAM)                    â”‚
â”‚  OS:   Windows                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.1.11.1 VRAM Budget

- ~1-2 GB: System/driver overhead (always used)
- Remaining: ~22 GB for models

| Configuration | VRAM Used | Remaining |
|---------------|-----------|-----------|
| Two medium models (Mistral-7B + CodeLlama-7B) | 8 GB | 14 GB free |
| One large model (Llama2-70B-4bit) | 17 GB | 5 GB (tight!) |
| Medium model + image gen (Mistral-7B + SDXL) | 11-14 GB | 8-11 GB |

#### 2.1.11.2 Speed: GPU vs CPU

âš¡ **Critical:** Running models from GPU VRAM is approximately 6x faster than running them from system RAM.

| Where Model Lives | Speed | When to Use |
|-------------------|-------|-------------|
| GPU VRAM | ~50-130 tokens/sec | Always prefer this |
| System RAM (CPU) | ~8-20 tokens/sec | Last resort / fallback |

#### 2.1.11.3 Practical Rules of Thumb

ğŸ“Œ **Model Size Formula:** A 7B parameter model at 4-bit quantization â‰ˆ 4GB VRAM

ğŸ“Œ **Safe Concurrent Limit:** 2-3 small models (7B) OR 1-2 medium models (13B) at once

ğŸ“Œ **Don't Mix Heavy Workloads:** Running SDXL image generation while querying a large LLM will likely exceed VRAM

ğŸ“Œ **Buffer for Context:** Long conversations use extra VRAM for "context" (what the model remembers). Budget 2-4GB headroom.

---

### 2.1.12 Architecture Block Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE (Frontend)                    â”‚
â”‚         Documents | Boards | Spreadsheets | Chat | Settings     â”‚
â”‚                        [Tauri + React/Vue]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Commands & Events
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ORCHESTRATOR (Python Backend)                 â”‚
â”‚  â€¢ Routes requests to appropriate AI models                     â”‚
â”‚  â€¢ Manages which models are loaded                              â”‚
â”‚  â€¢ Handles plugin execution                                     â”‚
â”‚  â€¢ Coordinates data sync                                        â”‚
â”‚  â€¢ Enforces Diary governance rules (gates, layers, modes)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                  â”‚                 â”‚
            â–¼                  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM RUNTIMES    â”‚ â”‚  LOCAL DATA    â”‚ â”‚    PLUGIN SYSTEM     â”‚
â”‚ (Ollama, vLLM)    â”‚ â”‚ (SQLite+CRDT)  â”‚ â”‚  (Sandboxed code)    â”‚
â”‚                   â”‚ â”‚                â”‚ â”‚                      â”‚
â”‚ â€¢ Mistral-7B      â”‚ â”‚ â€¢ Documents    â”‚ â”‚ â€¢ User automations   â”‚
â”‚ â€¢ CodeLlama       â”‚ â”‚ â€¢ Boards       â”‚ â”‚ â€¢ AI tools           â”‚
â”‚ â€¢ Creative LLM    â”‚ â”‚ â€¢ Spreadsheets â”‚ â”‚ â€¢ Integrations       â”‚
â”‚ â€¢ SDXL (images)   â”‚ â”‚ â€¢ Descriptors  â”‚ â”‚ â€¢ Extraction helpers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â€¢ Sync state   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RTX 3090 GPU    â”‚ â”‚   Hard Drive   â”‚
â”‚   (24GB VRAM)     â”‚ â”‚   (Files)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Key Takeaways**
- Architecture is a layered stack: Desktop Shell â†’ Coordinator â†’ Data Layer â†’ Model Runtime â†’ Workflow Engine â†’ Observability â†’ Security â†’ Connectors â†’ AI UX â†’ Taste Engine â†’ Dev Tools
- The Rust coordinator is the central orchestration point managing all inter-process communication and state
- All AI actions flow through the capability system and are logged to the Flight Recorder, making them auditable and reversible
- External data (email, calendar, webhooks) enters through Connectors and becomes part of the unified knowledge graph
- 24GB VRAM is generous but not unlimited â€” plan model loading carefully
- The Taste Engine integrates with Diary extraction pipeline (DES-001, IMG-001, SYM-001)

---

## 2.2 Data & Content Model

**Why**  
The data model is the foundation for all featuresâ€”documents, canvases, tables, AI collaboration, sync, and search. Misunderstanding it leads to incorrect implementations and broken invariants.

**What**  
Defines core workspace entities (Workspace, Project, Document, Block, Canvas, Table, etc.), the Raw/Derived/Display content separation with formal rules, the knowledge graph schema, the Shadow Workspace indexing pipeline, the CRDT sync model treating AI as a participant, and the file-tree storage architecture.

**Jargon**  
- **Block**: Smallest atomic unit of document content (paragraph, heading, code, image, etc.).
- **Canvas Node**: A positioned element on a spatial canvas (sticky note, card, frame, image).
- **Unified Node Schema**: Logical super-type encompassing doc blocks, canvas nodes, and workflow nodes.
- **RawContent**: User-authored or canonically imported content; never silently modified by AI.
- **DerivedContent**: AI-generated or computed metadata (embeddings, summaries, tags, plans); safe to regenerate.
- **DisplayContent**: UI-rendered projection of Raw+Derived with policy/safety filters applied.
- **Knowledge Graph**: Graph-relational schema where nodes represent entities and edges represent relationships.
- **Shadow Workspace**: Background indexer using Tree-sitter parsing, chunking, and embedding for retrieval.
- **CRDT Site ID**: Unique identifier for each editing participant, including AI agents.
- **Sidecar File**: Metadata file accompanying a primary content file (e.g., storing block IDs, generation parameters).
- **DescriptorRow**: (From Diary) A structured record describing an image or creative reference â€” feeds the Taste Engine.

---

### 2.2.1 Core Entities

At the lowest level, the workspace is a graph of entities. Key types include:

- **Workspace**: the overall root; contains projects and global resources.
- **Project**: a logical grouping of docs, canvases, tables, tasks, assets, and workflows.
- **Document**: a CRDTâ€‘based tree or sequence of **blocks** (paragraphs, headings, lists, embeds).
- **Block**: the smallest logical unit of doc RawContent; has a type (paragraph, heading, code, image, etc.) and content.
- **Canvas**: a spatial layout of **nodes** and **edges**.
- **Canvas Node**: a block of content on a canvas (sticky note, group, image, frame, card).
- **Table**: a schema plus rows; rows contain cells; columns have types.
- **Task / Event**: structured entities with dates, assignees, statuses, relations.
- **Asset**: files, images, media.
- **External Resource**: emails, calendar events, files from external systems.
- **Workflow / Automation**: node graphs that operate on workspace and external resources.

Each of these entities has:

- A **global ID** (UUID).
- A set of **RawContent** properties (canonical text, binary, or structured data).
- A set of **DerivedContent** properties (embeddings, summaries, tags, plans, layouts, style vectors, etc.).
- One or more **DisplayContent** projections (UI surfaces).

#### 2.2.1.1 Unified Node Schema (Logical Super-Node)

For implementation purposes, many visible elementsâ€”doc blocks, canvas cards, workflow nodesâ€”can be treated as instances of a common **logical node** schema. This is a *logical* model defined on top of the storage and CRDT choices in the base research, not a separate database engine.

A logical node has, at minimum, the following fields:

- `id` (**UUID**): globally unique identifier; used for CRDT referencing and graph edges.
- `content` (**RichText / JSON / payload**): the RawContent payload (e.g. ProseMirror JSON for text, or a small structured record).
- `parent_id` (**UUID | null**) and `order` (**number**): hierarchical placement for linear/block views (e.g. Notion-style page â†’ block tree).
- `graph_inputs` / `graph_outputs` (**edge refs**): references to other node IDs for workflow/dataflow views.
- `x, y, z, width, height` (**numbers**): spatial placement and size for canvas views.
- `kernel_state` (**enum | JSON**): optional execution/runtime state for nodes that participate in workflows (e.g. "idle", "running", "failed", last run metadata).

Different views read different slices of this schema:

- The **block editor** cares mostly about `content`, `parent_id`, and `order`.
- The **canvas view** cares mostly about `x, y, z, width, height` and a subset of content.
- The **workflow view** cares mostly about `graph_inputs`, `graph_outputs`, and `kernel_state`.

The physical layout (file-tree, CRDT documents, SQLite tables) and indexing remain as defined in `Project_Handshake_Research_merged_v2`; the unified node schema is a logical contract over that storage for the UI and AI systems.
These IDs and entity references also serve as the addressing basis for AI jobs. All AI operations reference entities by stable IDs (block ID, row ID, node ID, etc.), never by text offsets. See Â§AI-Job-Model.2.3 for the `EntityRef` structure used by the global AI job model.

---

### 2.2.2 Raw / Derived / Display: Formal Specification

#### 2.2.2.1 RawContent

**RawContent** is:

- Userâ€‘authored text, media, or structured data.
- Canonical representations of imported external content (emails, calendar events, PDFs, code files).

Rules:

1. **AI never directly edits RawContent without explicit user confirmation.**
   - For edits, AI produces a **change proposal** (diff or CRDT operations) which the user applies or rejects.
2. **Destructive operations are explicit.**
   - Deleting or overwriting RawContent requires clear user intent (e.g. selection + "delete" or "accept changes").

Examples:

- The text inside a doc paragraph.
- The body of an email stored in the workspace.
- The numeric value in a table cell.
- The pixels of an image file.

#### 2.2.2.2 DerivedContent

**DerivedContent** is any data that can be recomputed from RawContent and/or other DerivedContent. It is:

- Nonâ€‘authoritative and safe to discard or regenerate.
- Often produced by AI models or deterministic processors.

Common types:

- Embeddings (text, image, multimodal).
- Summaries, bulletâ€‘point outlines, interpretation notes.
- Topic tags, category labels, entities, links.
- Layouts: cluster memberships, autoâ€‘generated canvas groupings, graph structures.
- "Plans" and "diffs": JSON plans for workflows; patch sets to apply to documents.
- Taste descriptors: user style vectors, tonal preferences.

Rules:

1. **DerivedContent is versioned and attributable.**
   - Each Derived item records which model/agent produced it and when.
2. **DerivedContent may be pruned or regenerated at any time.**
   - Storage compaction can drop old Derived entries while retaining Raw; indexes can be rebuilt.
3. **AI typically reads Raw + existing Derived, and outputs new Derived, not Raw mutations.**

#### 2.2.2.3 DisplayContent

**DisplayContent** is:

- The userâ€‘facing rendering and transformation of Raw + Derived.
- The place where **policy, safety filters, redactions, and formatting** are applied.

Examples:

- The text you see in the editor, with or without certain Derived annotations.
- An onâ€‘screen summary of a violent email that hides details but preserves a link to RawContent.
- A simplified table view that hides certain Derived columns.

Rules:

1. **Policy and safety are applied only at Display/Export.**
   - Raw and Derived always retain the full unredacted information (subject to user privacy choices).
2. **DisplayContent may hide or transform content without destroying Raw or Derived.**
3. **Exports (PDF, DOCX, screenshots) are derived from Display.**
   - Export policies (e.g. no minors in output images) are enforced here.

---


## 2.3 Content Integrity (Diary Part 5: COR-700)

**Why**  
Content integrity is non-negotiable. User content must never be silently censored, redacted, or diluted inside the system. Safety filters apply only at export/display time.

**What**  
Defines the content preservation rules, the export-only redaction model, and how this maps to Raw/Derived/Display.

**Jargon**  
- **In-Diary Content**: Content stored within the system (Raw + Derived).
- **Export Content**: Content leaving the system (Display layer with safety filters).
- **Redaction**: Removing or hiding content for safety/policy reasons.
- **Dilution**: Weakening content by softening language or removing detail.
- **Export Guard**: Component that applies safety filters only at export time.

---

### 2.3.1 Core Principle: No In-System Censorship

```rust
// src/validators/cor700.rs

/// C700-01: Preserve raw, explicit intent
/// C700-30: In-diary content MUST remain unredacted and undiluted
#[clause("C700-01", "Preserve raw intent")]
pub struct ContentIntegrityGuard;

impl ContentIntegrityGuard {
    /// C700-30: Validate preservation
    #[clause("C700-30", "Unredacted preservation")]
    pub fn validate_preservation(&self, before: &Content, after: &Content) -> Result<()> {
        // Check for redaction
        if after.is_redacted_version_of(before) {
            return Err(IntegrityError::new(
                "C700-30", 
                "In-diary content cannot be redacted"
            ));
        }
        
        // Check for dilution
        if after.is_diluted_version_of(before) {
            return Err(IntegrityError::new(
                "C700-30", 
                "In-diary content cannot be diluted"
            ));
        }
        
        Ok(())
    }
    
    /// C700-32: NSFW and explicit material preserved
    #[clause("C700-32", "Explicit material preserved")]
    pub fn validate_explicit_preserved(&self, content: &Content) -> Result<()> {
        // Explicit content is valid in-diary
        // No special handling needed - just don't censor it
        Ok(())
    }
}
```

### 2.3.2 Export-Only Safety

```rust
// src/export/guard.rs

/// C700-02: Export safety boundaries
/// C700-31: Redaction for export MUST occur outside via Export Guard
#[clause("C700-02", "Define export safety boundaries")]
pub struct ExportGuard {
    /// Safety rules applied at export time
    safety_rules: Vec<SafetyRule>,
}

impl ExportGuard {
    /// C700-31: Redaction only at export
    #[clause("C700-31", "Export-only redaction")]
    pub fn apply_safety(&self, content: &Content, target: ExportTarget) -> ExportedContent {
        let mut exported = content.clone();
        
        for rule in &self.safety_rules {
            if rule.applies_to(&target) {
                exported = rule.apply(exported);
            }
        }
        
        ExportedContent {
            content: exported,
            safety_applied: true,
            original_hash: content.hash(),  // Prove original preserved
        }
    }
}

/// C700-11: External constraints respected at export
#[clause("C700-11", "External constraints at export")]
pub enum ExportTarget {
    /// User's own device (minimal filtering)
    LocalFile { path: PathBuf },
    /// External platform (platform rules apply)
    Platform { name: String, rules: PlatformRules },
    /// Public sharing (maximum filtering)
    Public { audience: Audience },
}
```

### 2.3.3 Mapping to Raw/Derived/Display

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTENT INTEGRITY                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     COR-700 APPLIES                        â”‚
â”‚  â”‚   RAW       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  Content    â”‚     â€¢ Never censor   â”‚                      â”‚
â”‚  â”‚             â”‚     â€¢ Never redact   â”‚                      â”‚
â”‚  â”‚  (L1/L2)    â”‚     â€¢ Never dilute   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚                      â”‚
â”‚         â”‚                             â”‚                      â”‚
â”‚         â–¼                             â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚                      â”‚
â”‚  â”‚  DERIVED    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚
â”‚  â”‚  Content    â”‚     â€¢ AI output      â”‚                      â”‚
â”‚  â”‚             â”‚     â€¢ Preserved too  â”‚                      â”‚
â”‚  â”‚  (metadata) â”‚                      â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚                      â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     EXPORT GUARD                           â”‚
â”‚  â”‚  DISPLAY    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚  â”‚  Content    â”‚     â€¢ Safety filters â”‚                      â”‚
â”‚  â”‚             â”‚     â€¢ Platform rules â”‚                      â”‚
â”‚  â”‚  (export)   â”‚     â€¢ ONLY here      â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3.4 Validator Integration

```rust
// src/validators/content.rs

/// COR-700 validator for all content operations
pub struct Cor700Validator {
    integrity_guard: ContentIntegrityGuard,
    export_guard: ExportGuard,
}

impl Validator for Cor700Validator {
    /// C700-10: Applies to all in-diary content
    #[clause("C700-10", "All in-diary content")]
    fn applies_to(&self, content: &Content) -> bool {
        content.is_in_diary()
    }
    
    fn validate(&self, op: &PlannedOperation, ctx: &Context) -> ValidationResult {
        // Only check integrity for in-diary operations
        if op.target.is_display_layer() {
            // Export operations handled by ExportGuard
            return ValidationResult::Pass;
        }
        
        // In-diary operations must preserve content
        match &op.operation_type {
            OpType::Write { before, after } => {
                self.integrity_guard.validate_preservation(before, after)?;
            }
            OpType::Delete { content } => {
                // Deletion requires explicit user action
                if !ctx.has_explicit_delete_consent() {
                    return ValidationResult::Fail {
                        clause: "C700-30",
                        reason: "Deletion requires explicit consent".into(),
                    };
                }
            }
            _ => {}
        }
        
        ValidationResult::Pass
    }
}
```

---

**Key Takeaways**
- **In-diary content is sacred**: Never censor, redact, or dilute Raw or Derived content
- **Safety at export only**: ExportGuard applies filters when content leaves the system
- **Maps to R/D/D model**: Raw + Derived = COR-700 protected; Display = safety filters allowed
- **Explicit content preserved**: NSFW/adult material is valid in-diary (user's sovereign data)
- **External constraints respected**: Platform rules apply at export, not storage
- **Deletion requires consent**: Can't silently remove content

---

### 2.3.5 Data Architecture: File-Tree Model

**Instead of a traditional database, Handshake stores data as files in foldersâ€”like how you organize documents on your computer, but structured for the application.**

#### 2.3.5.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **File-Tree Architecture** | Using folders and files instead of a database | Data is human-readable, portable, git-friendly |
| **Workspace** | A project or collection of related documents | Top-level folder for a user's project |
| **Sidecar File** | A small file that travels with another file (like subtitles with a video) | Stores metadata without modifying original files |
| **SQLite** | A lightweight database in a single file | Used for indexing/search, not primary storage |
| **CRDT State** | The sync information stored alongside content | Enables conflict-free collaboration |

#### 2.3.5.2 Why Files Instead of a Database?

> **Your data should be yours, in formats you can read.**
>
> | Database Approach | File-Tree Approach |
> |-------------------|-------------------|
> | Data locked in app-specific format | Data in Markdown, JSON, CSV |
> | Need special tools to read | Open in any text editor |
> | Backup requires export | Copy folder = backup |
> | Hard to version control | Git works perfectly |
> | App dies = data access complex | App dies = files remain |

#### 2.3.5.3 The Folder Structure

```
/Handshake/
â”‚
â”œâ”€â”€ workspaces/                          # All user projects
â”‚   â”‚
â”‚   â”œâ”€â”€ my-startup-project/              # One workspace
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ notes/                       # Document editor content
â”‚   â”‚   â”‚   â”œâ”€â”€ meeting-notes.md         # Markdown files
â”‚   â”‚   â”‚   â”œâ”€â”€ product-spec.md
â”‚   â”‚   â”‚   â””â”€â”€ .meta/                   # Metadata sidecar
â”‚   â”‚   â”‚       â”œâ”€â”€ meeting-notes.json   # Block IDs, timestamps
â”‚   â”‚   â”‚       â””â”€â”€ product-spec.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ canvas/                      # Moodboard/canvas content
â”‚   â”‚   â”‚   â”œâ”€â”€ brainstorm.json          # Board data
â”‚   â”‚   â”‚   â””â”€â”€ wireframes.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ sheets/                      # Spreadsheet data
â”‚   â”‚   â”‚   â”œâ”€â”€ budget.csv               # Actual data (portable!)
â”‚   â”‚   â”‚   â””â”€â”€ .meta/
â”‚   â”‚   â”‚       â””â”€â”€ budget.json          # Formulas, formatting
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ databases/                   # Notion-style databases
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.json               # Structured data
â”‚   â”‚   â”‚   â””â”€â”€ contacts.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ images/                      # All images
â”‚   â”‚   â”‚   â”œâ”€â”€ generated/               # AI-created
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ logo-v1.png
â”‚   â”‚   â”‚   â””â”€â”€ uploaded/                # User-added
â”‚   â”‚   â”‚       â””â”€â”€ reference.jpg
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ .handshake/                  # App-specific data
â”‚   â”‚       â”œâ”€â”€ workspace.json           # Settings, preferences
â”‚   â”‚       â”œâ”€â”€ crdt/                    # Sync state (if enabled)
â”‚   â”‚       â”‚   â””â”€â”€ sync-state.bin
â”‚   â”‚       â””â”€â”€ index.db                 # SQLite search index
â”‚   â”‚
â”‚   â””â”€â”€ personal-notes/                  # Another workspace
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                              # Downloaded AI models
â”‚   â”œâ”€â”€ llama-3-13b.gguf
â”‚   â”œâ”€â”€ codellama-7b.gguf
â”‚   â””â”€â”€ sdxl-base.safetensors
â”‚
â””â”€â”€ config/                              # Global settings
    â”œâ”€â”€ settings.json
    â”œâ”€â”€ api-keys.encrypted               # Google OAuth, etc.
    â””â”€â”€ model-registry.json              # What models are available
```

#### 2.3.5.4 File Formats by Content Type

| Content Type | Primary Format | Why This Format |
|-------------|----------------|-----------------|
| **Documents** | Markdown (.md) | Universal, readable, version-control friendly |
| **Canvas Boards** | JSON | Structured data, easy to parse |
| **Spreadsheets** | CSV + JSON sidecar | CSV = data (portable), JSON = formulas/formatting |
| **Databases** | JSON | Flexible schema, human-readable |
| **Images** | PNG/JPG + JSON sidecar | Standard formats, sidecar stores AI prompts |
| **Sync State** | Binary CRDT | Compact, efficient for sync algorithms |
| **Search Index** | SQLite | Fast full-text search |

#### 2.3.5.5 How AI-Generated Images Are Stored

```
/images/generated/
â”‚
â”œâ”€â”€ logo-v1.png                          # The actual image
â”‚
â””â”€â”€ logo-v1.json                         # Sidecar metadata
    {
      "generated_at": "2025-11-29T10:30:00Z",
      "model": "sdxl-1.0",
      "prompt": "minimalist tech startup logo, blue gradient",
      "negative_prompt": "text, watermark",
      "seed": 42,
      "steps": 30,
      "cfg_scale": 7.5,
      "workflow": "comfyui/basic-txt2img.json"
    }
```

ğŸ’¡ **Tip:** Storing generation parameters means you can recreate or tweak images later. The sidecar JSON acts like a "recipe" for the image.

#### 2.3.5.6 The Role of SQLite

âš ï¸ **Important:** SQLite is used for **indexing**, not as the primary data store.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA vs. INDEX                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  FILES (Source of Truth)           SQLite (Index/Cache)     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  â€¢ Markdown documents      â”€â”€â”€â–º   â€¢ Full-text search        â”‚
â”‚  â€¢ JSON databases          â”€â”€â”€â–º   â€¢ Tag lookups             â”‚
â”‚  â€¢ Canvas boards           â”€â”€â”€â–º   â€¢ Quick queries           â”‚
â”‚  â€¢ Spreadsheets            â”€â”€â”€â–º   â€¢ Recent files list       â”‚
â”‚                                                              â”‚
â”‚  If SQLite corrupts, rebuild from files.                    â”‚
â”‚  Files are authoritative; SQLite is derived.                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---


### 2.3.6 File Integrity & Promotion (Diary FIH-001)

**Why**  
Files must be named deterministically, their integrity verified, and promotions between layers controlled. This prevents file-not-found errors, stale previews, and unauthorized layer writes.

**What**  
Defines artifact naming, integrity verification via SHA1, and the promotion gates for L3â†’L2â†’L1 flow.

**Jargon**  
- **Artifact**: Any emitted file (document, image, config, etc.).
- **Promotion**: Moving content from a lower layer to a higher layer (L3â†’L2 or L2â†’L1).
- **Tampering**: SHA1 change without a corresponding manifest entry.

---

#### 2.3.6.1 Artifact Service

```rust
// src/files/artifact_service.rs

/// F001-01: Artifact management service
#[clause("F001-01", "Deterministic naming, integrity, promotions")]
pub struct ArtifactService {
    storage_root: PathBuf,
    layer_access: LayerAccessControl,
    integrity_checker: IntegrityChecker,
}

impl ArtifactService {
    /// F001-05: Only L3 is writable
    #[clause("F001-05", "Editing in L3 only; L1/L2 read-only")]
    pub fn can_write(&self, layer: Layer) -> bool {
        matches!(layer, Layer::L3)
    }
    
    /// F001-03: Validate before operations
    #[clause("F001-03", "Prevent file-not-found, stale preview, layer-write errors")]
    pub fn validate_operation(&self, op: &FileOperation) -> Result<(), FileError> {
        // Check file exists
        if op.requires_existing() && !op.path.exists() {
            return Err(FileError::NotFound { 
                path: op.path.clone(), 
                clause: "F001-03" 
            });
        }
        
        // Check layer writability
        if op.is_write() && !self.can_write(op.target_layer) {
            return Err(FileError::LayerReadOnly {
                layer: op.target_layer,
                clause: "F001-05"
            });
        }
        
        Ok(())
    }
}
```

#### 2.3.6.2 Promotion Gates

```rust
// src/files/promotion.rs

/// F001-11: Promotion paths
#[clause("F001-11", "Promotion Path = L3â†’L2 or L2â†’L1; reverse forbidden")]
pub enum PromotionPath {
    L3ToL2,
    L2ToL1,
}

impl PromotionPath {
    pub fn validate(from: Layer, to: Layer) -> Result<Self, PromotionError> {
        match (from, to) {
            (Layer::L3, Layer::L2) => Ok(Self::L3ToL2),
            (Layer::L2, Layer::L1) => Ok(Self::L2ToL1),
            (Layer::L1, _) => Err(PromotionError::L1Immutable),
            (Layer::L2, Layer::L3) => Err(PromotionError::DemotionForbidden),
            (Layer::L1, Layer::L2) => Err(PromotionError::DemotionForbidden),
            _ => Err(PromotionError::InvalidPath { from, to }),
        }
    }
}

/// F001-90 to F001-94: Promotion gate checks
pub struct PromotionGates;

impl PromotionGates {
    /// F001-90: Only promotions create L2/L1 artifacts
    #[clause("F001-90", "Only promotions may create/replace L2 or L1")]
    pub fn validate_promotion_required(&self, target: Layer, is_promotion: bool) -> Result<()> {
        if matches!(target, Layer::L1 | Layer::L2) && !is_promotion {
            return Err(PromotionError::PromotionRequired { 
                target, 
                clause: "F001-90" 
            });
        }
        Ok(())
    }
    
    /// F001-91: L3â†’L2 requirements
    #[clause("F001-91", "L3â†’L2: naming, integrity, link, manifest, lint must PASS")]
    pub fn validate_l3_to_l2(&self, ctx: &PromotionContext) -> Result<(), Vec<String>> {
        let mut failures = Vec::new();
        
        if !ctx.naming_passed { failures.push("naming".into()); }
        if !ctx.integrity_passed { failures.push("integrity".into()); }
        if !ctx.link_passed { failures.push("link".into()); }
        if ctx.manifest.is_none() { failures.push("manifest".into()); }
        if ctx.has_lint_failures { failures.push("lint".into()); }
        
        if failures.is_empty() { Ok(()) } else { Err(failures) }
    }
    
    /// F001-92: L2â†’L1 requirements (all L3â†’L2 plus stability)
    #[clause("F001-92", "L2â†’L1: all F001-91 gates plus stability attestation")]
    pub fn validate_l2_to_l1(&self, ctx: &PromotionContext) -> Result<()> {
        self.validate_l3_to_l2(ctx)?;
        
        if !ctx.stability_attested {
            return Err(PromotionError::StabilityNotAttested { clause: "F001-92" });
        }
        Ok(())
    }
    
    /// F001-93: Tampering detection
    #[clause("F001-93", "SHA1 change without manifest = TAMPER_DETECTED")]
    pub fn detect_tampering(&self, current: &str, recorded: &str, has_manifest: bool) -> Result<()> {
        if current != recorded && !has_manifest {
            return Err(PromotionError::TamperDetected {
                expected: recorded.into(),
                actual: current.into(),
                clause: "F001-93",
            });
        }
        Ok(())
    }
}
```

#### 2.3.6.3 Integrity Verification

```rust
// src/files/integrity.rs

/// F001-93: SHA1-based integrity checking
pub struct IntegrityChecker;

impl IntegrityChecker {
    /// Compute SHA1 hash of file content
    pub fn compute_sha1(&self, content: &[u8]) -> String {
        use sha1::{Sha1, Digest};
        let mut hasher = Sha1::new();
        hasher.update(content);
        format!("{:x}", hasher.finalize())
    }
    
    /// Verify file against recorded hash
    pub fn verify(&self, path: &Path, recorded_sha1: &str) -> Result<(), IntegrityError> {
        let content = std::fs::read(path)?;
        let actual = self.compute_sha1(&content);
        
        if actual != recorded_sha1 {
            return Err(IntegrityError::HashMismatch {
                path: path.to_path_buf(),
                expected: recorded_sha1.into(),
                actual,
            });
        }
        Ok(())
    }
}
```

#### 2.3.6.4 Integration with Layer System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAYER PROMOTION FLOW                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚   L3    â”‚  â† All edits happen here (F001-05)           â”‚
â”‚   â”‚ (Draft) â”‚                                               â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                               â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼  F001-91: naming, integrity, link, manifest, lint  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚   L2    â”‚  â† Promotion only (F001-90)                  â”‚
â”‚   â”‚(Stable) â”‚                                               â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                               â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼  F001-92: all L3â†’L2 gates + stability attestation  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚   L1    â”‚  â† Immutable (F001-90)                       â”‚
â”‚   â”‚(Frozen) â”‚                                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                              â”‚
â”‚   âœ— Reverse flow (demotion) is FORBIDDEN (F001-11)         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.3.7 Knowledge Graph & Storage

Handshake stores its "mental model" of the workspace and external context as a **knowledge graph schema** implemented on top of the primary local database from the base research (e.g. SQLite). Graph engines such as the following remain candidates for future optimisation or specialised queries:

- **CozoDB**
- **KuzuDB**
- **DuckDB with graph extensions**

Key properties:

- **Nodes** represent entities: docs, blocks, canvases, nodes, emails, events, people, organizations, tasks, tags, workflows, etc.
- **Edges** represent relationships: AuthoredBy, Mentions, RespondsTo, PartOf, DependsOn, ScheduledFor, DerivedFrom.
- **Attributes** on nodes/edges represent Raw and Derived fields.

Use cases:

- Structural queries:
  - "Show all docs tagged 'LLM observability' edited this month."
  - "Find all tasks linked to emails from Alice in Q4."
- Retrieval preâ€‘filter:
  - Graph queries narrow candidate sets before vector search.
- Provenance:
  - Edges represent how DerivedContent was created from Raw inputs (for citations, trust, and debugging).

The knowledge graph is updated by:

- **Workspace events** (new docs, edits, relations).
- **Connectors** (incoming emails, calendar events, bookmarks).
- **Agents** (adding/strengthening weak ties based on context).

---

### 2.3.8 Shadow Workspace & Indexing Pipeline

The **Shadow Workspace** is a semantic mirror of the workspace, optimized for retrieval and AI context.

Components:

1. **File and event watchers**
   - Monitor workspace database, CRDT updates, and external file changes.

2. **Incremental parsing**
   - Uses **Treeâ€‘sitter** (or equivalent) for syntaxâ€‘aware parsing of text documents and code.
   - For docs, the "syntax" can be blockâ€‘level or markdown; for code, actual language grammars.

3. **Chunking and node definition**
   - Each unit of embedding (a "node") corresponds to a semantically meaningful chunk:
     - Paragraphs or sections for prose.
     - Function or class blocks for code.
     - Visual groupings for canvases (frames, clusters).
     - Rows or logical slices for tables.

4. **Dirty node detection**
   - On each change, only affected nodes are recomputed.
   - Hashes of RawContent segments and metadata determine whether reâ€‘embedding is needed.

5. **Embedding & vector store**
   - Embedding models:
     - Text: e.g. `nomic-embed-text` or similar local model.
     - Images: CLIP or equivalent.
   - Vector storage:
     - Embedded in SQLite (sqliteâ€‘vec) or a local vector store like LanceDB.
   - Each vector record is tagged with:
     - Workspace entity IDs.
     - Node type (doc block, code block, canvas node, etc.).
     - Version and timestamp.

6. **Latency budgets**
   - Shadow Workspace and embedding updates are incremental and asynchronous; editing should not be blocked by indexing.
   - Large batch operations (e.g. imports, full re-index) may run in the background with progress indicators; concrete latency targets are defined and validated using the benchmark harness described in the base research.

A lightweight **projection engine** sits between the CRDT state and the Shadow Workspace indices:

- It listens to CRDT changes (e.g. Yjs updates) and keeps the knowledge graph, embeddings, and view-specific metadata in sync (for example, turning wiki-style references into graph edges, or updating canvas links when blocks change).
- For operations that originate in graph/canvas views (e.g. drawing a connection between two cards), it projects those relationships back into document/block representations (links, references, properties) so that all views remain consistent over time.

Handshake's retrieval and memory behaviour is **hybrid** by design:

- For fineâ€‘grained lookups ("find the paragraph about CRDTs"), it leans on vector search over Shadow Workspace embeddings.
- For higherâ€‘level questions ("what are the main themes in this project?"), it traverses the knowledge graph to aggregate and summarise related nodes.
- At runtime, agents operate over a simple memory hierarchy:
  - **Working context**: the current prompt window for a given model call.
  - **Shortâ€‘term memory**: recent blocks, active canvas nodes, and the last interactions in a session.
  - **Longâ€‘term memory**: the global knowledge graph and Shadow Workspace indices, accessed via tools.

In canvas and workflow views, these memory anchors can appear as explicit nodes (e.g. "Project Specs", "Research Cluster") that users can wire into pipelines, making the memory model visible and debuggable.

---

### 2.3.9 CRDT & Sync Model (Humanâ€“AI Collaboration)

Handshake uses a **CRDT** (Conflictâ€‘free Replicated Data Type) engine, such as **Yjs**, to represent collaboratively editable content:

- Documents are sequences/trees of CRDT blocks.
- Canvases are maps of CRDT objects (nodes, positions, styles).
- Tables are CRDT maps/arrays for schemas and rows.

Key extension in v1.5:

- **AI is a firstâ€‘class CRDT participant**:
  - The AI has its own site ID in the CRDT log.
  - When AI proposes edits, they are represented as CRDT operations authored by the AI site.
  - The user can accept/reject AI batches; accepted operations become part of the shared history.

Benefits:

- **Unified history and undo**:
  - AI and human changes share the same timeline; the user can roll back AI changes specifically.
- **Future collaboration**:
  - Multiple humans and one or more AI agents can coâ€‘edit the same doc/canvas/table, with conflicts resolved by CRDT semantics.
- **Offline friendliness**:
  - The user and AI can operate even offline; CRDT sync merges changes later.

Sync beyond one device:

- Initially, singleâ€‘machine only (CRDT mainly for AI vs human concurrency).
- Later, multiâ€‘device sync and smallâ€‘team collaboration using CRDTâ€‘based sync servers (e.g. Yâ€‘protocol over WebSocket, local relay, or custom).

---

**Key Takeaways (3.7 - Connectors)**
- External data uses open protocols (JMAP, CalDAV) over proprietary APIs
- All connectors flow into unified knowledge graph
- Connectors run under capability contracts with explicit permissions
- Secrets stored locally and encrypted

---
### 2.3.11 Taste Engine & Personalisation
#### 2.3.11.1 Goals

The Taste Engine captures the userâ€™s preferences so AI outputs feel â€œlike themâ€ while staying under their control.

Targets:

- Writing style (tone, level of detail, vocabulary).
- Visual style (moodboard aesthetics, colour palettes, composition).
- Common structural patterns (how they organise docs, canvases, tables).

#### 2.3.11.2 Signals & Data Sources

The engine learns from:

- User edits to AI drafts (what they accept vs change).
- Manually rated examples (â€œgood outputâ€, â€œbad outputâ€).
- Pinned reference docs (writing samples, moodboards).
- Frequently used templates and layouts.
- Tags and categories that the user applies or removes.

#### 2.3.11.3 Models & Representations

1. **Visual Taste**
   - Uses CLIP or similar to embed images and canvases.
   - Aggregates moodboard images into a **visual style vector**.

2. **Textual Taste**
   - Uses language model embeddings and possibly authorial language models or **LoRA adapters** to capture style.
   - A lightweight adapter can shift output towards the userâ€™s style when generating text.

3. **JSON Taste Descriptor**

The core representation is a compact JSON object, e.g.:

```json
{
  "tone": "informal but precise",
  "verbosity": 0.7,
  "structure_preferences": ["bullet-heavy", "short paragraphs"],
  "visual_style": {
    "palette": "muted",
    "density": "sparse",
    "style_vector": [0.12, -0.57, ...]
  },
  "banned_terms": ["synergy", "paradigm shift"]
}
```

This descriptor is:

- Stored as DerivedContent attached to the user profile / workspace.
- Injected into prompts and model configurations where relevant.


#### 2.3.11.4 Descriptor Extraction (DES-001)

**Why**  
The Taste Engine needs structured data to learn from. DES-001 defines the schema for extracting descriptors from images and creative content â€” this is the primary data source.

**What**  
Defines the `DescriptorRow` schema, domain structure, and extraction rules that feed the Taste Engine.

**Jargon**  
- **DescriptorRow**: A structured record describing one image or creative reference.
- **Domain**: A category of descriptor fields (e.g., pose_body, camera_optics, color_palette).
- **CONFIG Vocab**: Controlled vocabulary from configuration â€” no free-form strings allowed.

---

##### 2.3.11.4.1 DescriptorRow Schema

```rust
// src/extraction/descriptor_row.rs

/// DES001-80: Core DescriptorRow structure
#[clause("DES001-80", "Valid DescriptorRow shape")]
pub struct DescriptorRow {
    /// DES001-81: Unique identifier
    pub id: DescriptorId,
    
    /// DES001-82: Source image/content reference
    pub source: SourceRef,
    
    /// DES001-83: Extraction timestamp
    pub extracted_at: DateTime<Utc>,
    
    /// DES001-84: Domain-specific descriptor fields
    pub domains: Domains,
    
    /// DES001-85: Consent and attribution
    pub consent: ConsentBlock,
    
    /// DES001-86: Extraction confidence scores
    pub confidence: ConfidenceScores,
    
    /// DES001-87: Provenance trail
    pub provenance: Provenance,
}

/// DES001-60: Domain container
pub struct Domains {
    // Core visual domains
    pub pose_body: Option<PoseBody>,
    pub camera_optics: Option<CameraOptics>,
    pub lighting: Option<Lighting>,
    pub color_palette: Option<ColorPalette>,
    pub composition: Option<Composition>,
    
    // Extended domains
    pub face_morphology: Option<FaceMorphology>,
    pub stylization: Option<StylizationProfile>,
    pub materials: Option<MaterialProfile>,
    pub typography: Option<Typography>,
    
    // Symbolic layer (from SYM-001)
    pub shot_dna: Option<ShotDna>,
}
```

##### 2.3.11.4.2 Domain Field Requirements

Each domain has required and optional fields:

| Domain | Required Fields | Optional Fields |
|--------|-----------------|-----------------|
| `pose_body` | stance, gesture | weight_distribution, tension |
| `camera_optics` | angle, distance | lens_type, depth_of_field |
| `lighting` | direction, quality | color_temp, contrast_ratio |
| `color_palette` | dominant_colors | mood, saturation_profile |
| `composition` | framing, balance | rule_of_thirds, leading_lines |

##### 2.3.11.4.3 Validation Rules

```rust
/// DES001-100: All domain values must come from CONFIG vocab
#[clause("DES001-100", "CONFIG vocab only")]
pub fn validate_vocab(row: &DescriptorRow, config: &Config) -> Result<(), VocabError> {
    for domain in row.domains.iter_populated() {
        for (field, value) in domain.fields() {
            if !config.vocab.contains(domain.name(), field, value) {
                return Err(VocabError::InvalidValue {
                    domain: domain.name(),
                    field,
                    value: value.clone(),
                    clause: "DES001-100",
                });
            }
        }
    }
    Ok(())
}

/// DES001-101: No free-form strings in domain fields
#[clause("DES001-101", "No free-form strings")]
pub fn validate_no_freeform(row: &DescriptorRow) -> Result<(), SchemaError> {
    // All text must be vocab-controlled or structured
    // Free text only allowed in notes/comments fields
}
```

##### 2.3.11.4.4 How Descriptors Feed the Taste Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Image       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IMG-001       â”‚  â† Extraction pipeline
â”‚   Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DescriptorRow  â”‚  â† DES-001 schema
â”‚  (structured)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SYM-001       â”‚  â† Symbolic analysis
â”‚   (SHOT_DNA)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Taste Engine   â”‚  â† Aggregates descriptors
â”‚  (style vector) â”‚     into taste profile
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The Taste Engine aggregates many `DescriptorRow` records to learn:
- Which visual elements the user saves/likes
- Patterns in composition, color, lighting choices
- Symbolic layer preferences (mood, tone, themes)

#### 2.3.11.5 Learning Loop

Taste is updated over time via:

- **Direct Preference Optimisation (DPO)â€‘style learning:**
  - Pairs of (AI draft, userâ€‘edited version) are used to update internal preference representations.
- **Explicit feedback:**
  - â€œMore like thisâ€, â€œLess like thisâ€, â€œNever againâ€ controls on outputs.
- **Manual resets:**
  - The user can reset or fork their taste profile if it drifts or if they want different profiles per project/theme.

#### 2.3.11.6 Privacy & Control

- Taste data is local only; no upload to external servers by default.
- The user controls whether taste information is used when calling cloud models.
- Taste modifiers can be disabled for tasks that require neutrality or a different style.

---
#### 2.3.11.7 Implementation Notes (Local Preference Learning)

When this spec refers to "DPO-style" learning for taste, it is describing the **preference signal pattern**, not local fine-tuning of large models.

- Handshake does **not** update base LLM weights locally.
- Instead, it maintains a preference dataset over generations (kept, edited heavily, rejected).
- Small local models (e.g. logistic regression or a small MLP over embeddings and metadata) are trained to estimate "on-taste vs off-taste".
- At generation time, Handshake uses these signals to:
  - re-rank candidate generations (where the runtime supports n-best or sampling), and/or
  - inject structured taste descriptors into prompts (JSON taste profile) rather than changing model weights.

This keeps taste adaptation cheap, local, and reversible while still giving the system a stable sense of "what looks like you".

## 2.4 Extraction Pipeline (The Product)

**Why**  
This is what Handshake is for. Everything else â€” governance, infrastructure, mechanical layers â€” exists to make this work reliably. The extraction pipeline turns images and creative content into structured, searchable, learnable descriptors.

**What**  
Defines the complete extraction pipeline: IMG-001 (image analysis), SYM-001 (symbolic engine), and how they integrate with DES-001 (descriptor schema).

**Jargon**  
- **Extraction Pipeline**: The sequence of stages that transforms raw content into structured descriptors.
- **SHOT_DNA**: Symbolic fingerprint of an image's meaning and emotional content (from SYM-001).
- **Symbolic Layer**: One of 8 meaning layers (LITERAL through META).
- **Motif**: A recurring symbolic pattern with accumulating semantic weight.
- **Detector**: Component that extracts raw features from images (pose, color, etc.).
- **Mapping Layer**: Translates detector output to CONFIG vocabulary.

---

### 2.4.1 Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTRACTION PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  INPUT  â”‚â”€â”€â”€â–¶â”‚ IMG-001 â”‚â”€â”€â”€â–¶â”‚ DES-001 â”‚â”€â”€â”€â–¶â”‚ SYM-001 â”‚ â”‚
â”‚   â”‚ (image) â”‚    â”‚(extract)â”‚    â”‚(schema) â”‚    â”‚(symbolic)â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â”‚              â”‚              â”‚        â”‚
â”‚                       â–¼              â–¼              â–¼        â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                 â”‚         DescriptorRow               â”‚     â”‚
â”‚                 â”‚  (structured, validated, complete)  â”‚     â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚                         â”‚
â”‚                                    â–¼                         â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                           â”‚   CORPUS    â”‚                   â”‚
â”‚                           â”‚  (storage)  â”‚                   â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.4.2 IMG-001: Image Extraction Pipeline

#### 2.4.2.1 Purpose

IMG-001 defines the deterministic pipeline for extracting DescriptorRows from images. It ensures:
- Consistent extraction across different images
- No hallucinated or invented values
- All values from CONFIG vocabulary
- Silent batch processing (no per-image prompts)

#### 2.4.2.2 Eight-Stage Pipeline

```rust
// src/extraction/img001_pipeline.rs

/// IMG001-180: Eight-stage extraction pipeline
#[clause("IMG001-180", "Deterministic extraction pipeline")]
pub struct ExtractionPipeline {
    stages: [Box<dyn PipelineStage>; 8],
}

impl ExtractionPipeline {
    pub fn new(config: &Config) -> Self {
        Self {
            stages: [
                Box::new(InputCollection::new()),      // IMG001-181
                Box::new(DetectorPass::new(config)),   // IMG001-182
                Box::new(MappingLayer::new(config)),   // IMG001-183
                Box::new(DomainAssembly::new()),       // IMG001-184
                Box::new(Des001Alignment::new()),      // IMG001-185
                Box::new(Validation::new(config)),     // IMG001-186
                Box::new(WriteStage::new()),           // IMG001-187
                Box::new(SidecarGeneration::new()),    // IMG001-188
            ],
        }
    }
    
    /// Run pipeline on image batch
    pub fn extract(&self, images: &[ImageSource]) -> Vec<ExtractionResult> {
        let mut results = Vec::new();
        for image in images {
            let mut ctx = PipelineContext::new(image);
            for stage in &self.stages {
                match stage.process(&mut ctx) {
                    Ok(()) => continue,
                    Err(e) => {
                        ctx.mark_failed(e);
                        break;
                    }
                }
            }
            results.push(ctx.into_result());
        }
        results
    }
}
```

#### 2.4.2.3 Stage Details

| Stage | Clause | What It Does |
|-------|--------|--------------|
| **InputCollection** | IMG001-181 | Gather raw pixels, EXIF, pose skeletons, depth maps |
| **DetectorPass** | IMG001-182 | Run ML detectors (pose, face, objects, colors) |
| **MappingLayer** | IMG001-183 | Map detector output â†’ CONFIG vocabulary |
| **DomainAssembly** | IMG001-184 | Assemble mapped values into domain structs |
| **Des001Alignment** | IMG001-185 | Ensure schema compliance with DES-001 |
| **Validation** | IMG001-186 | Run validators (vocab check, required fields) |
| **WriteStage** | IMG001-187 | Write DescriptorRow to storage |
| **SidecarGeneration** | IMG001-188 | Generate sidecar metadata file |

#### 2.4.2.4 Extraction Rules

```rust
/// IMG001-80: Determinism required
#[clause("IMG001-80", "Pipeline must be deterministic")]
// Same image + same config = same descriptor (always)

/// IMG001-81: Detection/mapping separation
#[clause("IMG001-81", "Detectors don't touch CONFIG")]
// Detectors output raw features; MappingLayer converts to vocab

/// IMG001-82: CONFIG vocab only
#[clause("IMG001-82", "No free-form strings")]
// All domain field values must come from CONFIG

/// IMG001-83: Silent by default
#[clause("IMG001-83", "No per-image user prompts")]
// Batch processing without interruption
```

---

### 2.4.3 SYM-001: Symbolic Engine

#### 2.4.3.1 Purpose

SYM-001 computes the symbolic meaning of an image â€” what it "says" beyond what it literally shows. It produces a `SHOT_DNA` structure that captures mood, tone, themes, and motifs.

#### 2.4.3.2 The Eight Symbolic Layers

```rust
// src/symbolic/layers.rs

/// SYM001-60: Eight symbolic layers
#[clause("SYM001-60", "Layer enum definition")]
pub enum SymbolicLayer {
    Literal,      // What's physically in the image
    Compositional, // How elements are arranged
    Technical,    // Camera, lighting, color choices
    Emotional,    // Mood and feeling evoked
    Narrative,    // Story being told
    Cultural,     // References and context
    Archetypal,   // Universal patterns
    Meta,         // Self-referential/artistic intent
}
```

#### 2.4.3.3 SHOT_DNA Structure

```rust
// src/symbolic/shot_dna.rs

/// SYM001-80: SHOT_DNA canonical fingerprint
#[clause("SYM001-80", "Symbolic fingerprint structure")]
pub struct ShotDna {
    /// SYM001-81: Per-layer scores
    pub layer_scores: LayerScores,
    
    /// SYM001-82: Dominant mood
    pub mood: Mood,
    
    /// SYM001-83: Tonal quality
    pub tone: Tone,
    
    /// SYM001-84: Active motifs
    pub motifs: Vec<MotifActivation>,
    
    /// SYM001-85: Thematic tags
    pub themes: Vec<Theme>,
    
    /// SYM001-86: Visual style classification
    pub style_class: StyleClass,
    
    /// SYM001-87: Color emotion mapping
    pub color_emotion: ColorEmotion,
    
    /// SYM001-88: Composition archetype
    pub composition_archetype: CompositionArchetype,
}

/// SYM001-90: Layer scores (0.0-1.0 per layer)
pub struct LayerScores {
    pub literal: f32,
    pub compositional: f32,
    pub technical: f32,
    pub emotional: f32,
    pub narrative: f32,
    pub cultural: f32,
    pub archetypal: f32,
    pub meta: f32,
}
```

#### 2.4.3.4 Motif System

```rust
// src/symbolic/motifs.rs

/// SYM001-110: Motif structure
#[clause("SYM001-110", "Motif with accumulating weight")]
pub struct Motif {
    /// Unique identifier
    pub id: MotifId,
    
    /// Human-readable name
    pub name: String,
    
    /// Which layers this motif operates on
    pub active_layers: Vec<SymbolicLayer>,
    
    /// Accumulated semantic weight (grows with occurrences)
    pub weight: f32,
    
    /// Trigger conditions
    pub triggers: Vec<MotifTrigger>,
}

/// SYM001-111: Motif activation in a specific image
pub struct MotifActivation {
    pub motif_id: MotifId,
    pub strength: f32,  // 0.0-1.0
    pub evidence: Vec<String>,  // What triggered this
}
```

#### 2.4.3.5 Symbolic Pipeline

```rust
/// SYM001-140: Symbolic analysis pipeline
#[clause("SYM001-140", "Analysis pipeline stages")]
pub struct SymbolicPipeline {
    layer_analyzer: LayerAnalyzer,
    mood_classifier: MoodClassifier,
    motif_detector: MotifDetector,
    dna_assembler: DnaAssembler,
}

impl SymbolicPipeline {
    pub fn analyze(&self, descriptor: &DescriptorRow) -> ShotDna {
        // 1. Compute per-layer scores
        let layer_scores = self.layer_analyzer.score(descriptor);
        
        // 2. Classify mood and tone
        let mood = self.mood_classifier.classify(descriptor, &layer_scores);
        let tone = self.mood_classifier.tone(descriptor, &layer_scores);
        
        // 3. Detect active motifs
        let motifs = self.motif_detector.detect(descriptor, &layer_scores);
        
        // 4. Assemble final SHOT_DNA
        self.dna_assembler.assemble(layer_scores, mood, tone, motifs)
    }
}
```

---

### 2.4.4 Integration: The Complete Flow

```
User saves image
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IMG-001        â”‚
â”‚  Pipeline       â”‚
â”‚                 â”‚
â”‚  1. InputCollection
â”‚  2. DetectorPass  (ML models extract features)
â”‚  3. MappingLayer  (features â†’ CONFIG vocab)
â”‚  4. DomainAssembly
â”‚  5. Des001Alignment
â”‚  6. Validation
â”‚  7. WriteStage
â”‚  8. SidecarGen
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DescriptorRow  â”‚  (DES-001 schema)
â”‚                 â”‚
â”‚  - id, source   â”‚
â”‚  - domains      â”‚
â”‚  - consent      â”‚
â”‚  - confidence   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYM-001        â”‚
â”‚  Pipeline       â”‚
â”‚                 â”‚
â”‚  - Layer scores â”‚
â”‚  - Mood/tone    â”‚
â”‚  - Motifs       â”‚
â”‚  - SHOT_DNA     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Complete       â”‚
â”‚  Descriptor     â”‚
â”‚  (stored in     â”‚
â”‚   CORPUS)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Taste Engine   â”‚  (aggregates many descriptors)
â”‚  (style vector) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Key Takeaways**
- The extraction pipeline is **the product** â€” everything else supports it
- IMG-001 extracts visual features deterministically
- DES-001 defines the schema that holds extracted data
- SYM-001 adds symbolic meaning (mood, motifs, SHOT_DNA)
- All values come from CONFIG vocabulary â€” no free-form strings
- The pipeline is deterministic: same input always produces same output
- Extracted descriptors feed the Taste Engine to learn user preferences

---

## 2.5 AI Interaction Patterns

**Why**  
Understanding how AI integrates into user workflowsâ€”across documents, canvases, and tablesâ€”is essential for building coherent UX. This section defines the interaction models that govern all AI-assisted editing and generation.

**What**  
Describes the AI stack (model roles, runtime topology, routing), the three primary interaction patterns (Command Palette, Structural Editor, Background Agent), and how AI behaves specifically in docs, canvases, tables, and the "Project Brain" RAG interface. Introduces the cyclical "Thinking Pipeline" that moves content between views. **Includes the complete Docs & Sheets AI Job Profile.**

**Jargon**  
- **Model Roles**: Logical categories (orchestrator, code, creative, small, vision) mapped to concrete model endpoints via configuration.
- **Model Runtime Layer**: Abstraction providing a stable internal API (`generate_text`, `embed_text`, `embed_image`, `run_image_pipeline`) over various inference backends.
- **Command Palette**: Explicit AI invocation where user highlights text and requests a specific transformation.
- **Structural Editor**: Zed/Cursor-style multi-step refactors with highlighted changes and undo support.
- **Background Agent**: Continuous DerivedContent computation (suggestions, links, glossary) that never directly edits RawContent.
- **Project Brain**: RAG-style chat interface over a project's docs, canvases, tables, emails, and events with citation support.
- **Thinking Pipeline**: Cyclical workflow moving content through Capture (Docs) â†’ Organise (Canvas) â†’ Refine (Workflows) â†’ Synthesise (Docs).
- **AI Job**: A discrete unit of AI work with lifecycle, provenance, and capability constraints.

---

### 2.5.1 AI Stack & Model Roles

Handshake's AI layer is multiâ€‘model and multiâ€‘modal. Planned roles:

1. **Orchestrator / Router Model**
   - Handles user commands, decides which tools/models to call.
   - Coordinates multiâ€‘step tasks and agents.

2. **Code Model**
   - Specialised LLM for coding tasks, refactors, and scripting.
   - Used for devâ€‘agent, workflow script generation, and explanation of code.

3. **Creative / Uncensored Model**
   - Used for brainstorming, raw idea generation, creative writing, and adult/NSFW contexts when permitted by user settings.
   - Kept separate from orchestrator to maintain clear safety boundaries in Display.

4. **Small / Utility Model**
   - Fast, lowâ€‘latency model for short commands, autocomplete, quick classification/tagging, and microâ€‘transformations.

5. **Vision / Image Models**
   - **CLIP** or similar for image embeddings and visual taste modelling.
   - **SDXL / Stable Diffusion** pipeline (possibly via ComfyUI) for image generation and inpainting.

6. **Optional Cloud Models**
   - GPTâ€‘class or Claudeâ€‘class models for highâ€‘stakes reasoning or largeâ€‘context tasks.
   - Explicitly optâ€‘in and clearly labelled when used.

---

### 2.5.2 Hosting & Runtime Topology

Handshake distinguishes between the **coordinator** (Rust/Tauri host) and the **Model Runtime Layer** (local model servers chosen from the runtime research). The coordinator never talks to individual models directly; it always uses a small, stable internal API.

#### 2.5.2.1 Model Runtime Layer Contract

All model calls go through a versioned internal interface:

- `generate_text(role, task_type, input, settings) â†’ stream<TextChunk> | Error`
- `embed_text(model_id, texts) â†’ EmbeddingBatch | Error`
- `embed_image(model_id, images) â†’ EmbeddingBatch | Error`
- `run_image_pipeline(pipeline_id, inputs) â†’ ImageBatch | Error`

Where:

- `role` is a logical role: `orchestrator | code | creative | small | vision`.
- `task_type` captures the intended use: `chat | completion | tool_call | system_utility`.
- `input` is a structured payload (messages, tools, workspace metadata as needed).
- `settings` includes temperature, max_tokens, top_p, etc.

**Error semantics**

Every call returns either a streaming sequence of chunks (for text) or a single batch, wrapped in a result:

- `ok(...)` on success.
- `error` with:
  - `type`: `runtime_unavailable | rate_limited | invalid_request | internal_error | timeout`  
  - `message`: human-readable explanation for logs/UI.

The coordinator is responsible for:

- Retrying idempotent calls where appropriate (respecting rate limits).
- Surfacing failures to the user (status toasts, inline error markers).
- Falling back to alternate runtimes when possible (e.g. from vLLM to a smaller local model).

**Streaming vs non-streaming**

- `generate_text` is **streaming-first**: the runtime returns chunks as they are generated.  
- Non-streaming behaviour (single final string) is implemented by buffering the stream on the client side.
- The interface uses explicit end-of-stream markers to avoid ambiguity.

**Context and token management**

- The Model Runtime Layer is responsible for enforcing each model's:
  - max context window,  
  - max tokens per call.
- Truncation policy is deterministic and configurable per model family (e.g. keep system + last N turns).
- Actual throughput/latency are measured via the benchmark harness described in the base research; no hard numbers are baked into this spec.

**Role â†’ model mapping**

- Logical roles are mapped to concrete endpoints via configuration, not hard-coded:
  - `orchestrator` â†’ e.g. `gpt-4.1` (cloud) or a strong local model.
  - `code` â†’ code-specialised model (local or cloud).
  - `creative` â†’ uncensored/creative local model.
  - `small` â†’ fast, small model for utility actions.
- These mappings are defined in a config file / DB managed by the coordinator and may change over time without altering the core interface.

#### 2.5.2.2 Deployment Topology

Recommended runtime topology:

- **Rust Coordinator (Tauri host)**
  - UI + local HTTP/gRPC server for the frontâ€‘end.
  - Manages workspace data, CRDT docs, workflows, capabilities, logging, and calls into the Model Runtime Layer.

- **Model Runtime Layer (local model servers)**
  - One or more local model servers as described in the inference runtime research (e.g. Ollama, vLLM/TGI, llama.cpp, SDXL/ComfyUI).
  - Typically run as separate OS processes exposing HTTP/gRPC APIs on localhost.
  - Provide a stable internal interface to the rest of Handshake:
    - `generate_text(role, task_type, input, settings)`
    - `embed_text(model_id, texts)`
    - `embed_image(model_id, images)`
    - `run_image_pipeline(pipeline_id, inputs)`

Benefits of this separation:

- Isolates crashes and OOMs in model servers from the main app.
- Allows independent upgrading of model runtimes and models.
- Fits the runtime options and hosting patterns described in the broader inference runtime research, without hardâ€‘coding a single implementation language or process layout.
- Allows use of Python's rich ML ecosystem while keeping the main app in Rust/Tauri.

---

### 2.5.3 Routing & Session Management

Routing logic lives primarily in the **orchestrator model** and the coordinator:

- For each user request:
  1. The frontâ€‘end collects context: selection, current doc/canvas/table, project brain, taste descriptor, user settings.
  2. The orchestrator decides:
     - Which **model** to call (code vs creative vs utility).
     - Which **tools** to enable (workspace search, workflow engine, connectors).
  3. The coordinator verifies required **capabilities** before allowing tool use.

Session features:

- **Perâ€‘doc/project sessions**:
  - Models receive short "working memory" about the current doc/project, plus links into the knowledge graph.
- **Taste injection**:
  - Each call includes extracted taste parameters relevant to the task (tone, verbosity, visual style vector).

---

### 2.5.4 Multiâ€‘Agent Patterns

Handshake supports multiâ€‘agent patterns atop the orchestrator:

- **Supervisor + Specialists**
  - Supervisor model decides which specialist agent to invoke:
    - Research agent (RAGâ€‘heavy).
    - Writing agent.
    - Coding agent.
    - Workflow design agent.
- **Tool use as graph traversal**
  - Agents use tools to:
    - Search the knowledge graph + Shadow Workspace.
    - Read/write docs, canvases, tables via CRDTâ€‘safe operations.
    - Interact with external connectors (email, calendar, HTTP).

Constraints:

- Agents operate under **capability contracts** that limit:
  - Which projects they can see.
  - Which operations they can execute (READ_DOC, WRITE_DOC, RUN_WORKFLOW, SEND_EMAIL, etc.).
- All agent decisions are logged in Flight Recorder and visible to the user.

---

### 2.5.5 AI Operations on Docs

Docs are structured as **block trees** (similar to Notion, AFFiNE's BlockSuite, and many modern editors).
All AI edits to documents MUST be executed as **Docs & Sheets AI jobs** (Section 2.5.10), operating on block and segment IDs and updating provenance (`ai_origin`).

#### 2.5.5.1 Block Model

- Each block has:
  - `id`
  - `type` (paragraph, heading, list, code, quote, todo, embed, etc.)
  - `raw_text` or structured raw data.
  - Derived annotations (tags, entities, summary, etc.)
  - Relations to other blocks and entities.

#### 2.5.5.2 Primary AI Interactions

1. **Command Palette (explicit)**
   - User highlights text or positions cursor and invokes a command:
     - "Summarise this section"
     - "Refactor into bullet points"
     - "Rewrite in X tone"
     - "Explain this in simple terms"
   - Behaviour:
     - AI reads relevant RawContent + DerivedContext.
     - AI outputs a proposed change as:
       - A **diff** (e.g. unified diff).
       - A set of **CRDT operations**.
     - User preview displays a sideâ€‘byâ€‘side diff.
     - User accepts, modifies, or rejects.

2. **Structural Editor**
   - Zed/Cursorâ€‘style dynamic refactors:
     - Multiâ€‘step transformations (e.g. reorganise section, rename concepts, reâ€‘order paragraphs).
   - Transparency:
     - Highlight AIâ€‘changed blocks.
     - Provide a humanâ€‘readable plan of changes (DerivedContent) and allow "undo AI" on a range.

3. **Background Agent**
   - Continuously computes DerivedContent:
     - Suggested headings.
     - Related docs.
     - Glossary entries.
   - Never directly edits RawContent; only shows suggestions in sidebars or inline nonâ€‘blocking prompts.

---

### 2.5.6 AI Operations on Canvases

Canvases are spatial, visual workspaces inspired by Milanote, Miro, and tldraw.

#### 2.5.6.1 Canvas Model

- Nodes: sticky notes, images, frames, groups, embedded docs.
- Edges: arrows, relationships.
- Layout: positions, zâ€‘order, bounding boxes.

#### 2.5.6.2 Key AI Behaviours

1. **Clustering & Grouping**
   - AI reads sticky note content (RawContent) and creates DerivedContent:
     - Cluster IDs.
     - Suggested group labels.
   - The UI can create **group containers** and arrange notes inside them.

2. **Autoâ€‘Layout**
   - AI proposes canvas layouts (DerivedContent) without immediately applying them.
   - User can "Apply layout" or revert.

3. **Autoâ€‘Moodboard**
   - User provides keywords, links, or existing pins.
   - AI (via CLIP and/or external search connectors) builds a DerivedContent moodboard plan:
     - Candidate images.
     - Style descriptors.
   - User chooses which images to actually add as RawContent.

4. **Visual Summaries & Maps**
   - AI creates summary overlays or miniâ€‘maps of large canvases for navigation and explanation.

Again, AI primarily manipulates **Derived** structures (clusters, layouts) and **proposals** for new nodes rather than mutating existing Raw nodes without consent.

---

### 2.5.7 AI Operations on Tables

Tables combine databaseâ€‘style records with spreadsheetâ€‘style formulas.
All AI transforms on tables MUST be executed as **Docs & Sheets AI jobs** (Section 2.5.10), operating on row and column IDs and updating sheet provenance (`ai_source`, overrides).

#### 2.5.7.1 Table Model

- Schema: columns with types (text, number, date, enum, relation, formula, etc.).
- Rows: CRDT arrays of cells.
- Formulas: optionally backed by a library like **HyperFormula**.

#### 2.5.7.2 Typical AI Assist

- **Generate tables** from prompts ("Create a content calendar for next month with columns X, Y, Z").
- **Fill formulas** and explain formula logic.
- **Suggest derived metrics** and transformations (grouping, pivoting).
- **Identify anomalies** and outliers.

Safety considerations:

- "Dangerous" bulk actions (e.g. mass overwrites, deletes) are always gated by explicit confirmation.
- AI mass actions are applied via diff previews or undoable batch operations.

---

### 2.5.8 Project Brain (RAG Interface)

Each project has a **Project Brain** â€“ a RAGâ€‘style notebook:

- Includes:
  - Relevant docs, canvases, tables.
  - Emails, events, assets linked to the project.
  - Derived embeddings and graph edges.

Features:

- **Askâ€‘theâ€‘Project**: chat interface over the project.
- **Citations**: answers always cite supporting RawContent fragments.
- **Save to Note**: any answer can be turned into a doc block with references preserved.
- **Multiâ€‘agent use**: other agents use the same project brain as context for tasks.

The Project Brain is implemented on top of the Shadow Workspace + knowledge graph:

- Graph filters pick the project's relevant entities.
- Hybrid dense/sparse retrieval chooses snippets to show the model.
- Citations track node IDs for reproducibility and trust.

---

### 2.5.9 Thinking Pipeline (Docs â†” Canvas â†” Workflows)

Handshake is designed around a cyclical "thinking pipeline" that moves through its main views instead of treating them as separate apps:

1. **Capture (Docs)** â€“ the user writes quickly in the linear editor: notes, outlines, rough drafts.
2. **Organise (Canvas)** â€“ blocks appear as cards on canvases; the user clusters, annotates, and links them spatially.
3. **Refine (Workflows)** â€“ selected clusters feed into nodeâ€‘based workflows (e.g. critic â†’ rewriter â†’ summariser) to transform or expand ideas.
4. **Synthesise (Docs)** â€“ workflow outputs are written back into docs as new sections or drafts, ready for manual editing.

This cycle can repeat many times within a project. All three views operate on the same underlying nodes and graph, so moving between them is a change of perspective, not a copyâ€‘paste exercise.

---

### 2.5.10 Docs & Sheets AI Job Profile

**Implements:** AI Job Model (Section 2.6.6)  
**Profile ID:** `docs_sheets_ai_v0.5`

**Protocol metadata**  
- `protocol_name`: `Handshake_Docs_and_Sheets_AI_Integration_Protocol`  
- `protocol_version`: `0.5-draft`  
- `previous_version`: `0.3-draft`

**Requirement language**  
The key words **MUST**, **MUST NOT**, **SHOULD**, **SHOULD NOT**, and **MAY**
in this document are to be interpreted as described in RFC 2119.

**Status**: Draft protocol spec for internal use in Project Handshake.  
**Audience**: Non-technical founder + future engineers and assistants implementing the Handshake editor stack, orchestrator, and tooling.

---
AI jobs defined in this section are executed by the Workflow & Automation Engine described in Section 2.6; this protocol specialises that engine for documents and sheets.

#### 2.5.10.1 Protocol Overview

**Why**  
You need a clear, stable way to plug AI into documents and spreadsheets that:

- Works with local models on your machine.  
- Keeps everything file-backed and auditable.  
- Never turns into â€œAI randomly rewrote my doc and I donâ€™t know whyâ€.

This protocol gives that structure: it defines how AI jobs talk to docs/sheets, how they reference content by ID, and how provenance is tracked.

**What**  
- Defines how **AI jobs** operate on documents and sheets via stable IDs and structure-aware patches.  
- Specifies provenance metadata for blocks, segments, rows, and cells.  
- Describes job configuration, versioning, and observability requirements.  
- Establishes safety and policy expectations for non-destructive AI operations.

**Relationship to Global Job Model:**  
This section defines a **profile** of the global AI Job Model (Â§AI-Job-Model). It inherits all core job fields and specialises:
- `PlannedOperation` types for documents and spreadsheets
- Provenance fields (`ai_origin` for docs, `ai_source`/override for sheets)
- Validation rules for block/row/cell-level changes
- Layer mapping for Raw/Derived/Display content

**Jargon**  
- **Docs & Sheets AI Integration Protocol** â€“ This document: the protocol for integrating AI jobs with documents and sheets.  
- **AI job** â€“ One AI operation with a `job_id`, config file, inputs, outputs, status, and metrics.  
- **Entity reference** â€“ An ID-based handle to a specific part of a doc or sheet (e.g. a paragraph, a line, or a set of rows).  
- **Provenance** â€“ Machine-readable metadata that answers â€œwhich job/model produced this content?â€.  
- **Override** â€“ A human change that takes precedence over AI-generated output for a cell or segment.

---

##### 2.5.10.1.1 Why this protocol exists

Typical â€œAI in Officeâ€ integrations:

- Treat AI as a chat sidebar or a one-shot â€œmagicâ€ action.  
- Mutate large documents/workbooks without stable anchors or provenance.  
- Depend heavily on remote state that conflicts with local-first design.

This protocol exists so that:

- AI operates over **stable IDs and structured entities**, not just raw text.  
- Every AI action is a **job** with configuration, inputs, outputs, and logs.  
- Effects are **non-destructive and auditable**, with clear provenance.  
- Local models and optional cloud fallbacks share the same rules.

##### 2.5.10.1.2 Scope of this protocol

This protocol covers:

1. **Document integration (â€œWord-likeâ€)**  
   - Block-level and segment-level IDs, visible to users.  
   - Structure-aware editing jobs that propose diffs.  
   - `ai_origin` metadata per block/segment.

2. **Spreadsheet integration (â€œExcel-likeâ€)**  
   - Stable row IDs and column identifiers.  
   - AI transforms over ranges expressed via IDs.  
   - `ai_source` + `override` semantics per cell.

It is **implementation-agnostic**:

- It does NOT pick a specific editor engine (e.g. ProseMirror vs Lexical).  
- It does NOT pick a specific sheet engine.  
- It defines contracts that editors and the orchestrator MUST satisfy so that jobs, provenance, and safety are consistent.

---

**Key Takeaways**
- The protocol is job-centric, structure-aware, and non-destructive.  
- Stable IDs and explicit provenance are non-negotiable.  
- It is designed to compose with other local-first protocols and runtimes.

---

#### 2.5.10.3 Core Concepts: Jobs, Provenance, IDs

**Why**  
Before wiring AI into editors, you need a clear concept of â€œwhat is a job?â€, â€œhow do we reference parts of docs/sheets?â€, and â€œhow do we track where AI touched content?â€.

**What**  
- Defines the AI job object and lifecycle assumptions.  
- Specifies provenance fields for docs and sheets.  
- Introduces line/segment addressing for documents.  
- Formalises job configuration and versioning.  
- Defines entity reference shapes and ID invariants.

**Jargon**  
- **AI job** â€“ The single unit of AI work; everything flows through jobs.  
- **`task.yaml`** â€“ Human-readable YAML config file describing one jobâ€™s intent/scope.  
- **`ai_origin`** â€“ Provenance record attached to doc blocks/segments.  
- **`ai_source`** â€“ Provenance record attached to sheet cells.  
- **Entity reference** â€“ A structured pointer like â€œthis paragraphâ€, â€œthese rowsâ€.

---

##### 2.5.10.3.1 AI jobs

An AI job is defined by at least:

- `job_id` â€“ Unique job identifier.  
- `kind` â€“ Job kind (e.g. `doc_edit`, `doc_review`, `sheet_transform`, `sheet_explain`).  
- `config_path` â€“ Path to a configuration document (typically `task.yaml`) describing intent and constraints.  
- `inputs` â€“ Entity references (see (EntityRef section)) specifying what the job MAY read/change.  
- `outputs` â€“ Paths to job outputs (text, diffs, cell values, metrics).  
- `status` â€“ Lifecycle state (see below).  
- `metrics` â€“ Optional performance/quality metrics.

Assumptions:

- The orchestrator executes jobs using local models by default.  
- Jobs are file-backed: configuration, inputs, and outputs live on disk.  
- There is a durable job record that can be inspected after the fact.

UI actions like â€œRewrite paragraphâ€, â€œSummarise this sectionâ€, â€œFill this columnâ€ map to AI jobs under the hood.

Each job MUST track a `status` from the following set (aligned with the global job lifecycle in Â§AI-Job-Model.3):

- `queued` â€“ Job is registered but not yet running; basic capability checks passed.  
- `running` â€“ Job is actively executing.  
- `awaiting_validation` â€“ Mechanical operations ready; validators must approve/reject diffs.
- `awaiting_user` â€“ Validators require human decision (e.g., risky large diff); UI shows preview.
- `completed` â€“ Job finished successfully; all proposed outputs/diffs are
  available, passed validation, and user accepted (if required). Changes committed.  
- `completed_with_issues` â€“ Job finished, but one or more validators reported
  non-fatal issues (e.g. soft constraints violated). Outputs MAY be presented
  only in preview form.  
- `failed` â€“ Job could not complete due to an execution error (e.g. model
  failure, I/O error, invalid config). No outputs are applied.  
- `cancelled` â€“ Job was explicitly cancelled by the user or policy.  
- `poisoned` â€“ Job is known to be unsafe to retry (e.g. systematic validator
  failures or policy violations); implementations MUST NOT auto-retry poisoned
  jobs.

> **Status Mapping Note:** This profile uses the global job lifecycle (Â§AI-Job-Model.3). The `awaiting_validation` and `awaiting_user` states are mandatory for this profile due to the non-destructive editing requirement.

Implementations MAY add additional status values, but they MUST map cleanly
onto this taxonomy for the purposes of UI and analytics.

---

##### 2.5.10.3.2 Provenance metadata

For documents:

- Blocks/segments MAY carry an `ai_origin` structure with at least:
  - `job_id`  
  - `kind`  
  - `model_id` or runtime identifier  
  - `timestamp`  
  - OPTIONAL: taste profile, validator summary, other flags.

For sheets:

- Cells MAY carry an `ai_source` structure with at least:
  - Transform spec path or identifier.  
  - `job_id`  
  - `timestamp`  
  - OPTIONAL: quality flags, model identifiers.

For overrides:

- When a user edits an AI-produced cell value, the cell SHOULD be marked
  `override = true` and future automatic â€œreapply AIâ€ operations SHOULD NOT
  overwrite the value unless the user explicitly resets overrides.

Provenance MUST allow implementations to answer:

- â€œWhich parts of this document came from AI?â€  
- â€œWhich job produced this block or cell?â€  
- â€œWhat did this specific job change?â€

---

##### 2.5.10.3.3 Line/segment addressing for documents

Documents use:

- `block_id` â€“ Stable ID for a logical block (paragraph, list item, code block).  
- `segment_id` â€“ OPTIONAL stable ID or index for a logical subrange inside a
  multi-line block (e.g. a line in a code block).

Requirements:

- `{block_id, segment_id}` resolves to a stable region of text.  
- That region either survives small edits (mapped forward) or is explicitly
  invalidated.  
- Editors can expose these IDs to users (e.g. copy/paste into commands).

Typical UI patterns:

- Gutter labels showing human-friendly IDs (e.g. `3.02`) with full internal IDs
  (`B0234-L02`) on hover or in an inspector.  
- â€œCopy referenceâ€ for the current block/segment.  
- â€œGo to IDâ€¦â€ commands resolving `{block_id, segment_id?}`.

IDs SHOULD be assigned once per node and remain stable across text edits that
preserve identity. When nodes are split or merged, new IDs MUST be created; an
implementation MAY track lineage, but the protocol does not require it.

---

##### 2.5.10.3.4 Job configuration and versioning

Each AI job MUST be backed by a configuration document (typically `task.yaml`)
that captures its intent, scope, and safety constraints.

At minimum, `task.yaml` MUST contain:

- `task_schema_version`  
  Semantic version of the configuration schema (e.g. `1.0.0`).

- `kind`  
  Job kind (e.g. `doc_edit`, `doc_review`, `sheet_transform`, `sheet_explain`).

- `why`  
  Short human-readable intent (â€œwhat problem is this job solving?â€).

- `what`  
  Short behavioural description of the expected output or transform.

- `scope`  
  Description of the target entities (blocks/segments, rows/columns, ranges)
  in terms of the implementationâ€™s ID scheme.

- `inputs`  
  A list of entity references that the job is allowed to read from and/or
  propose changes for (see (EntityRef section)).

- `safety_mode`  
  Execution mode such as `preview_only` (job MAY only propose diffs) or
  `auto_apply` (job MAY apply diffs directly, subject to policy).

Each job MUST also record:

- `protocol_version`  
  Version of this protocol in effect when the job was created (e.g. `0.4`).

Implementations MAY extend `task.yaml` with additional fields (e.g. taste
profiles, model hints, extra constraints) as long as they do not change the
meaning of the core fields above.

Jobs created under an unknown or unsupported `protocol_version` MUST be treated
as read-only provenance artefacts. Implementations MAY support multiple protocol
versions concurrently, but MUST document which versions are replayable.

The protocol does not constrain on-disk layout of configuration files, only that
these fields are available to orchestrators, validators, and provenance logic.

###### Schema evolution and compatibility

Implementations MUST define a compatibility policy for `task_schema_version`
and `protocol_version`.

At minimum:

- Newer orchestrators SHOULD be able to read and display jobs created with
  older `task_schema_version` and `protocol_version` values.  
- When replaying or cloning existing jobs, orchestrators MAY:
  - run them under their original versions, if still supported; or  
  - migrate them to newer versions explicitly, recording the migration.

If a job references a `task_schema_version` or `protocol_version` that is not
supported:

- The job MUST be treated as read-only provenance;  
- Re-execution MUST fail deterministically with a clear error; and  
- Implementations SHOULD emit diagnostics so operators can decide whether to
  add support or migrate jobs manually.

Unknown fields in `task.yaml` MUST be ignored by default (i.e. treated as
forward-compatible extensions) unless a policy explicitly disallows them.

---

##### 2.5.10.3.5 Entity references and ID schemes

AI jobs operate on **entity references**, not raw coordinates.

An entity reference MUST include:

- An artefact identifier (document, sheet, canvas, etc.).  
- A stable, implementation-defined local identifier within that artefact.  
- OPTIONAL additional fields (e.g. sub-range, segment index).

Abstract shapes:

- `doc_segment_ref`  
  - `doc_id` â€“ Document identifier.  
  - `block_id` â€“ Stable block identifier.  
  - `segment_id` â€“ OPTIONAL stable sub-block anchor (e.g. line index).

- `sheet_range_ref`  
  - `sheet_id` â€“ Sheet identifier.  
  - `row_ids[]` â€“ List of stable row identifiers.  
  - `column_ids[]` â€“ List of stable column identifiers or names.

- `canvas_nodes_ref` (future extension)  
  - `canvas_id` â€“ Canvas identifier.  
  - `node_ids[]` â€“ List of stable node identifiers.

Implementations MUST:

- Document their concrete ID schemes (`doc_id`, `block_id`, `row_id`, etc.).  
- Ensure IDs are stable across sessions and local edits.  
- Preserve IDs across sync and merge operations where identity is preserved.  
- Expose IDs to both UI and models for operations like â€œgo to IDâ€ and â€œcopy
  referenceâ€.

The protocol does not mandate specific ID formats; only the invariants above.

###### ID namespacing and collisions

IDs are scoped as follows:

- `doc_id`, `sheet_id`, `canvas_id` MUST be unique within a workspace.  
- `block_id`, `segment_id` MUST be unique within a given `doc_id`.  
- `row_id` MUST be unique within a given `sheet_id`.  
- `column_id` MUST be unique within a given `sheet_id`.

Implementations SHOULD:

- Use opaque, non-semantic IDs (e.g. UUIDs) for all internal identifiers.  
- Treat any human-readable labels (e.g. "H1.3", "Row 42") as presentation only,
  derived from internal IDs.

When importing legacy documents/sheets that lack IDs:

- Implementations MUST assign new IDs to all entities, and  
- MAY store a mapping from legacy positions (e.g. original A1 indices) to new
  IDs for reference and debugging.
- Such mappings SHOULD be treated as implementation detail and MAY be
  garbage-collected after a bounded retention period or once no active
  jobs depend on them.

If an ID collision is detected inside a namespace that MUST be unique:

- The implementation MUST treat this as a schema error,  
- MUST NOT run AI jobs against the affected artefact until the conflict is
  resolved, and  
- SHOULD surface a clear diagnostic to the user or administrator.

---

**Key Takeaways**
- Jobs, provenance, line/segment IDs, job config, and entity references are all
  explicitly defined.  
- Jobs are file-backed, versioned, and bound to stable entity IDs.  
- Provenance fields, override semantics, and ID namespacing are core, not optional.

---

#### 2.5.10.4 Document Integration

**Why**  
Tie the protocolâ€™s ideas (jobs, IDs, patches) to a Word-like editor so AI can
safely manipulate documents in a way that you and the system can understand.

**What**  
- Specifies expectations for the document model.  
- Describes block/segment-level editing via jobs.  
- Defines transaction semantics for applying patches.  
- Describes provenance requirements and hooks into a knowledge graph.  
- Clarifies concurrency expectations for overlapping jobs.

**Jargon**  
- **Document model** â€“ The editorâ€™s internal tree of sections, blocks, inlines.  
- **Patch** â€“ A structured diff over `block_id`/`segment_id`, not raw text.  
- **`ai_origin`** â€“ A record on a block/segment pointing to the AI job that touched it.

---

##### 2.5.10.4.1 Document model expectations

A compliant Word-like editor MUST expose at least:

- A hierarchical document model (document â†’ sections/blocks â†’ inline content).  
- A stable `block_id` attached to each block node.  
- OPTIONAL segment-level anchors (`segment_id`) for multi-line blocks (e.g.
  code blocks, lists).  
- A way to bind comments/annotations to `{block_id, segment_id?, offset}`.  
- Serialisation/deserialisation to a file-backed representation compatible with
  the protocol.

Cell-like structures inside docs (e.g. inline tables, code fences) MAY add
internal IDs, but any IDs used in jobs/provenance MUST still satisfy (EntityRef section).

---

##### 2.5.10.4.2 Block- and segment-level AI-assisted editing

Document-level AI operations such as:

- â€œRewrite this paragraph.â€  
- â€œTighten this section.â€  
- â€œExplain this code block line by line.â€

MUST be defined as jobs where:

- Inputs are `doc_segment_ref`s (single block, contiguous range of blocks, or a
  set of segment IDs).  
- `task.yaml` clearly describes allowed operations (inline-only vs structural).  
- Outputs are diffs/patches expressed over `block_id`/`segment_id`.

Editors MAY offer richer UX, but MUST route through jobs rather than ad-hoc
model calls that directly mutate state.

---

##### 2.5.10.4.3 Structure-aware operations

Higher-level doc operations such as:

- â€œNormalise heading levels in this chapter.â€  
- â€œExtract background material into its own section.â€  
- â€œInsert a new subsection under this heading using this outline.â€

MUST be implemented as jobs that:

- Take ranges of `block_id`s (and optionally segment IDs) as inputs.  
- Specify allowed structural changes in config (e.g. â€œmay reorder blocks, may
  re-level headings, may insert new blocks, may not delete content except
  duplicatesâ€).  
- Emit patches describing structural changes using the same ID scheme as the
  document model.

Implementations SHOULD avoid one giant â€œrewrite entire documentâ€ job and instead
compose smaller, scoped jobs.

---

##### 2.5.10.4.4 Inspection, replay, and addressing

Recommended editor behaviours:

- **Inspect AI provenance**  
  - User selects a block/segment and requests â€œShow AI jobâ€.  
  - UI fetches `ai_origin` and opens the job record (config, diffs, logs).

- **Replay/edit jobs**  
  - User can â€œre-run with changed instructionsâ€ or â€œre-run on updated contentâ€,
    creating a new job with its own `job_id` and config.  
  - Replay MUST NOT overwrite the old job; it creates a new provenance record.

- **ID-based navigation**  
  - â€œGo to blockâ€ / â€œGo to lineâ€ style commands resolve `{block_id, segment_id?}`
    to the current visual position, using CRDT/document mappings.

These are not strictly required for compliance but are strongly recommended.

---

##### 2.5.10.4.5 Transaction semantics for document patches

Assume document state is maintained by a CRDT or similar multi-writer structure.

For document jobs:

- Each accepted job MUST be applied as **one logical transaction per document**
  in the underlying document state (e.g. one CRDT transaction).  
- Within that transaction, the implementation applies all block/segment
  insertions, deletions, and modifications that the job produced.  
- The implementation SHOULD record a mapping between `job_id` and any
  transaction IDs used by the engine so later inspection and undo see the job as
  a single unit.

Before applying a patch, an implementation SHOULD:

- Validate that the patch still matches current structure at referenced IDs
  (e.g. blocks still exist and havenâ€™t been replaced wholesale).  
- If divergence is detected, either:
  - Downgrade to `preview_only` and present the patch as a diff; or  
  - Regenerate the patch against a fresh snapshot.

The protocol does not dictate conflict-resolution strategy. It REQUIRES only:

- Patches are structure-aware (IDs, not raw offsets).  
- Implementations avoid silently discarding or overwriting user edits when
  applying patches based on stale snapshots.

---

##### 2.5.10.4.6 Document provenance and knowledge-graph hooks

When a document job is accepted and its patch applied, the implementation MUST:

- Attach or update `ai_origin` on any blocks/segments materially changed, with
  at least:
  - `job_id`  
  - `kind`  
  - `model_id` or runtime identifier  
  - `timestamp`  
  - OPTIONAL: taste profile IDs, validator flags, etc.

- Persist provenance so that:
  - â€œShow AI job for this block/segmentâ€ reliably resolves to the job and its
    config.  
  - â€œShow everything this job changedâ€ can be derived from stored IDs.

If the system has a knowledge graph or Shadow Workspace, it SHOULD:

- Represent each job as a first-class node.  
- Maintain edges from content nodes (blocks/segments) to job nodes (e.g.
  â€œderived from jobâ€, â€œaffected by jobâ€, â€œexplained by jobâ€).  
- Index job instructions and summaries so retrieval tools can cite them when
  answering questions about document history.

The protocol does not define a specific graph schema; it assumes job and
provenance metadata are rich enough to support such integrations.

---

##### 2.5.10.4.7 Concurrency and overlapping document jobs

Implementations MUST define how concurrent jobs interact when they target
overlapping document regions.

At minimum:

- Orchestrators SHOULD avoid running multiple `doc_edit` jobs that target the
  same `block_id` or overlapping ranges of `block_id`s simultaneously.  
- If two jobs are allowed to run concurrently and both produce patches that
  touch the same entities, the implementation MUST either:
  - serialize application of patches in a well-defined order, and  
  - reject or rebase the second patch if it no longer matches current state;

  or:

  - downgrade the second job to `completed_with_issues` and present its patch
    only as a preview for manual review.

In this context, **rebase** means recomputing the jobâ€™s proposed patch against
the latest document snapshot using the original job instructions (or performing
an equivalent three-way-merge), rather than naively shifting offsets.

Implementations MUST NOT silently drop user edits when applying a patch generated
against an outdated snapshot; they MUST detect divergence and either rebase or
require explicit user confirmation.

---

**Key Takeaways**
- Jobs MUST be durable and observable, not one-off RPCs.  
- There MUST be a clear mapping from job config/inputs to diffs and
  spans/metrics.  
- Local vs cloud runtimes are interchangeable as long as the job model is
  respected.

---

#### 2.5.10.7 Safety, Capabilities, and Cross-Artefact Jobs

**Why**  
AI should not silently wreck content. You also need ways to say â€œthese job types
are allowed here; these arenâ€™tâ€ and to handle jobs that touch both docs and
sheets.

**What**  
- States safety and non-destructive editing requirements.  
- Describes capability and policy expectations.  
- Defines semantics for cross-artefact jobs (doc â†” sheet â†” canvas).  
- Specifies validator interfaces and their effect on job status.

**Jargon**  
- **Capability profile** â€“ A declarative description of what a job kind is
  allowed to do and over what scope.  
- **Policy** â€“ Rules that enable/disable or narrow capabilities in a given
  context.  
- **Cross-artefact job** â€“ A job that operates across multiple artefacts at
  once (e.g. doc + sheet).  
- **Validator** â€“ A component that inspects proposed changes before they are applied.

---

##### 2.5.10.7.1 Safety and non-destructive editing

To prevent silent damage, implementations MUST:

- Persist job config, inputs, and outputs before applying any changes.  
- Run validators over diffs before they affect user-visible state, enforcing
  rules like â€œdo not delete headingsâ€ or â€œdo not modify protected regionsâ€.  
- Surface previews for any change beyond a small, obvious scope.  
- Allow users to revert or override AI outputs at any time.

Editors/orchestrators SHOULD default to `preview_only` for wide-impact jobs.

###### Validator interface

A validator is a component that inspects a proposed diff or transform before it
is applied.

Validators MUST accept at least:

- `job_id`  
- A description of the proposed changes (doc patches, sheet transforms)  
- OPTIONAL contextual metadata (e.g. project, artefact IDs, user identity)

Validators MUST return:

- `result`: one of `allow`, `deny`, `warn`  
- OPTIONAL `reasons[]`: machine-readable codes  
- OPTIONAL `message`: human-readable explanation

Semantics:

- If any validator returns `deny`, the job MUST NOT be auto-applied. The job
  status MUST be set to `failed` or `completed_with_issues` depending on whether
  outputs are still usable as previews.  
- If all validators return `allow` or `warn`, implementations MAY auto-apply
  changes subject to job `safety_mode` and local policy. `warn` conditions
  SHOULD be surfaced in the UI and recorded in job metadata.
- When multiple validators run, the effective result MUST be computed as:
  - `deny` if any validator returns `deny`; else  
  - `warn` if at least one validator returns `warn` and none return `deny`; else  
  - `allow` if all validators return `allow`.  
  Implementations MAY short-circuit on the first `deny`, but MUST respect this
  aggregation rule.

Validators MAY be synchronous or asynchronous, but from the jobâ€™s perspective
validation MUST complete before any changes are committed.

---

##### 2.5.10.7.2 Observability and hygiene

Implementations SHOULD:

- Log job events with `job_id`, `kind`, status, and metrics.  
- Maintain hygiene signals such as:  
  - High validation failure rate for a specific transform.  
  - Excessive job volume/latency for a particular model.  
  - Misconfigured instructions causing repeated failures.

These signals feed dashboards, governance, and auto-tuning.

---

##### 2.5.10.7.3 Future extensions

Future work MAY include:

- Richer cross-artefact jobs (e.g. â€œsummarise this sheet into these doc
  segmentsâ€) with stronger consistency guarantees.  
- Model-assisted validators for business rules in sheets and consistency checks
  in docs.  
- More detailed ID lineage (tracking splits/merges) for advanced audits.  
- Higher-level policy frameworks per project/workspace controlling allowed AI
  operations.

None of this is required for baseline compliance.

---

##### 2.5.10.7.4 Capabilities and policy

Not all jobs are appropriate everywhere. Implementations MUST define a capability
and policy model that controls which job kinds can run, over what scopes, and in
which modes.

At minimum:

- Each job kind SHOULD have a capability profile describing:  
  - Allowed operations (inline edits, structural changes, bulk transforms).  
  - Scope limits (max blocks, rows, characters, cells).  
  - Default `safety_mode` (e.g. `preview_only` for wide-impact kinds).

- Policies MAY be layered (global â†’ workspace â†’ project â†’ artefact) as long as:  
  - More specific policies do NOT silently widen capabilities beyond higher
    layers unless explicitly and audibly configured.  
  - Denied jobs fail predictably with a machine-readable reason.

The protocol does not define a policy language. It REQUIRES that:

- Capability constraints are declarative and inspectable.  
- Orchestrators can check capabilities before running jobs.  
- Users can understand why a job was allowed, downgraded, or denied.

---

##### 2.5.10.7.5 Cross-artefact jobs

The protocol explicitly supports jobs that operate across multiple artefacts
(documents, sheets, canvases).

A cross-artefact job MUST:

- Declare its `inputs` as a list of typed entity references ((EntityRef section)) spanning one
  or more artefacts.  
- Restrict its proposed changes to entities reachable from that input list.

Implementations SHOULD:

- Apply changes as separate transactions per artefact in the data model, while
  treating them as **one logical job** for provenance, undo/redo, and
  observability.  
- Provide tooling to:  
  - Preview diffs per artefact.  
  - Accept or reject the job as a whole, optionally with finer-grained controls
    per artefact or region.

If any artefact-level transaction fails during application of a cross-artefact
job:

- The job status MUST be set to `completed_with_issues` or `failed`, and  
- Implementations MUST NOT silently present a partially-applied job as if it
  were fully successful.

Implementations SHOULD:

- Roll back successful artefact-level transactions when feasible, to preserve
  intuitive all-or-nothing behaviour; or  
- Surface a clear, per-artefact status in the UI so users can see which parts
  of the job succeeded and which did not.

The default expectation is that cross-artefact jobs behave as a single logical
unit in the eyes of the user, even if the underlying storage model applies
changes per artefact.

---

**Key Takeaways**
- Cloud and external runtimes are treated as untrusted by default.  
- Data minimisation and redaction are required for off-device execution.  
- Capability profiles and policies are the primary mechanism for limiting
  exfiltration risk.
---
#### 2.5.10.9 Docs & Sheets Glossary

- **AI job** â€“ A single AI operation with config, inputs, outputs, status.  
- **Artefact** â€“ A first-class workspace object: document, sheet, canvas, etc.  
- **Shadow Workspace** â€“ A derived/indexed representation of workspace artefacts
  used for search, retrieval, and analytics; not the primary source of truth.  
- **Block** â€“ A logical unit in a document (paragraph, list item, code block).  
- **Segment** â€“ A sub-part of a block (e.g. a line in a code block).  
- **Row** â€“ A logical record in a sheet, identified by `row_id`.  
- **Entity reference** â€“ Structured pointer to a specific part of an artefact.  
- **Provenance** â€“ Metadata linking content to jobs, models, and instructions.  
- **Transform spec** â€“ Saved configuration for a repeatable sheet transform.  
- **Validator** â€“ Component that checks proposed changes before they are applied.  
- **Capability profile** â€“ Declarative description of allowed operations for a
  job kind.  
- **Policy** â€“ Rules that apply capability profiles in a particular context.  
- **Trusted runtime** â€“ Local executor within the appâ€™s process boundary.  
- **Untrusted runtime** â€“ Any executor that MAY send data off-device (e.g. cloud LLMs).

---

#### 2.5.10.10 Docs & Sheets Profile Summary

**Why**  
A consolidated summary helps implementers quickly understand what this profile adds to the global job model without re-reading the entire section.

**What**  
Summarizes the Docs & Sheets profile extensions: profile-specific fields, PlannedOperation types, provenance fields, validation rules, capability requirements, and typical job flow.

**Jargon**  
- **block_mode**: Constraint on which block types a job may operate on.
- **range_kind**: Specifies whether a sheet operation targets rows, columns, cells, or ranges.

---

This profile extends the global AI Job Model (Â§AI-Job-Model) with:

| Extension | Description |
|-----------|-------------|
| **Profile-specific fields** | `doc_id`, `sheet_id`, `block_mode`, `range_kind` |
| **PlannedOperation types** | `insert_block`, `replace_block`, `move_block`, `update_metadata`, `apply_formula`, `fill_down`, `normalize_column` |
| **Provenance** | Docs: `ai_origin` per block/segment; Sheets: `ai_source`/override per cell |
| **Validation rules** | Scope validators, layer validators, structural validators, overlap detection |
| **Layer mapping** | Encodes Raw/Derived/Display rules per (Raw/Derived/Display section) |

**Capability Requirements:**
- `READ_DOC` / `WRITE_DOC` for document operations
- `READ_SHEET` / `WRITE_SHEET` for spreadsheet operations
- Additional capabilities per operation type (see (Operations & Dev Experience, Section 7.3))

**Typical Job Flow (Document Edit):**
```
1. queued              â†’ User triggers "rewrite paragraph"; capability check passed
2. running             â†’ LLM generates diff
3. awaiting_validation â†’ Validators check scope, layer rules
4. awaiting_user       â†’ UI shows diff preview
5. completed           â†’ User accepts; changes committed with ai_origin
```

**Key Takeaways**
- AI is multi-model (orchestrator, code, creative, small, vision) with roles mapped to endpoints via configuration, not hard-coded.
- The Model Runtime Layer provides a stable internal API (`generate_text`, `embed_*`, `run_image_pipeline`) that abstracts over various inference backends.
- Three primary interaction patterns: Command Palette (explicit user-triggered), Structural Editor (multi-step refactors with diff preview), Background Agent (continuous DerivedContent suggestions).
- AI on docs produces diffs/CRDT operations for user approval; AI on canvases proposes clusters and layouts; AI on tables suggests formulas and detects anomaliesâ€”all without silently mutating RawContent.
- The Project Brain provides RAG-style chat with citations over all project content.
- The Thinking Pipeline connects Docs â†’ Canvas â†’ Workflows â†’ Docs in a cyclical refinement loop.
- All AI edits to docs and tables run as structured jobs (Section 2.5.10) over stable IDs with explicit provenance and auditability.

---
## 2.6 Workflow & Automation Engine

**Why**  
Automations are how Handshake scales beyond manual AI commands to repeatable, composable pipelines. This section defines the workflow model, execution semantics, and safety constraints that make automations trustworthy.

**What**  
Specifies the workflow engine's goals, the node-based workflow model (triggers, workspace ops, AI ops, connectors, control flow), AI-assisted workflow design constraints, durable execution with SQLite state, and the validation pipeline for safety. **Includes the global AI Job Model that all artefact profiles inherit from.**

**Jargon**  
- **Workflow**: A directed graph of typed nodes that execute in sequence or parallel, triggered by events or schedules.
- **Trigger Node**: Entry point for a workflow (time-based, event-based, webhook).
- **Workspace Operation Node**: Reads/writes docs, canvases, tables, or queries the knowledge graph.
- **AI Operation Node**: Invokes LLM calls, embedding jobs, or image generation.
- **Connector Node**: Interacts with external systems (email, HTTP, MCP tools).
- **Control Flow Node**: Branching (if/else), loops, error handling (try/catch).
- **Durable Execution**: Temporal-inspired pattern where workflow state is checkpointed before each node, enabling crash recovery.
- **Capability Contract**: The set of permissions assigned to a workflow, validated before execution.
- **Gate**: (From Diary) A validation checkpoint that must pass before an operation proceeds.

---

### 2.6.1 Goals & Constraints

The workflow engine must:

- Run **locally**, with no SaaS dependency.
- Be **durable** and resumable (crashes and reboots do not lose state).
- Be **inspectable**: users can see and understand what workflows do.
- Respect **capabilities** for safety (no arbitrary access to file system or network).
- Offer a good surface for **AIâ€‘assisted authoring** without giving AI arbitrary code execution powers.

---

### 2.6.2 Workflow Model

Workflows are **directed graphs of nodes**. Node types include:

1. **Triggers**
   - Timeâ€‘based (cron, interval).
   - Eventâ€‘based (workspace changes, external webhooks, email received).

2. **Workspace Operations**
   - Read/write documents, canvases, tables.
   - Search the knowledge graph.
   - Modify tags, relations, statuses.

3. **AI Operations**
   - LLM call nodes.
   - Embedding / indexing jobs.
   - Image generation/inpainting.

4. **Connectors**
   - Email send/receive.
   - HTTP requests.
   - MCP tools.

5. **Control Flow**
   - Branching (if/else).
   - Loops.
   - Error handling (try/catch, fallback).

6. **Utility Nodes**
   - Formatting, parsing, mapping, filtering.

Each node has:

- A **type**.
- **Inputs** and **outputs** with JSONâ€‘serialisable schemas.
- A list of required **capabilities**.
Docs & Sheets AI jobs (Section 2.5.10), ASR jobs (Section 6.2.26), and Docling jobs (Section 6.1.6) are all **profiles** of the global AI Job Model (Section 2.6.6). They are specialised workflow runs built from the node types above; their job config, lifecycle, and provenance fields are defined in that unified model.

---

### 2.6.3 AIâ€‘Assisted Workflow Design

AI helps users build workflows but under strict constraints:

- The AI is given:
  - User intent in natural language.
  - A **catalog** of node types and their schemas.
  - Information about available capabilities and connectors.

- AI outputs:
  - A **workflow plan** as a JSON structure describing nodes and edges, using only known types and fields.
  - Optional textual explanation for the user.

- The coordinator:
  - Validates the structure against schemas.
  - Checks capability requirements.
  - Presents the plan to the user as a visual graph for approval.

The AI **does not**:

- Emit arbitrary code (no raw Python, JS, or shell script).
- Bypass capability checks or access unapproved resources.

---

### 2.6.4 Execution & Durability

The engine takes inspiration from **Temporal** and **n8n** but is customâ€‘built for localâ€‘first:

- **State Store**
  - Workflow definitions and execution state stored in **SQLite**.
  - Each node execution produces an event record (start, success, failure, outputs).

- **Durable Execution**
  - Before executing each node, state is checkpointed.
  - On crash or reboot, the engine replays events and resumes from the last successful node.

- **Scheduling**
  - A scheduler component watches for triggers and ready nodes.
  - Concurrency is configurable (max concurrent workflows, max nodes per workflow).

---

### 2.6.5 Safety & Validation Pipeline

Before a workflow runs or is modified, a validation pipeline enforces:

1. **Schema validation**
   - All nodes and edges conform to known types and input/output schemas.

2. **Graph validation**
   - No cycles where they're not allowed (unless explicitly supported).
   - No disconnected required segments.

3. **Capability checks**
   - Each node's required capabilities are satisfied by the workflow's assigned capability contract.

4. **Credential checks**
   - If the node uses secrets (API keys, tokens), they must be available and scoped.

5. **Resource budget checks**
   - Limits on number of operations, rate of requests, or cost per run.

For sensitive workflows (e.g. sending emails, deleting data), an additional **explicit user confirmation** step is required.

---

**Key Takeaways**  
- Workflows are directed graphs of typed nodes: triggers, workspace ops, AI ops, connectors, control flow, and utilities.
- AI assists workflow design by emitting JSON plans over a known node catalogâ€”never arbitrary code.
- Execution is durable (Temporal-inspired): state checkpointed in SQLite, crash-recoverable, resumable.
- A five-stage validation pipeline (schema, graph, capabilities, credentials, resource budgets) runs before any workflow executes.
- Sensitive actions (email, delete) require explicit user confirmation beyond validation.

---

### 2.6.6 AI Job Model (Global)

**Why**  
AI capabilities span multiple artefact typesâ€”docs, sheets, canvas, ASR, document ingestion, and future additions. A unified job model ensures consistent execution semantics, provenance tracking, capability enforcement, and observability across all AI work. Without this abstraction, each artefact type would define its own incompatible "job-like" concept.

**What**  
Defines the core AI job schema (identity, profiles, scope, execution plan, lifecycle, observability), the job lifecycle state machine, integration points with the workflow engine, and the profile extension pattern that artefact-specific protocols implement.

**Jargon**  
- **AI Job**: Durable, capability-scoped unit of AI work executed by the workflow engine; identified by a stable `job_id`.
- **Profile**: Artefact-specific extension of the core job model (e.g., Docs Profile, Sheets Profile, Canvas Profile, ASR Profile).
- **PlannedOperation**: Typed struct describing an intended workspace mutation (e.g., `insert_block`, `apply_formula`, `cluster_nodes`).
- **access_mode**: Consent level governing what a job may doâ€”`analysis_only`, `preview_only`, or `apply_scoped`.
- **layer_scope**: Which content layers (raw, derived, display) a job may read or write.
- **safety_mode**: Global behavior presetâ€”`strict`, `normal`, or `experimental`.
- **EntityRef**: ID-based reference into workspace artefacts (block ID, row ID, node ID, etc.).

---

#### 2.6.6.1 Definition and Core Properties

**Definition (v0.1):**

> An AI job is a durable, capability-scoped unit of AI work executed by the Handshake workflow engine. It is defined by a stable `job_id`, a governing protocol/profile, explicit workspace scope (ID-based), an execution plan, a lifecycle state, and a complete provenance/telemetry record.

**Core Properties:**

| Property | Description |
|----------|-------------|
| **Durable** | Fully serialisable; can be replayed, inspected, scheduled, or resumed without the original LLM/session |
| **Capability-scoped** | Tied to explicit capability/consent profiles; never "ambient write access" |
| **ID-based** | Operates on entity IDs (blocks, segments, rows, columns, nodes, media IDs), not raw text offsets |
| **Runtime-owned** | Scheduled and executed by the workflow engine; no separate "jobs executor" |
| **Validated** | Passes validation phases (scope, layer rules, policy, structural checks) before commit |
| **Observable** | Fully logged in Flight Recorder (inputs, outputs, status, metrics, errors) |

**Key Invariant:** LLMs are **clients** of the runtime. They can act as editors when explicitly allowed, but the **runtime and tools execute** all changes. Jobs encode this discipline in data structures, not prompt rules.

---

#### 2.6.6.2 Core Schema

The core schema defines fields that **every AI job must have**. Artefact-specific profiles extend this schema; they do not redefine it.

##### 2.6.6.2.1 Identity and Versioning

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `job_id` | JobId | âœ“ | Globally unique, stable identifier; immutable over job lifetime |
| `job_kind` | JobKind | âœ“ | Logical type (e.g., `doc_edit`, `sheet_transform`, `canvas_cluster`, `asr_transcribe`) |
| `protocol_id` | ProtocolId | âœ“ | Governing protocol/spec section (e.g., `docs_sheets_ai_v0.5`, `asr_v0.2`) |
| `task_schema_version` | SemVer | âœ“ | Schema version for migrations |
| `parent_job_id` | JobId | | For jobs spawned by other jobs |

##### 2.6.6.2.2 Profiles and Access Modes

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `profile_id` | ProfileId | âœ“ | References job profile bundling capabilities, validation, provenance, layer rules |
| `capability_profile_id` | CapabilityProfileId | âœ“ | Effective capability profile for this job |
| `access_mode` | AccessMode | âœ“ | `analysis_only` (read-only), `preview_only` (propose only), `apply_scoped` (apply to explicit entities) |
| `safety_mode` | SafetyMode | âœ“ | `strict`, `normal`, or `experimental` |
| `consent_profile_id` | ConsentProfileId | | User/topic-level consent settings |
| `topic_scope` | TopicScope | | Optional high-level topic lock for policy |

##### 2.6.6.2.3 Workspace Scope (Inputs)

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `workspace_scope` | WorkspaceScope | âœ“ | Which workspace/project/folder/document(s) job may touch |
| `entity_refs` | [EntityRef] | âœ“ | Concrete ID references (artefact_type, artefact_id, selector) |
| `context_snapshot` | ContextSnapshot | | Minimal snapshot for debugging/reproducibility |
| `layer_scope` | LayerScope | | Allowed layers: `read: [raw, derived, display]`, `write: [derived]` |

**EntityRef Structure:**
```
{
  artefact_type: "doc" | "sheet" | "canvas" | "audio" | "file" | ...
  artefact_id: UUID
  selector: BlockId | SegmentId | RowId | ColumnId | Range | NodeId | TimeRange | ...
}
```

Jobs **MUST NOT** operate on entities not explicitly listed or derivable from `entity_refs`.

##### 2.6.6.2.4 Execution Plan

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `planned_operations` | [PlannedOperation] | âœ“ | Structured plan of intended mutations |
| `execution_mode` | ExecutionMode | âœ“ | `single_step` or `multi_step` |
| `validation_plan` | ValidationPlan | | Which validators run in which phase |
| `provenance_plan` | ProvenancePlan | | How outputs map to provenance fields |

**PlannedOperation Examples:**
- `insert_block(after=BlockId, content=BlockPayload)`
- `move_block(block_id, after=BlockId)`
- `apply_formula(range=SheetRangeId, formula=FormulaSpec)`
- `cluster_nodes(node_ids=[...], label=...)`
- `transcribe_segment(media_id, time_range)`

The mechanical layer translates these into actual editor/engine calls.

##### 2.6.6.2.5 Runtime and Models

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `runtime_id` | RuntimeId | âœ“ | Logical identifier of execution runtime |
| `planner_model_id` | ModelId | | Model that generated the plan |
| `executor_model_id` | ModelId | | Model used for content generation/transformation |
| `fallback_model_id` | ModelId | | Fallback model if primary fails |
| `runtime_config` | RuntimeConfig | | Limits (token budgets, timeouts), precision settings |

##### 2.6.6.2.6 Lifecycle and Status

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `state` | JobState | âœ“ | Current lifecycle state (see (Lifecycle section)) |
| `status_reason` | StatusReason | âœ“ | Machine-readable reason for current state |
| `capability_check_result` | CapabilityCheckResult | | Result of capability validation |
| `validation_result` | ValidationResult | | Result of diff/change validation |
| `error_summary` | ErrorSummary | | Error details if failed |
| `warnings` | [Warning] | | Non-fatal issues |

##### 2.6.6.2.7 Observability and Telemetry

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `created_at` | Timestamp | âœ“ | Job creation time |
| `updated_at` | Timestamp | âœ“ | Last state change |
| `completed_at` | Timestamp | | Completion time (if terminal) |
| `metrics` | JobMetrics | âœ“ | Duration, token counts, entity counts, validator counts |
| `flight_recorder_span_id` | SpanId | âœ“ | Primary link into Flight Recorder |
| `aux_trace_ids` | [SpanId] | | Additional spans for sub-jobs |

**JobMetrics (minimal v0.1):**
- `duration_ms`
- `tokens_planner`
- `tokens_executor`
- `entities_read_count`
- `entities_written_count`
- `validators_run_count`

---

#### 2.6.6.3 Job Lifecycle Model

AI jobs follow a state machine aligned with the workflow engine's run model.

##### 2.6.6.3.1 State Machine

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                                      â”‚
                    â–¼                                                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ queued  â”‚â”€â”€â”€â–¶â”‚ running â”‚â”€â”€â”€â–¶â”‚ awaiting_validation â”‚â”€â”€â”€â–¶â”‚ awaiting_user â”‚â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚                   â”‚                      â”‚          â”‚
                    â”‚                   â”‚                      â”‚          â”‚
                    â–¼                   â–¼                      â–¼          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
              â”‚ failed  â”‚         â”‚ completed â”‚          â”‚ cancelled â”‚    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                    â–²                   â”‚                                 â”‚
                    â”‚                   â–¼                                 â”‚
                    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                    â”‚         â”‚ completed_with_issuesâ”‚                    â”‚
                    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                    â”‚                                                     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ poisoned â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 2.6.6.3.2 State Definitions

| State | Description |
|-------|-------------|
| `queued` | Job created; capability and basic schema checks passed |
| `running` | Workflow run has started; nodes executing |
| `awaiting_validation` | Mechanical operations ready; validators must approve/reject |
| `awaiting_user` | Validators require human decision (e.g., risky large diff); UI shows preview |
| `completed` | All validators passed, user accepted; changes committed, provenance written |
| `completed_with_issues` | Completed but with non-fatal warnings (e.g., partial application) |
| `failed` | Execution, validation, or capability checks failed; no changes committed |
| `cancelled` | Explicitly stopped before completion |
| `poisoned` | Known to be unsafe to retry; implementations MUST NOT auto-retry |

##### 2.6.6.3.3 Constraints

- Changes to workspace artefacts MUST only be applied when the job transitions to `completed` or `completed_with_issues`.
- Failed/cancelled/poisoned jobs MUST NOT commit edits; they may still produce logs and previews.

---

#### 2.6.6.4 Workflow Engine Integration

Every AI job MUST be representable as:

1. **A workflow definition** â€” directed graph of nodes (LLM calls, mechanical tools, validators)
2. **A workflow run** â€” concrete execution instance bound to a `workflow_run_id`

**Relationship:**
- `job_id` is the **logical** identity (stable across retries, visible to users)
- `workflow_run_id` is the **runtime** instance (one per execution attempt)

**Key Principle:** There is no separate AI jobs executor. The workflow engine (Section 2.6) is the **only** execution path for AI jobs.

**Integration Points:**

| Workflow Engine Component | AI Job Usage |
|---------------------------|--------------|
| Triggers ((Workflow & Automation Engine, Section 2.6) | Can spawn AI jobs on workspace changes, schedules, webhooks |
| Workspace Operations ((Workflow & Automation Engine, Section 2.6) | AI jobs use these nodes to read/write artefacts |
| AI Operations ((Workflow & Automation Engine, Section 2.6) | AI jobs compose these for LLM calls, embeddings, etc. |
| Validation Pipeline ((Validation section) | AI jobs run through schema, graph, capability, credential, resource checks |
| Durable Execution ((Durable Execution section) | AI job state checkpointed in SQLite; crash-recoverable |

---

#### 2.6.6.5 Provenance and Capability Integration

##### 2.6.6.5.1 Provenance

Jobs are the **primary provenance unit** for AI changes:

- Every AI-modified entity MUST be traceable back to a `job_id`
- The `provenance_plan` field declares where provenance is stored per profile:
  - **Docs Profile:** `ai_origin` per block/segment â†’ `job_id`, `planner_model_id`, `executor_model_id`, `protocol_id`
  - **Sheets Profile:** `ai_source`/override per cell â†’ same linkage
  - **Other Profiles:** Profile-defined fields with same traceability

##### 2.6.6.5.2 Capabilities

Jobs are evaluated under capability profiles:

| Field | Role |
|-------|------|
| `capability_profile_id` | Determines what the job can read/write in the workspace |
| `access_mode` | Read-only, preview-only, or scoped-apply |
| `layer_scope` | Which layers (raw/derived/display) are writable |

**Enforcement Points:**

1. **Before `queued`:** Basic capability check
2. **At `awaiting_validation`:** Full capability and policy check
3. **On commit:** Final verification that only allowed entities were modified

A job can be rejected even if syntactically well-formed if it violates capability constraints.

---

#### 2.6.6.6 Profile Extension Pattern

The core model above is **artefact-agnostic**. Concrete use cases are implemented as **profiles** that extend it.

##### 2.6.6.6.1 Profile Structure

Each profile:
- Reuses all core job fields ((Core Schema section))
- Adds profile-specific fields (e.g., `doc_id`, `sheet_id`, `canvas_id`)
- Specialises `PlannedOperation` types for its artefact
- Defines `ProvenancePlan` for its provenance fields
- Defines `ValidationPlan` for its validation rules
- Is referenced by `profile_id`

##### 2.6.6.6.2 Defined Profiles

| Profile | Section | Artefact Types | Key Operations |
|---------|---------|----------------|----------------|
| Docs & Sheets | Â§AI-Interaction-10 | Documents, Spreadsheets | `insert_block`, `apply_formula`, etc. |
| Canvas | (Future) | Visual canvases | `create_nodes`, `cluster_nodes`, `reposition_nodes` |
| ASR | (ASR Profile section) | Audio/Media | `transcribe_segment`, `align_transcript` |
| Docling | (Docling Profile section) | File ingestion | `extract_structure`, `import_blocks` |

See referenced sections for full profile definitions.

---

## 2.7 Response Behavior Contract (Diary ANS-001)

**Why**  
An assistant that just answers questions is a search engine. A governed assistant proactively shows intent understanding, risks, conflicts, better alternatives, and next steps â€” without being asked every time. This is the behavioral DNA that makes AI collaboration trustworthy.

**What**  
Defines the required behaviors for every governed response: what the assistant must show, when to show it, and how work modes affect behavior.

**Jargon**  
- **Work Mode**: The operational context (STRICT, FREE, BRAINSTORM, etc.) that determines behavior constraints.
- **Intent Confirmation**: Explicit statement of what the assistant understood from the request.
- **Operation Plan**: The what/where of planned actions before execution.
- **Proactive Surfacing**: Showing risks, conflicts, and alternatives without being asked.

---

### 2.7.1 The Behavior Contract

Every governed response MUST include these behaviors (format is implementation-specific):

```rust
// src/response/behavior_contract.rs

/// A001-81 to A001-89: Core response behaviors
#[clause("A001-81", "MUST address all explicit questions")]
pub struct ResponseBehaviorContract {
    /// What the assistant will do / answer
    pub answer: Answer,
    
    /// What the assistant understood (intent confirmation)
    pub intent: IntentConfirmation,
    
    /// Current operational context
    pub mode_context: ModeContext,
    
    /// Planned operations (what/where)
    pub operation_plan: Option<OperationPlan>,
    
    /// Proactively surfaced information
    pub proactive: ProactiveSurfacing,
    
    /// What comes next
    pub next_steps: Option<NextSteps>,
}
```

#### 2.7.1.1 Answer (Direct Response)

```rust
/// A001-81 to A001-89: Answer requirements
#[clause("A001-81", "MUST answer all explicit questions")]
#[clause("A001-82", "MUST NOT skip interrogatives")]
#[clause("A001-84", "Concise and task-focused")]
#[clause("A001-86", "MUST NOT request clarification when task is executable")]
pub struct Answer {
    /// The actual response content
    pub content: String,
    
    /// All questions from request addressed?
    pub addresses_all_questions: bool,
    
    /// At most ONE clarifying question (A001-85)
    pub clarifying_question: Option<String>,
}

impl Answer {
    #[clause("A001-85", "MAY ask at most one clarifying question")]
    pub fn validate(&self) -> Result<(), Vec<Violation>> {
        let mut violations = Vec::new();
        
        if !self.addresses_all_questions {
            violations.push(Violation::new("A001-81", "Not all questions answered"));
        }
        
        // Can't have multiple clarifying questions
        if self.clarifying_question.as_ref()
            .map(|q| q.matches('?').count() > 1)
            .unwrap_or(false) 
        {
            violations.push(Violation::new("A001-85", "Multiple clarifying questions"));
        }
        
        if violations.is_empty() { Ok(()) } else { Err(violations) }
    }
}
```

#### 2.7.1.2 Intent Confirmation

```rust
/// What the assistant understood from the request
#[clause("A001-87", "MUST reflect current understanding")]
#[clause("A001-88", "MUST NOT silently broaden scope")]
pub struct IntentConfirmation {
    /// Restated understanding of what user wants
    pub understood_request: String,
    
    /// Scope boundaries (what's included/excluded)
    pub scope: ScopeBoundary,
    
    /// Any assumptions made
    pub assumptions: Vec<Assumption>,
}

pub struct ScopeBoundary {
    pub included: Vec<String>,
    pub excluded: Vec<String>,
}

pub struct Assumption {
    pub what: String,
    pub why: String,
    pub impact_if_wrong: String,
}
```

#### 2.7.1.3 Mode Context

```rust
/// A001-48: Work Modes
#[clause("A001-48", "WORK MODES determine behavior constraints")]
pub enum WorkMode {
    /// A001-170: Full ceremony, one op per step, deterministic
    Strict,
    
    /// A001-172: Reduced ceremony, scoped edits allowed
    Free,
    
    /// A001-173: Minimal ceremony, rapid completion
    Fasttrack,
    
    /// A001-174: No frame, no edits, exploration only
    Brainstorm,
    
    /// A001-175: Factual only, no inference
    Data,
}

pub struct ModeContext {
    pub mode: WorkMode,
    pub determinism: bool,
    pub can_edit: bool,
    pub layer: Layer,
}

impl WorkMode {
    /// A001-74: Determinism requirements
    #[clause("A001-74", "Determinism ON for all modes except BRAINSTORM")]
    pub fn requires_determinism(&self) -> bool {
        !matches!(self, WorkMode::Brainstorm)
    }
    
    /// What operations are allowed in this mode
    pub fn allowed_operations(&self) -> AllowedOperations {
        match self {
            WorkMode::Strict => AllowedOperations::ANALYSIS_ONLY,
            WorkMode::Free => AllowedOperations::SCOPED_EDITS,
            WorkMode::Fasttrack => AllowedOperations::SCOPED_EDITS,
            WorkMode::Brainstorm => AllowedOperations::NONE,
            WorkMode::Data => AllowedOperations::ANALYSIS_ONLY,
        }
    }
}
```

#### 2.7.1.4 Operation Plan (What/Where)

```rust
/// Shows planned operations BEFORE execution
#[clause("A001-49", "Operation plan shows what/where")]
pub struct OperationPlan {
    /// What will be done
    pub operations: Vec<PlannedOperation>,
    
    /// Where (affected entities/locations)
    pub affected: Vec<AffectedEntity>,
    
    /// Validation status
    pub validation: ValidationStatus,
    
    /// Can user abort?
    pub abortable: bool,
}

pub struct PlannedOperation {
    pub operation_type: OperationType,
    pub target: String,
    pub description: String,
    pub reversible: bool,
}

pub struct AffectedEntity {
    pub entity_id: String,
    pub entity_type: String,
    pub change_type: ChangeType,
}

pub enum ChangeType {
    Create,
    Modify,
    Delete,
    Reference,
}
```

#### 2.7.1.5 Proactive Surfacing

This is the core differentiator â€” showing risks, conflicts, and better ideas without being asked:

```rust
/// Proactively surfaced information
pub struct ProactiveSurfacing {
    /// Risks identified (A001: Risks & Edge Cases)
    pub risks: Vec<Risk>,
    
    /// Conflicts detected (A001: Conflicts)
    pub conflicts: Vec<Conflict>,
    
    /// Better alternatives (A001: Improve / Bulletproof Plan)
    pub alternatives: Vec<Alternative>,
    
    /// Key findings from analysis
    pub findings: Vec<Finding>,
}

/// A risk the user should know about
pub struct Risk {
    pub description: String,
    pub severity: Severity,
    pub mitigation: Option<String>,
    pub affects: Vec<String>,
}

/// A conflict in the request or data
pub struct Conflict {
    pub description: String,
    pub between: (String, String),
    pub resolution_options: Vec<String>,
    pub recommended: Option<String>,
}

/// A better way to accomplish the goal
pub struct Alternative {
    pub description: String,
    pub why_better: String,
    pub tradeoffs: Vec<String>,
    pub effort_delta: EffortDelta,
}

pub enum EffortDelta {
    Less,
    Same,
    More,
    Unknown,
}

/// Something discovered during analysis
pub struct Finding {
    pub what: String,
    pub significance: Significance,
    pub action_needed: bool,
}
```

#### 2.7.1.6 Next Steps

```rust
/// What comes after this response
pub struct NextSteps {
    /// Immediate next actions
    pub immediate: Vec<NextAction>,
    
    /// Future considerations
    pub future: Vec<String>,
    
    /// Blockers or dependencies
    pub blockers: Vec<String>,
}

pub struct NextAction {
    pub description: String,
    pub who: Actor,
    pub urgency: Urgency,
}

pub enum Actor {
    User,
    Assistant,
    Either,
    External(String),
}
```

---

### 2.7.2 Behavior by Mode

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BEHAVIOR BY WORK MODE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Behavior     â”‚ STRICT   â”‚ FREE     â”‚ FASTTRACK â”‚BRAINSTORMâ”‚ DATA  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Answer       â”‚ Required â”‚ Required â”‚ Required  â”‚ Requiredâ”‚Requiredâ”‚
â”‚ Intent       â”‚ Full     â”‚ Brief    â”‚ Minimal   â”‚ None    â”‚ Brief  â”‚
â”‚ Op Plan      â”‚ Full     â”‚ Brief    â”‚ Minimal   â”‚ None    â”‚ N/A    â”‚
â”‚ Risks        â”‚ Full     â”‚ Relevant â”‚ Critical  â”‚ None    â”‚Relevantâ”‚
â”‚ Conflicts    â”‚ Full     â”‚ Relevant â”‚ Critical  â”‚ None    â”‚Relevantâ”‚
â”‚ Alternatives â”‚ Full     â”‚ If betterâ”‚ None      â”‚ All     â”‚ None   â”‚
â”‚ Next Steps   â”‚ Full     â”‚ Brief    â”‚ Minimal   â”‚ None    â”‚ Brief  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Determinism  â”‚ ON       â”‚ ON       â”‚ ON        â”‚ OFF     â”‚ ON     â”‚
â”‚ Can Edit     â”‚ No       â”‚ Yes      â”‚ Yes       â”‚ No      â”‚ No     â”‚
â”‚ Ceremony     â”‚ High     â”‚ Medium   â”‚ Low       â”‚ None    â”‚ Medium â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.7.3 Prohibitions

```rust
// src/response/prohibitions.rs

/// A001-200 to A001-219: Hard prohibitions
pub struct ResponseProhibitions;

impl ResponseProhibitions {
    pub fn validate(&self, response: &ResponseBehaviorContract, ctx: &Context) -> Result<()> {
        // A001-205: No multi-operation unless batching permitted
        if let Some(ref plan) = response.operation_plan {
            if !ctx.batching_allowed && plan.operations.len() > 1 {
                return Err(Violation::new("A001-205", "Multi-op without batching"));
            }
        }
        
        // A001-208: No silent corrections
        if response.has_silent_corrections() {
            return Err(Violation::new("A001-208", "Silent corrections"));
        }
        
        // A001-209: No creative inference in STRICT/DATA
        if matches!(ctx.mode, WorkMode::Strict | WorkMode::Data) {
            if response.has_creative_inference() {
                return Err(Violation::new("A001-209", "Creative inference in restricted mode"));
            }
        }
        
        // A001-212: No silent scope changes
        if response.scope_changed() && !response.scope_change_acknowledged() {
            return Err(Violation::new("A001-212", "Silent scope change"));
        }
        
        // A001-219: No determinism degradation
        if ctx.mode.requires_determinism() && response.determinism_degraded() {
            return Err(Violation::new("A001-219", "Determinism degraded"));
        }
        
        Ok(())
    }
}
```

---

### 2.7.4 Integration with AI Job Model

The Response Behavior Contract integrates with the existing AI Job Model:

```rust
// src/jobs/job_response.rs

impl Job {
    /// Generate response with behavior contract
    pub fn generate_response(&self, result: JobResult) -> ResponseBehaviorContract {
        ResponseBehaviorContract {
            answer: self.format_answer(&result),
            intent: self.confirm_intent(),
            mode_context: self.mode_context(),
            operation_plan: self.get_operation_plan(),
            proactive: ProactiveSurfacing {
                risks: self.analyze_risks(),
                conflicts: self.detect_conflicts(),
                alternatives: self.suggest_alternatives(),
                findings: self.extract_findings(),
            },
            next_steps: self.determine_next_steps(),
        }
    }
}
```

---

**Key Takeaways**
- **Every governed response** confirms intent, shows risks, flags conflicts, offers alternatives
- **Proactive surfacing** is the differentiator â€” user doesn't have to ask for risks/conflicts/ideas
- **Work modes** control ceremony level but NOT the presence of core behaviors
- **Format is flexible** â€” the 14-section frame may be refactored, but functionality persists
- **Prohibitions are hard** â€” no silent corrections, no scope creep, no determinism loss

---

### 2.7.5 Validation Gates (from Diary COR-701)

**Why**  
Every edit operation must pass through validation gates before execution. This prevents invalid states and ensures deterministic behavior.

**What**  
Defines the 11 gates that every edit operation must pass, with clause-level provenance from Diary governance.

**Jargon**  
- **Gate**: A validation checkpoint that must pass before an operation proceeds.
- **Manifest**: A declaration of what an edit will do, validated before execution.
- **Step**: The smallest atomic edit unit.

---

#### 2.7.5.1 Gate Pipeline Overview

All edit operations pass through this pipeline:

```
PlannedOperation
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  G-SCOPE        â”‚ â†’ Is edit within permitted scope?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-MANIFEST     â”‚ â†’ Does manifest declare all changes?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-INTEGRITY    â”‚ â†’ Will this preserve document integrity?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-LAYER        â”‚ â†’ Is target layer writable?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CONFLICT     â”‚ â†’ Any concurrent edit conflicts?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-SCHEMA       â”‚ â†’ Does output match expected schema?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CONSENT      â”‚ â†’ Are consent requirements met?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CAPABILITY   â”‚ â†’ Does agent have required capabilities?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-DETERMINISM  â”‚ â†’ Is operation deterministic?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-REVERSIBLE   â”‚ â†’ Can this be undone?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-AUDIT        â”‚ â†’ Is audit trail complete?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
   Execute
```

#### 2.7.5.2 Gate Implementations

```rust
// src/validation/gates.rs

/// Gate trait from Diary COR-701
#[clause("C701-50", "All 11 gates as trait")]
pub trait Gate {
    fn name(&self) -> &'static str;
    fn check(&self, op: &PlannedOperation, ctx: &Context) -> GateResult;
}

pub enum GateResult {
    Pass,
    Fail { reason: String, clause: &'static str },
    Defer { to_gate: &'static str },
}

/// C701-51: Scope gate
pub struct ScopeGate;
impl Gate for ScopeGate {
    fn name(&self) -> &'static str { "G-SCOPE" }
    fn check(&self, op: &PlannedOperation, ctx: &Context) -> GateResult {
        // Verify edit is within declared scope
        if op.affected_blocks().iter().all(|b| ctx.scope.contains(b)) {
            GateResult::Pass
        } else {
            GateResult::Fail {
                reason: "Edit affects blocks outside declared scope".into(),
                clause: "C701-51",
            }
        }
    }
}

/// C701-52: Manifest gate
pub struct ManifestGate;
impl Gate for ManifestGate {
    fn name(&self) -> &'static str { "G-MANIFEST" }
    fn check(&self, op: &PlannedOperation, ctx: &Context) -> GateResult {
        // Verify manifest declares all changes
        let declared = op.manifest.declared_changes();
        let actual = op.compute_changes();
        if declared == actual {
            GateResult::Pass
        } else {
            GateResult::Fail {
                reason: "Manifest does not match actual changes".into(),
                clause: "C701-52",
            }
        }
    }
}

/// C701-53: Layer gate  
pub struct LayerGate;
impl Gate for LayerGate {
    fn name(&self) -> &'static str { "G-LAYER" }
    fn check(&self, op: &PlannedOperation, ctx: &Context) -> GateResult {
        // L1 = immutable, L2 = promotion only, L3 = writable
        match (op.target_layer, op.operation_type) {
            (Layer::L1, _) => GateResult::Fail {
                reason: "L1 content is immutable".into(),
                clause: "C701-53",
            },
            (Layer::L2, OpType::Write) => GateResult::Fail {
                reason: "L2 only accepts promotions".into(),
                clause: "C701-53",
            },
            _ => GateResult::Pass,
        }
    }
}

// ... remaining 8 gates follow same pattern
```

#### 2.7.5.3 Gate Pipeline Execution

```rust
/// C701-60: Gate pipeline executor
#[clause("C701-60", "Pipeline executes all gates in order")]
pub struct GatePipeline {
    gates: Vec<Box<dyn Gate>>,
}

impl GatePipeline {
    pub fn new() -> Self {
        Self {
            gates: vec![
                Box::new(ScopeGate),
                Box::new(ManifestGate),
                Box::new(IntegrityGate),
                Box::new(LayerGate),
                Box::new(ConflictGate),
                Box::new(SchemaGate),
                Box::new(ConsentGate),
                Box::new(CapabilityGate),
                Box::new(DeterminismGate),
                Box::new(ReversibleGate),
                Box::new(AuditGate),
            ],
        }
    }

    /// Run all gates, fail fast on first failure
    pub fn validate(&self, op: &PlannedOperation, ctx: &Context) -> Result<(), GateFailure> {
        for gate in &self.gates {
            match gate.check(op, ctx) {
                GateResult::Pass => continue,
                GateResult::Fail { reason, clause } => {
                    return Err(GateFailure {
                        gate: gate.name(),
                        reason,
                        clause,
                    });
                }
                GateResult::Defer { to_gate } => {
                    // Handle deferred checks
                }
            }
        }
        Ok(())
    }
}
```


#### 2.7.5.4 Pre-Commit Quality Gate (FIT-100)

FIT-100 is a mandatory quality gate that runs before any commit to governance or corpus content.

**Why**  
Catching issues before commit prevents bad content from entering the system. The gate integrates with COR-701's validation pipeline.

**What**  
Defines the FIT analysis gate: what must pass, how waivers work, and integration with COR-701.

```rust
// src/validators/fit100.rs

/// FIT-100: Pre-commit quality gate
#[clause("FIT-100", "Mandatory pre-commit quality gate for L3 edits")]
pub struct FitPreCommitGate {
    fit_runner: FitRunner,
    waiver_registry: WaiverRegistry,
}

impl Gate for FitPreCommitGate {
    fn name(&self) -> &'static str { "G-FIT" }
    
    fn check(&self, op: &PlannedOperation, ctx: &Context) -> GateResult {
        let findings = self.fit_runner.analyze(op, ctx);
        
        // Must have run FIT
        if !findings.ran {
            return GateResult::Fail {
                reason: "FIT analysis was not run".into(),
                clause: "FIT-100",
            };
        }
        
        // Check for unresolved FAIL-class findings
        let unresolved: Vec<_> = findings.items
            .iter()
            .filter(|f| f.severity == Severity::Fail && !f.waived)
            .collect();
        
        if !unresolved.is_empty() {
            return GateResult::Fail {
                reason: format!("{} unresolved FAIL findings", unresolved.len()),
                clause: "FIT-100",
            };
        }
        
        GateResult::Pass
    }
}

/// FIT-100: Waiver handling
impl FitPreCommitGate {
    #[clause("FIT-100", "Waivers require L2+ authority with audit note")]
    pub fn apply_waiver(&mut self, finding_id: &str, waiver: Waiver) -> Result<()> {
        // Waivers require L2+ authority
        if waiver.authority_level < Layer::L2 {
            return Err(WaiverError::InsufficientAuthority {
                required: Layer::L2,
                provided: waiver.authority_level,
                clause: "FIT-100",
            });
        }
        
        // Must have audit note
        if waiver.audit_note.is_empty() {
            return Err(WaiverError::MissingAuditNote { clause: "FIT-100" });
        }
        
        self.waiver_registry.register(finding_id, waiver)?;
        Ok(())
    }
}
```

#### 2.7.5.5 Extended Gate Pipeline

With FIT-100, the complete gate pipeline becomes:

```
PlannedOperation
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  G-SCOPE        â”‚  (C701-51)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-MANIFEST     â”‚  (C701-52)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-INTEGRITY    â”‚  (C701-53)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-LAYER        â”‚  (C701-53)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CONFLICT     â”‚  (C701-54)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-SCHEMA       â”‚  (C701-55)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CONSENT      â”‚  (C701-56)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-CAPABILITY   â”‚  (C701-57)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-DETERMINISM  â”‚  (C701-58)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-REVERSIBLE   â”‚  (C701-59)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-AUDIT        â”‚  (C701-60)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  G-FIT          â”‚  (FIT-100) â† NEW
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
   Execute
```

---

## 2.8 Governance Runtime (Diary Parts 1-2)

**Why**  
The Diary's Bootloader and Execution Charter define how governance activates and behaves at runtime. These become the lifecycle rules and capability assignments in Handshake.

**What**  
Defines runtime behavior (when governance activates, what governs what), LAW vs behavior separation, activation triggers, capability precedence, and all clause-level implementation details.

**Jargon**  
- **LAW**: User-designated constraints that define what's allowed/required â€” validators enforce LAW.
- **Behavior**: How the system acts (logging, defaults, formatting) â€” runtime implements behavior.
- **Activation**: When governance "wakes up" and starts governing work.
- **Capability**: Scoped permission assigned at activation time.
- **Charter**: The rules governing when and how governance activates.
- **RID**: Reference Identifier - unique identifier for a governed entity.
- **Drift**: State where system detects deviation from expected governance.

---

### 2.8.1 Bootloader: Runtime Behavior (Part 1)

The Bootloader defines **how** the system behaves. These become runtime hooks, lifecycle rules, and Flight Recorder behavior.

---

#### 2.8.1.1 Scope & Goals (BL-10 to BL-15)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-10 | Governs micro-logger sessions for AI work | `GovernedJob` |
| BL-11 | Project-agnostic config | `ProjectConfig` |
| BL-12 | Pluggable validators | `ValidatorRegistry` |
| BL-13 | LAW enforcement | `LawEnforcer` |
| BL-14 | Runtime + Flight Recorder always on | `Runtime` |
| BL-15 | All operations traceable | `TraceContext` |

```rust
// src/core/runtime.rs

/// BL-10 to BL-15: Core runtime structure
#[clause("BL-14", "Runtime + Flight Recorder always on")]
pub struct Runtime {
    /// BL-11: Project-agnostic config
    pub config: ProjectConfig,
    
    /// BL-12: Pluggable validators
    pub validator_registry: ValidatorRegistry,
    
    /// BL-14: Always-on logging
    pub flight_recorder: FlightRecorder,
    
    /// BL-13: LAW enforcement
    pub law_enforcer: LawEnforcer,
    
    /// BL-15: Trace context for all operations
    pub trace_context: TraceContext,
}

impl Runtime {
    /// BL-10: All AI jobs are governed
    #[clause("BL-10", "Governs micro-logger sessions")]
    pub fn start_job(&self, job: Job) -> GovernedJob {
        GovernedJob {
            inner: job,
            governed: true,  // Always true
            recorder: self.flight_recorder.new_session(),
            trace: self.trace_context.new_span("job"),
        }
    }
}
```

---

#### 2.8.1.2 LAW vs Behaviour (BL-20 to BL-25)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-20 | LAW is user-designated constraints | `LawSource` |
| BL-21 | LAW wins on allowed/required decisions | Precedence rules |
| BL-22 | Behavior is how system acts | `BehaviorConfig` |
| BL-23 | LAW can override behavior defaults | `with_law_overrides()` |
| BL-24 | Behavior includes logging, formatting | Config fields |
| BL-25 | LAW is immutable during session | Freeze semantics |

```rust
// src/core/governance.rs

/// BL-20: LAW = user-designated constraints
#[clause("BL-20", "LAW is user-designated constraints")]
pub enum LawSource {
    /// User-defined rules (highest precedence)
    UserConfig(ConfigPath),
    /// Project-specific rules
    ProjectRules(ProjectPath),
    /// Default system rules (lowest precedence)
    SystemDefaults,
}

/// BL-22: Behavior = runtime implementation
#[clause("BL-22", "Behavior is how system acts")]
pub struct BehaviorConfig {
    /// BL-24: Logging verbosity
    pub log_level: LogLevel,
    /// BL-24: Default formatting rules
    pub format_defaults: FormatDefaults,
    /// BL-24: Flight recorder settings
    pub flight_recorder: FlightRecorderConfig,
}

impl BehaviorConfig {
    /// BL-23: LAW overrides behavior
    #[clause("BL-23", "LAW can override behavior")]
    pub fn with_law_overrides(mut self, law: &LawSource) -> Self {
        if let Some(overrides) = law.behavior_overrides() {
            self.apply_overrides(overrides);
        }
        self
    }
}

/// BL-25: LAW is frozen once session starts
pub struct FrozenLaw {
    inner: LawSource,
    frozen_at: DateTime<Utc>,
}
```

---

#### 2.8.1.3 Behaviour Overlays (BL-30 to BL-94)

##### Challenge-First (BL-30 to BL-32)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-30 | Challenge hidden assumptions before planning | `ChallengeFirstValidator` |
| BL-31 | Surface assumptions explicitly | `AssumptionSurface` |
| BL-32 | User confirms or corrects assumptions | `AssumptionConfirmation` |

```rust
// src/ai/challenge_first.rs

/// BL-30: Challenge-first behavior
#[clause("BL-30", "Challenge hidden assumptions")]
pub struct ChallengeFirstValidator;

impl ChallengeFirstValidator {
    /// BL-30: Before planning, surface assumptions
    pub fn validate(&self, job: &Job, context: &Context) -> ValidationResult {
        let assumptions = self.detect_assumptions(job);
        
        if !assumptions.is_empty() && !job.assumptions_acknowledged {
            ValidationResult::Warning {
                code: "BL-30",
                message: "Hidden assumptions detected",
                assumptions,
            }
        } else {
            ValidationResult::Pass
        }
    }
    
    /// BL-31: Surface detected assumptions
    fn detect_assumptions(&self, job: &Job) -> Vec<Assumption> {
        let mut assumptions = Vec::new();
        
        // Check for implicit file assumptions
        if job.references_files() && !job.file_existence_verified {
            assumptions.push(Assumption::FileExists);
        }
        
        // Check for implicit state assumptions
        if job.depends_on_state() && !job.state_verified {
            assumptions.push(Assumption::StateValid);
        }
        
        assumptions
    }
}
```

##### Refine-3 Loop (BL-40 to BL-45)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-40 | Complex tasks get 3 refinement passes | `Refine3Loop` |
| BL-41 | Each pass narrows scope | Scope reduction |
| BL-42 | User can skip refinement | `skip_refinement` flag |
| BL-43 | Refinement logged to Flight Recorder | Audit trail |
| BL-44 | Final pass produces actionable plan | `ActionablePlan` |
| BL-45 | Refinement timeout configurable | `refinement_timeout` |

```rust
// src/ai/refine_loop.rs

/// BL-40 to BL-45: Three-pass refinement
#[clause("BL-40", "Complex tasks get 3 refinement passes")]
pub struct Refine3Loop {
    pub max_passes: usize,  // Default: 3
    pub timeout: Duration,  // BL-45
}

impl Refine3Loop {
    pub fn refine(&self, task: &Task, context: &mut Context) -> Result<ActionablePlan> {
        let mut current = task.initial_scope();
        
        for pass in 0..self.max_passes {
            // BL-43: Log each pass
            context.flight_recorder.log_refinement(pass, &current);
            
            // BL-41: Narrow scope each pass
            current = self.narrow_scope(current, context)?;
            
            // BL-42: Check for skip request
            if context.skip_refinement_requested() {
                break;
            }
        }
        
        // BL-44: Produce actionable plan
        Ok(ActionablePlan::from_scope(current))
    }
}
```

##### Dual-Path (BL-50 to BL-52)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-50 | Offer alternative approaches when uncertainty high | `DualPathGenerator` |
| BL-51 | User selects preferred path | Path selection |
| BL-52 | Non-selected paths logged for reference | Audit trail |

```rust
// src/ai/dual_path.rs

/// BL-50: Generate alternative approaches
#[clause("BL-50", "Offer alternatives when uncertain")]
pub struct DualPathGenerator;

impl DualPathGenerator {
    pub fn generate(&self, task: &Task, context: &Context) -> Vec<ApproachPath> {
        if context.uncertainty_score() > 0.7 {
            vec![
                self.conservative_path(task),
                self.aggressive_path(task),
            ]
        } else {
            vec![self.default_path(task)]
        }
    }
}
```

##### Quality Rubric (BL-60 to BL-61)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-60 | Every output scored against quality rubric | `QualityRubric` |
| BL-61 | Below-threshold outputs flagged | Quality gate |

```rust
// src/ai/quality.rs

/// BL-60: Quality scoring
#[clause("BL-60", "Output scored against rubric")]
pub struct QualityRubric {
    pub min_threshold: f32,
    pub criteria: Vec<QualityCriterion>,
}

impl QualityRubric {
    /// BL-61: Check if output meets threshold
    pub fn evaluate(&self, output: &Output) -> QualityResult {
        let score = self.criteria.iter()
            .map(|c| c.score(output))
            .sum::<f32>() / self.criteria.len() as f32;
        
        if score < self.min_threshold {
            QualityResult::BelowThreshold { score, threshold: self.min_threshold }
        } else {
            QualityResult::Pass { score }
        }
    }
}
```

##### Continuity Check (BL-70 to BL-71)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-70 | Verify session continuity on resume | `ContinuityChecker` |
| BL-71 | Detect context loss and recover | Recovery protocol |

##### Answer Structure (BL-80 to BL-82)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-80 | Answers follow consistent structure | `AnswerStructure` |
| BL-81 | Structure includes rationale | Rationale field |
| BL-82 | Structure includes confidence | Confidence score |

---

#### 2.8.1.4 Flight Recorder Contract (BL-100 to BL-115)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-100 | Flight Recorder captures all state transitions | `FlightRecorder` |
| BL-101 | Entries are immutable once written | Append-only |
| BL-102 | Each entry has monotonic timestamp | `entry_timestamp` |
| BL-103 | Each entry has unique ID | `EntryId` |
| BL-104 | Flight Recorder is append-only | No delete/update |
| BL-105 | Recorder survives process restart | Persistence |
| BL-110 | Entry includes operation type | `operation_type` |
| BL-111 | Entry includes affected entities | `affected_entities` |
| BL-112 | Entry includes before/after state | State diff |
| BL-113 | Entry includes user context | `user_context` |
| BL-114 | Entry includes AI context | `ai_context` |
| BL-115 | Entries queryable by time range | Time-based query |

```rust
// src/observability/flight_recorder.rs

/// BL-100 to BL-115: Flight Recorder
#[clause("BL-100", "Captures all state transitions")]
pub struct FlightRecorder {
    storage: AppendOnlyStorage,  // BL-104
    sequence: AtomicU64,         // BL-103
}

/// BL-100: Flight Recorder entry
#[derive(Debug, Clone, Serialize)]
pub struct FlightRecorderEntry {
    /// BL-103: Unique entry ID
    pub id: EntryId,
    /// BL-102: Monotonic timestamp
    pub timestamp: DateTime<Utc>,
    /// BL-110: Operation type
    pub operation_type: OperationType,
    /// BL-111: Affected entities
    pub affected_entities: Vec<EntityRef>,
    /// BL-112: State before operation
    pub before_state: Option<StateSnapshot>,
    /// BL-112: State after operation
    pub after_state: StateSnapshot,
    /// BL-113: User context
    pub user_context: UserContext,
    /// BL-114: AI context
    pub ai_context: AiContext,
}

impl FlightRecorder {
    /// BL-104: Append-only - no delete or update methods exist
    #[clause("BL-104", "Append-only")]
    pub fn append(&self, entry: FlightRecorderEntry) -> Result<EntryId> {
        let id = EntryId::new(self.sequence.fetch_add(1, Ordering::SeqCst));
        self.storage.append(&entry)?;
        Ok(id)
    }
    
    /// BL-115: Query by time range
    #[clause("BL-115", "Time-based query")]
    pub fn query_range(&self, start: DateTime<Utc>, end: DateTime<Utc>) -> Vec<FlightRecorderEntry> {
        self.storage.query_by_time(start, end)
    }
    
    // Note: No delete() or update() methods - BL-104 compliance
}
```

---

#### 2.8.1.5 Log Entry Schema (BL-190 to BL-196)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-190 | Log entries follow schema | `LogEntrySchema` |
| BL-191 | Schema version tracked | `schema_version` |
| BL-192 | Unknown fields rejected | `deny_unknown_fields` |
| BL-193 | Required fields enforced | Validation |
| BL-194 | Entry types enumerated | `EntryType` enum |
| BL-195 | Timestamps in UTC | `DateTime<Utc>` |
| BL-196 | IDs are UUIDs | `Uuid` |

```rust
// src/observability/log_schema.rs

/// BL-190 to BL-196: Log entry schema
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(deny_unknown_fields)]  // BL-192
pub struct LogEntry {
    /// BL-196: UUID identifier
    pub id: Uuid,
    /// BL-195: UTC timestamp
    pub timestamp: DateTime<Utc>,
    /// BL-191: Schema version
    pub schema_version: SchemaVersion,
    /// BL-194: Entry type
    pub entry_type: EntryType,
    /// BL-193: Required payload
    pub payload: LogPayload,
}

/// BL-194: Enumerated entry types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EntryType {
    SessionStart,
    SessionEnd,
    OperationStart,
    OperationComplete,
    OperationFailed,
    StateChange,
    ValidationResult,
    UserAction,
    AiDecision,
}
```

---

#### 2.8.1.6 Deterministic Edits (BL-270 to BL-279)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-270 | PlannedOperation for all edits | `PlannedOperation` |
| BL-271 | Operation has entity reference | `entity_ref` |
| BL-272 | No unanchored operations | Validation |
| BL-273 | Location selector required | `LocationSelector` |
| BL-274 | Deletions explicit with before | `before` field |
| BL-275 | After content required | `after` field |
| BL-276 | Reason documented | `reason` field |
| BL-277 | Risks enumerated | `risks` field |
| BL-278 | Operation logged before execution | Pre-logging |
| BL-279 | Result logged after execution | Post-logging |

```rust
// src/edit/planned_operation.rs

/// BL-270 to BL-279: PlannedOperation
#[clause("BL-270", "All edits via PlannedOperation")]
#[derive(Debug, Clone)]
pub struct PlannedOperation {
    /// BL-271: Entity being modified
    pub entity_ref: EntityRef,
    /// BL-273: Where in entity
    pub location: LocationSelector,
    /// BL-274: Content before (required for delete)
    pub before: Option<ContentSnapshot>,
    /// BL-275: Content after
    pub after: ContentSnapshot,
    /// BL-276: Why this edit
    pub reason: String,
    /// BL-277: Known risks
    pub risks: Vec<Risk>,
    /// Operation type
    pub operation_type: OperationType,
}

impl PlannedOperation {
    /// BL-272: Validate operation is anchored
    pub fn validate(&self) -> Result<()> {
        if self.entity_ref.is_empty() {
            return Err(Error::UnanchoredOperation { clause: "BL-272" });
        }
        
        // BL-274: Deletions require before content
        if self.operation_type == OperationType::Delete && self.before.is_none() {
            return Err(Error::DeleteWithoutBefore { clause: "BL-274" });
        }
        
        Ok(())
    }
}
```

---

#### 2.8.1.7 Prohibited Behaviours (BL-260 to BL-267)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-260 | No silent modifications | `SilentModificationGuard` |
| BL-261 | No undocumented side effects | Side effect tracking |
| BL-262 | No bypassing validation | Validation mandatory |
| BL-263 | No fabricating references | Reference verification |
| BL-264 | No ignoring errors | Error handling required |
| BL-265 | No optimistic assumptions | Explicit verification |
| BL-266 | No scope creep | Scope boundary enforcement |
| BL-267 | No unlogged operations | Logging mandatory |

```rust
// src/core/prohibitions.rs

/// BL-260 to BL-267: Prohibited behaviors
pub struct ProhibitionEnforcer;

impl ProhibitionEnforcer {
    /// BL-260: Detect silent modifications
    pub fn check_silent_modification(&self, before: &State, after: &State, logged: &[LogEntry]) -> Result<()> {
        let changes = before.diff(after);
        for change in changes {
            if !logged.iter().any(|e| e.covers(&change)) {
                return Err(ProhibitionViolation::SilentModification { 
                    clause: "BL-260",
                    change 
                });
            }
        }
        Ok(())
    }
    
    /// BL-263: Verify reference exists
    pub fn verify_reference(&self, reference: &EntityRef, index: &Index) -> Result<()> {
        if !index.contains(reference) {
            return Err(ProhibitionViolation::FabricatedReference {
                clause: "BL-263",
                reference: reference.clone()
            });
        }
        Ok(())
    }
}
```

---

#### 2.8.1.8 Startup Sequence (BL-120 to BL-141)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-120 | Startup is atomic | Transaction semantics |
| BL-121 | Load config first | Config loading |
| BL-122 | Validate config | Config validation |
| BL-123 | Initialize Flight Recorder | Recorder init |
| BL-130 | Load LAW definitions | LAW loading |
| BL-131 | Freeze LAW for session | LAW freezing |
| BL-140 | Activate governance | Governance activation |
| BL-141 | Log startup complete | Startup logging |

```rust
// src/core/startup.rs

/// BL-120 to BL-141: Startup sequence
pub struct StartupSequence;

impl StartupSequence {
    /// BL-120: Atomic startup
    #[clause("BL-120", "Startup is atomic")]
    pub fn execute(&self) -> Result<Runtime> {
        // BL-121: Load config first
        let config = self.load_config()?;
        
        // BL-122: Validate config
        self.validate_config(&config)?;
        
        // BL-123: Initialize Flight Recorder
        let flight_recorder = FlightRecorder::new(&config.recorder_config)?;
        
        // BL-130: Load LAW definitions
        let law = self.load_law(&config)?;
        
        // BL-131: Freeze LAW
        let frozen_law = FrozenLaw::freeze(law);
        
        // BL-140: Activate governance
        let runtime = Runtime::new(config, flight_recorder, frozen_law)?;
        
        // BL-141: Log startup complete
        runtime.flight_recorder.log_startup_complete();
        
        Ok(runtime)
    }
}
```

---

#### 2.8.1.9 Dead Ends and Handover (BL-240 to BL-253)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-240 | Detect dead ends explicitly | `DeadEndDetector` |
| BL-241 | Dead end triggers handover | Handover protocol |
| BL-242 | Handover includes full context | Context bundle |
| BL-243 | Handover logged | Audit trail |
| BL-250 | User can resolve dead end | User intervention |
| BL-251 | Dead end timeout configurable | `dead_end_timeout` |
| BL-252 | Partial progress preserved | Progress snapshot |
| BL-253 | Handover target specified | Target specification |

```rust
// src/ai/dead_end.rs

/// BL-240 to BL-253: Dead end handling
#[clause("BL-240", "Detect dead ends")]
pub struct DeadEndDetector {
    pub timeout: Duration,  // BL-251
}

impl DeadEndDetector {
    pub fn check(&self, context: &Context) -> Option<DeadEnd> {
        if context.no_progress_for(self.timeout) {
            Some(DeadEnd {
                reason: context.blocked_reason(),
                progress: context.progress_snapshot(),  // BL-252
            })
        } else {
            None
        }
    }
}

/// BL-241: Handover on dead end
pub struct Handover {
    /// BL-242: Full context
    pub context: ContextBundle,
    /// BL-253: Target
    pub target: HandoverTarget,
    /// BL-243: Logged
    pub logged_at: DateTime<Utc>,
}
```

---

#### 2.8.1.10 Feasibility and File Awareness (BL-290 to BL-304)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-290 | Check feasibility before commit | `FeasibilityChecker` |
| BL-291 | File existence verified | File check |
| BL-292 | Permissions verified | Permission check |
| BL-293 | Space availability verified | Space check |
| BL-294 | Dependencies verified | Dependency check |
| BL-300 | File operations atomic | Transaction semantics |
| BL-301 | Backup before destructive ops | Backup creation |
| BL-302 | File locks respected | Lock checking |
| BL-303 | Path canonicalization required | Path normalization |
| BL-304 | No operations on temp files | Temp file guard |

```rust
// src/file/feasibility.rs

/// BL-290 to BL-304: Feasibility checking
#[clause("BL-290", "Check feasibility")]
pub struct FeasibilityChecker;

impl FeasibilityChecker {
    pub fn check(&self, operation: &FileOperation) -> Result<FeasibilityResult> {
        // BL-291: File exists
        if !operation.target.exists() && operation.requires_existing() {
            return Ok(FeasibilityResult::Blocked { 
                reason: "File does not exist",
                clause: "BL-291"
            });
        }
        
        // BL-292: Permissions
        if !operation.target.is_writable() && operation.is_write() {
            return Ok(FeasibilityResult::Blocked {
                reason: "No write permission",
                clause: "BL-292"
            });
        }
        
        // BL-302: Lock check
        if operation.target.is_locked() {
            return Ok(FeasibilityResult::Blocked {
                reason: "File is locked",
                clause: "BL-302"
            });
        }
        
        // BL-303: Canonicalize path
        let canonical = operation.target.canonicalize()?;
        
        // BL-304: Not a temp file
        if canonical.is_temp() {
            return Ok(FeasibilityResult::Blocked {
                reason: "Cannot operate on temp files",
                clause: "BL-304"
            });
        }
        
        Ok(FeasibilityResult::Feasible)
    }
}
```

---

#### 2.8.1.11 Assumptions and Stateful Edits (BL-310 to BL-323)

| Clause | Text | Implementation |
|--------|------|----------------|
| BL-310 | Assumptions must be explicit | `ExplicitAssumption` |
| BL-311 | Implicit assumptions surfaced | Assumption detection |
| BL-312 | User confirms assumptions | Confirmation flow |
| BL-313 | Unconfirmed assumptions block | Blocking behavior |
| BL-320 | Stateful edits tracked | State tracking |
| BL-321 | State dependencies explicit | Dependency graph |
| BL-322 | State changes logged | Change logging |
| BL-323 | State rollback supported | Rollback capability |

```rust
// src/ai/assumptions.rs

/// BL-310 to BL-323: Assumption handling
#[clause("BL-310", "Assumptions explicit")]
pub struct AssumptionManager {
    pub explicit: Vec<ExplicitAssumption>,
    pub detected: Vec<DetectedAssumption>,
    pub confirmed: HashSet<AssumptionId>,
}

impl AssumptionManager {
    /// BL-311: Surface implicit assumptions
    pub fn surface_implicit(&mut self, context: &Context) {
        let detected = self.detect_implicit(context);
        self.detected.extend(detected);
    }
    
    /// BL-313: Check if unconfirmed assumptions block
    pub fn blocks_progress(&self) -> bool {
        self.detected.iter().any(|a| !self.confirmed.contains(&a.id))
    }
}

/// BL-320 to BL-323: Stateful edit tracking
pub struct StatefulEditTracker {
    state_before: StateSnapshot,
    dependencies: DependencyGraph,
}

impl StatefulEditTracker {
    /// BL-323: Rollback state
    pub fn rollback(&self, target: &mut State) -> Result<()> {
        *target = self.state_before.clone();
        Ok(())
    }
}
```

---

### 2.8.2 Execution Charter: Bootstrap & Capabilities (Part 2)

The Execution Charter defines **when** governance activates and **what** capabilities are available.

---

#### 2.8.2.1 Activation (EXEC-001 to EXEC-005)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-001 | Governance activates on project open | `ActivationTrigger` |
| EXEC-002 | Activation is atomic | Transaction semantics |
| EXEC-003 | Default to activated mode | `ProjectMode::Activated` |
| EXEC-004 | Dormant mode requires explicit opt-out | Opt-out flag |
| EXEC-005 | Activation logged | Audit trail |

```rust
// src/governance/activation.rs

/// EXEC-001 to EXEC-005: Activation
#[clause("EXEC-001", "Governance activates on project open")]
pub struct ActivationTrigger;

impl ActivationTrigger {
    /// EXEC-002: Atomic activation
    pub fn activate(&self, project: &Project) -> Result<ActivatedProject> {
        // EXEC-003: Default to activated
        let mode = project.mode.unwrap_or(ProjectMode::Activated);
        
        // EXEC-004: Check for explicit dormant opt-out
        if mode == ProjectMode::Dormant && !project.explicit_dormant_flag {
            return Err(Error::ImplicitDormant { clause: "EXEC-004" });
        }
        
        let activated = ActivatedProject::new(project, mode)?;
        
        // EXEC-005: Log activation
        activated.flight_recorder.log_activation();
        
        Ok(activated)
    }
}
```

---

#### 2.8.2.2 Precedence (EXEC-006 to EXEC-009)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-006 | LAW > Charter > Behavior | Precedence order |
| EXEC-007 | Higher precedence overrides | Override semantics |
| EXEC-008 | Conflicts resolved by precedence | Conflict resolution |
| EXEC-009 | Precedence logged when applied | Audit trail |

```rust
// src/governance/precedence.rs

/// EXEC-006 to EXEC-009: Precedence rules
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum PrecedenceLevel {
    Behavior = 0,   // Lowest
    Charter = 1,
    Law = 2,        // Highest
}

pub struct PrecedenceResolver;

impl PrecedenceResolver {
    /// EXEC-006/007: Higher wins
    pub fn resolve<T>(&self, sources: Vec<(PrecedenceLevel, T)>) -> T {
        sources.into_iter()
            .max_by_key(|(level, _)| *level)
            .map(|(_, value)| value)
            .unwrap()
    }
}
```

---

#### 2.8.2.3 Topic Boundaries (EXEC-010 to EXEC-014)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-010 | Topics define governance boundaries | `TopicBoundary` |
| EXEC-011 | Cross-topic operations require escalation | Escalation check |
| EXEC-012 | Topic scope enforced | Scope enforcement |
| EXEC-013 | Topic membership explicit | Membership rules |
| EXEC-014 | Orphan content gets default topic | Default topic |

```rust
// src/governance/topics.rs

/// EXEC-010 to EXEC-014: Topic boundaries
#[clause("EXEC-010", "Topics define boundaries")]
pub struct TopicBoundary {
    pub topic_id: TopicId,
    pub members: HashSet<EntityRef>,
}

impl TopicBoundary {
    /// EXEC-011: Check cross-topic
    pub fn requires_escalation(&self, operation: &Operation) -> bool {
        operation.targets.iter().any(|t| !self.members.contains(t))
    }
    
    /// EXEC-014: Assign default topic
    pub fn assign_orphan(entity: &Entity) -> TopicId {
        TopicId::default()
    }
}
```

---

#### 2.8.2.4 RID Governance (EXEC-015 to EXEC-023)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-015 | RIDs are unique identifiers | `RID` type |
| EXEC-016 | RID format enforced | Format validation |
| EXEC-017 | RID creation logged | Audit trail |
| EXEC-018 | RID deletion requires approval | Approval flow |
| EXEC-019 | RID references verified | Reference check |
| EXEC-020 | RID scope defined at creation | Scope assignment |
| EXEC-021 | RID changes tracked | Change tracking |
| EXEC-022 | LOG-001 governs session logging | Session integration |
| EXEC-023 | RID metadata required | Metadata fields |

```rust
// src/governance/rid.rs

/// EXEC-015 to EXEC-023: RID governance
#[clause("EXEC-015", "RIDs are unique")]
pub struct RID {
    pub prefix: String,    // e.g., "COR", "BL", "EXEC"
    pub number: u32,       // e.g., 701, 30, 001
    pub scope: RidScope,   // EXEC-020
    pub metadata: RidMetadata, // EXEC-023
}

impl RID {
    /// EXEC-016: Format validation
    pub fn parse(s: &str) -> Result<Self> {
        let re = Regex::new(r"^([A-Z]{2,4})-(\d{1,4})$")?;
        let caps = re.captures(s)
            .ok_or(Error::InvalidRidFormat { clause: "EXEC-016" })?;
        
        Ok(RID {
            prefix: caps[1].to_string(),
            number: caps[2].parse()?,
            scope: RidScope::default(),
            metadata: RidMetadata::default(),
        })
    }
}
```

---

#### 2.8.2.5 Layer Operation (EXEC-024 to EXEC-028)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-024 | Layer 1 is immutable | L1 write guard |
| EXEC-025 | Layer 2 requires approval | L2 approval flow |
| EXEC-026 | Layer 3 is standard work | L3 default |
| EXEC-027 | Layer 4 is ephemeral | L4 no persistence |
| EXEC-028 | Layer violations blocked | Layer guard |

```rust
// src/governance/layers.rs

/// EXEC-024 to EXEC-028: Layer operation
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Layer {
    L1,  // EXEC-024: Immutable
    L2,  // EXEC-025: Approval required
    L3,  // EXEC-026: Standard work
    L4,  // EXEC-027: Ephemeral
}

pub struct LayerGuard;

impl LayerGuard {
    /// EXEC-028: Check layer violations
    pub fn check_write(&self, layer: Layer, operation: &Operation) -> Result<()> {
        match layer {
            Layer::L1 => Err(Error::L1Immutable { clause: "EXEC-024/028" }),
            Layer::L2 if !operation.has_approval() => {
                Err(Error::L2RequiresApproval { clause: "EXEC-025/028" })
            }
            _ => Ok(()),
        }
    }
}
```

---

#### 2.8.2.6 Data, JSON, Machine Structures (EXEC-029 to EXEC-033)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-029 | Data mode for structured content | `WorkMode::Data` |
| EXEC-030 | JSON validated against schema | Schema validation |
| EXEC-031 | Machine structures typed | Type enforcement |
| EXEC-032 | Data transformations logged | Transform logging |
| EXEC-033 | Data integrity verified | Integrity check |

```rust
// src/governance/data_mode.rs

/// EXEC-029 to EXEC-033: Data mode
#[clause("EXEC-029", "Data mode for structured")]
pub struct DataModeValidator;

impl DataModeValidator {
    /// EXEC-030: JSON schema validation
    pub fn validate_json(&self, json: &Value, schema: &Schema) -> Result<()> {
        schema.validate(json)
            .map_err(|e| Error::SchemaViolation { clause: "EXEC-030", error: e })
    }
}
```

---

#### 2.8.2.7 Design vs Operation Modes (EXEC-034 to EXEC-038)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-034 | Design mode for structural changes | `WorkMode::Design` |
| EXEC-035 | Operation mode for data changes | `WorkMode::Operation` |
| EXEC-036 | Mode determines allowed operations | Operation filter |
| EXEC-037 | Mode switch logged | Mode logging |
| EXEC-038 | Incompatible operations blocked | Mode guard |

```rust
// src/governance/work_mode.rs

/// EXEC-034 to EXEC-038: Work modes
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum WorkMode {
    Design,     // EXEC-034: Structural changes
    Operation,  // EXEC-035: Data changes
    Chat,       // Conversation only
    Data,       // EXEC-029: Structured data
    Brainstorm, // Exploratory
}

impl WorkMode {
    /// EXEC-036: Allowed operations
    pub fn allows(&self, op_type: OperationType) -> bool {
        match (self, op_type) {
            (WorkMode::Design, OperationType::Structural) => true,
            (WorkMode::Operation, OperationType::Data) => true,
            (WorkMode::Design, OperationType::Data) => true,
            _ => false,
        }
    }
}
```

---

#### 2.8.2.8 Escalation Rules (EXEC-039 to EXEC-045)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-039 | High-risk operations require escalation | `EscalationEngine` |
| EXEC-040 | Escalation thresholds configurable | `escalation_config` |
| EXEC-041 | Escalation includes context | Context bundle |
| EXEC-042 | Escalation logged | Audit trail |
| EXEC-043 | No structural edits in non-structural modes | Mode enforcement |
| EXEC-044 | Escalation timeout configurable | `escalation_timeout` |
| EXEC-045 | Escalation target configurable | Target specification |

```rust
// src/governance/escalation.rs

/// EXEC-039 to EXEC-045: Escalation
#[clause("EXEC-039", "High-risk requires escalation")]
pub struct EscalationEngine {
    pub config: EscalationConfig,
}

impl EscalationEngine {
    /// EXEC-043: Mode enforcement
    pub fn validate_mode_allows_structural(
        &self, 
        mode: WorkMode, 
        operation: &Operation
    ) -> Result<()> {
        if operation.is_structural() && !matches!(mode, WorkMode::Design) {
            return Err(Error::StructuralInWrongMode { clause: "EXEC-043" });
        }
        Ok(())
    }
    
    /// EXEC-039: Check if escalation needed
    pub fn requires_escalation(&self, operation: &Operation) -> bool {
        operation.risk_score() > self.config.threshold
    }
}
```

---

#### 2.8.2.9 RID Discovery Protocol (EXEC-046 to EXEC-053)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-046 | RID discovery is explicit | `ReferenceDiscovery` |
| EXEC-047 | Full scan before operations | Scan requirement |
| EXEC-048 | Discovered RIDs indexed | Index building |
| EXEC-049 | Unknown RIDs flagged | Unknown detection |
| EXEC-050 | Cannot fabricate non-existent RIDs | Fabrication guard |
| EXEC-051 | Discovery logged | Audit trail |
| EXEC-052 | Index refresh on change | Refresh trigger |
| EXEC-053 | Partial discovery allowed | Partial mode |

```rust
// src/governance/discovery.rs

/// EXEC-046 to EXEC-053: RID discovery
#[clause("EXEC-046", "Discovery is explicit")]
pub struct ReferenceDiscovery {
    index: HashMap<RefId, EntityRef>,
    scanned: bool,
}

impl ReferenceDiscovery {
    /// EXEC-047: Full scan
    pub fn full_scan(&mut self, corpus: &Corpus) -> Result<()> {
        self.index = corpus.scan_all_references()?;
        self.scanned = true;
        Ok(())
    }
    
    /// EXEC-050: No fabrication
    pub fn get_reference(&self, id: &RefId) -> Result<&EntityRef> {
        self.index.get(id)
            .ok_or(Error::ReferenceNotFound { clause: "EXEC-050", id: id.clone() })
    }
}
```

---

#### 2.8.2.10 Requery Mandate (EXEC-054 to EXEC-056)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-054 | Ambiguous requests trigger requery | `RequeryEngine` |
| EXEC-055 | Requery includes options | Option presentation |
| EXEC-056 | Max requery attempts configurable | `max_requery` |

```rust
// src/ai/requery.rs

/// EXEC-054 to EXEC-056: Requery
#[clause("EXEC-054", "Ambiguous triggers requery")]
pub struct RequeryEngine {
    pub max_attempts: usize,  // EXEC-056
}

impl RequeryEngine {
    pub fn check_ambiguity(&self, request: &Request) -> Option<RequeryPrompt> {
        if request.ambiguity_score() > 0.5 {
            Some(RequeryPrompt {
                original: request.clone(),
                options: self.generate_clarifications(request),
            })
        } else {
            None
        }
    }
}
```

---

#### 2.8.v02.12 ANS-001 Invocation (EXEC-057 to EXEC-060)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-057 | ANS-001 governs response behavior | Section 2.7 |
| EXEC-058 | ANS-001 invoked for all AI responses | Response wrapper |
| EXEC-059 | ANS-001 violations blocked | Violation guard |
| EXEC-060 | ANS-001 compliance logged | Compliance logging |

```rust
// src/ai/ans001.rs

/// EXEC-057 to EXEC-060: ANS-001 integration
pub struct Ans001Enforcer;

impl Ans001Enforcer {
    /// EXEC-058: Wrap all responses
    pub fn enforce(&self, response: &AiResponse) -> Result<ValidatedResponse> {
        // Check ANS-001 compliance (see Section 2.7)
        let validation = self.validate_ans001(response)?;
        
        // EXEC-059: Block violations
        if !validation.compliant {
            return Err(Error::Ans001Violation { 
                clause: "EXEC-059",
                violations: validation.violations 
            });
        }
        
        Ok(ValidatedResponse::new(response, validation))
    }
}
```

---

#### 2.8.2.12 CORPUS Contract (EXEC-061 to EXEC-064)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-061 | CORPUS is the content repository | `Corpus` struct |
| EXEC-062 | CORPUS access governed | Access control |
| EXEC-063 | CORPUS changes logged | Change logging |
| EXEC-064 | CORPUS integrity verified | Integrity check |

```rust
// src/content/corpus.rs

/// EXEC-061 to EXEC-064: CORPUS contract
#[clause("EXEC-061", "CORPUS is content repository")]
pub struct Corpus {
    storage: Storage,
    access_control: AccessControl,  // EXEC-062
    change_log: ChangeLog,          // EXEC-063
}

impl Corpus {
    /// EXEC-064: Verify integrity
    pub fn verify_integrity(&self) -> Result<IntegrityReport> {
        self.storage.verify_all_hashes()
    }
}
```

---

#### 2.8.2.13 Future-Model Compatibility (EXEC-065 to EXEC-068)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-065 | External models share NO mutable state | State isolation |
| EXEC-066 | Model interface versioned | Interface versioning |
| EXEC-067 | Model changes logged | Change logging |
| EXEC-068 | Backward compatibility maintained | Compat layer |

```rust
// src/ai/model_compat.rs

/// EXEC-065 to EXEC-068: Model compatibility
pub struct ModelInterface {
    pub version: InterfaceVersion,
    state_isolation: StateIsolation,  // EXEC-065
}

impl ModelInterface {
    /// EXEC-065: Isolated state
    pub fn call(&self, request: &ModelRequest) -> Result<ModelResponse> {
        // No mutable state shared between calls
        let isolated_context = self.state_isolation.create_isolated()?;
        self.model.call_with_context(request, isolated_context)
    }
}
```

---

#### 2.8.2.14 Drift Handling (EXEC-069 to EXEC-073)

| Clause | Text | Implementation |
|--------|------|----------------|
| EXEC-069 | Drift detected when governance diverges | `DriftDetector` |
| EXEC-070 | Drift triggers investigation | Investigation flow |
| EXEC-071 | Drift resolution required before continue | Blocking behavior |
| EXEC-072 | Drift logged with evidence | Evidence logging |
| EXEC-073 | No governed tasks during drift | Drift guard |

```rust
// src/governance/drift.rs

/// EXEC-069 to EXEC-073: Drift handling
#[clause("EXEC-069", "Drift detection")]
pub struct DriftHandler {
    drift_detected: bool,
    evidence: Vec<DriftEvidence>,
}

impl DriftHandler {
    /// EXEC-073: Block tasks during drift
    pub fn can_execute_governed_task(&self) -> Result<()> {
        if self.drift_detected {
            return Err(Error::DriftBlocksTasks {
                clause: "EXEC-073",
                evidence: self.evidence.clone()
            });
        }
        Ok(())
    }
    
    /// EXEC-071: Resolution required
    pub fn require_resolution(&self) -> DriftResolution {
        DriftResolution {
            required: self.drift_detected,
            evidence: &self.evidence,
        }
    }
}
```

---

**Key Takeaways (2.8)**
- âœ“ **Bootloader** defines runtime behavior (LAW vs Behavior, Flight Recorder, quality rubrics)
- âœ“ **Execution Charter** defines activation and capabilities (modes, layers, escalation)
- âœ“ **BL-100..115**: Flight Recorder is append-only, always-on
- âœ“ **BL-270..279**: All edits via PlannedOperation
- âœ“ **EXEC-024..028**: Layer system (L1 immutable, L2 approval, L3 work, L4 ephemeral)
- âœ“ **EXEC-069..073**: Drift blocks all governed tasks until resolved


---

## 2.9 Deterministic Edit Process (COR-701)

**Why**  
Edits to governed content must be deterministic, verifiable, and reversible. COR-701 ensures that every edit produces a provable audit trail, prevents corruption through race conditions, and enables safe rollback on failure.

**What**  
Defines the micro-step execution model, gate pipeline, manifest structure, and assistant behavior requirements for all edit operations.

**Jargon**  
- **Step**: Single targeted replace/insert/delete within one content window.
- **Window**: [start, end] range defining the edit boundary.
- **Manifest**: Deterministic record of pre/post hashes, coordinates, gates passed, and lint results.
- **Gate**: Validation checkpoint that must pass before edit proceeds.
- **Integrity Chain**: Hash chain over SHA1s proving edit history.
- **Batch**: Ordered list of Steps targeting a single document.
- **HALT Manifest**: Failure record when edit cannot complete.

---

### 2.9.1 Purpose & Core Types

| Clause | Text | Implementation |
|--------|------|----------------|
| C701-01 | Define deterministic micro-steps for edits and gated promotion | `DeterministicEditEngine` |
| C701-02 | Prevent corruption, scope creep, concurrency races, nondeterminism | Validation gates |
| C701-03 | Require verifiable Manifests, integrity proof, gate outcomes | `Manifest` struct |
| C701-04 | Provide deterministic contract for AI assistants | Contract definition |

```rust
// src/edit/engine.rs

/// C701-01: Deterministic edit engine
#[clause("C701-01", "Deterministic micro-steps")]
pub struct DeterministicEditEngine {
    gates: Vec<Box<dyn Gate>>,
    manifest_builder: ManifestBuilder,
}

impl DeterministicEditEngine {
    pub fn execute_step(&self, step: Step, context: &mut Context) -> Result<Manifest> {
        // C701-03: Build manifest with all required fields
        let mut manifest = self.manifest_builder.start(&step);
        
        // Run all gates
        for gate in &self.gates {
            let outcome = gate.check(&step, context)?;
            manifest.record_gate(gate.name(), outcome);
            
            if !outcome.passed {
                return Err(GateError::new(&outcome));
            }
        }
        
        // Apply edit
        context.apply(&step)?;
        
        // Finalize manifest
        manifest.finalize(context)?;
        
        Ok(manifest)
    }
}
```

---

### 2.9.2 Definitions

| Clause | Definition | Rust Type |
|--------|------------|-----------|
| C701-20 | Step = single targeted replace/insert/delete within one Window | `Step` |
| C701-21 | Window = [start, end] interval defining edit boundary | `Window` |
| C701-22 | Block = contiguous content unit being modified | `Block` |
| C701-23 | Manifest = deterministic record of pre/post SHA1, coordinates, gates, lint | `Manifest` |
| C701-25 | Integrity chain = hash chain over SHA1s per Step/Batch | `IntegrityChain` |
| C701-26 | Batch = ordered list of Steps targeting single document | `Batch` |
| C701-27 | Batch Plan = pre-declared structure for Batch | `BatchPlan` |
| C701-28 | HALT Manifest = failure record | `HaltManifest` |

#### 2.9.2.1 Step vs PlannedOperation

The system has two related but distinct operation concepts:

| Concept | Source | Purpose | When Used |
|---------|--------|---------|-----------|
| **PlannedOperation** | Bootloader BL-270..279 | High-level edit intent | AI planning phase |
| **Step** | COR-701 | Low-level execution unit | Deterministic execution |

**Conversion Flow:**
```
User Request
    â†“
PlannedOperation (BL-270..279)
    â†“ validation: entity exists, not unanchored
    â†“ resolve: entity_ref â†’ coordinates
    â†“ capture: compute pre_sha1
Step (COR-701)
    â†“ gates: C701-50..60
Manifest (proof of execution)
```

```rust
// src/edit/types.rs

/// C701-20: Step = single targeted edit
#[derive(Debug, Clone)]
pub struct Step {
    pub operation: EditOperationType,
    pub document_id: DocumentId,
    pub window: Window,
    pub before_content: Option<String>,
    pub after_content: String,
    pub anchors: Vec<String>,      // C701-50: Required for validation
    pub pre_sha1: String,          // C701-55, C701-60: Integrity
}

impl Step {
    /// Create Step from PlannedOperation (BL-270 â†’ COR-701 bridge)
    pub fn from_planned_operation(
        op: &PlannedOperation,
        document_id: DocumentId,
        window: Window,
        pre_sha1: String,
    ) -> Self {
        Self {
            operation: op.operation_type,
            document_id,
            window,
            before_content: op.before.as_ref().map(|s| s.content.clone()),
            after_content: op.after.content.clone(),
            anchors: vec![op.entity_ref.to_string()],
            pre_sha1,
        }
    }
}

/// C701-21: Window
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Window {
    pub start: usize,
    pub end: usize,
}

impl Window {
    pub fn new(start: usize, end: usize) -> Self {
        Self { start, end }
    }
    
    pub fn contains(&self, position: usize) -> bool {
        position >= self.start && position <= self.end
    }
}

/// C701-23: Manifest
#[derive(Debug, Clone)]
pub struct Manifest {
    pub step_id: StepId,
    pub pre_sha1: String,
    pub post_sha1: String,
    pub window: Window,
    pub size_delta: i64,
    pub gate_outcomes: HashMap<String, GateOutcome>,
    pub timestamp: DateTime<Utc>,
}

/// C701-25: Integrity chain
pub struct IntegrityChain {
    links: Vec<IntegrityLink>,
}

pub struct IntegrityLink {
    pub previous_sha1: String,
    pub current_sha1: String,
    pub operation: String,
    pub timestamp: DateTime<Utc>,
}

/// C701-28: HALT Manifest
pub struct HaltManifest {
    pub step_id: Option<StepId>,
    pub error_codes: Vec<String>,
    pub reason: String,
    pub evidence: String,
}
```

---

### 2.9.3 Gate Pipeline

| Clause | Gate | Purpose |
|--------|------|---------|
| C701-50 | anchors_present | All anchors/IDs locatable |
| C701-51 | window_matches_plan | Window matches planned coordinates |
| C701-52 | content_untouched_outside_window | Content outside window unchanged |
| C701-53 | exactly_one_target | Single target modified |
| C701-54 | path_canonical_and_accessible | Document accessible |
| C701-55 | pre_sha1_captured | SHA1 recorded before modification |
| C701-56 | post_sha1_captured | SHA1 recorded after modification |
| C701-57 | size_delta_matches_expected | Size change matches expectation |
| C701-58 | all_references_resolvable | Internal references valid |
| C701-59 | manifest_written | Manifest persisted |
| C701-60 | current_matches_preimage | Concurrency check passed |

```rust
// src/edit/gates.rs

pub trait Gate: Send + Sync {
    fn name(&self) -> &str;
    fn check(&self, step: &Step, context: &Context) -> Result<GateOutcome>;
}

/// C701-50: Anchors present
pub struct AnchorsPresent;
impl Gate for AnchorsPresent {
    fn name(&self) -> &str { "anchors_present" }
    fn check(&self, step: &Step, context: &Context) -> Result<GateOutcome> {
        for anchor in &step.anchors {
            if !context.document.contains_anchor(anchor) {
                return Ok(GateOutcome::fail("C701-50", 
                    format!("Anchor {} not found", anchor)));
            }
        }
        Ok(GateOutcome::pass())
    }
}

/// C701-52: Content untouched outside window
pub struct ContentUntouchedOutsideWindow;
impl Gate for ContentUntouchedOutsideWindow {
    fn name(&self) -> &str { "content_untouched_outside_window" }
    fn check(&self, step: &Step, context: &Context) -> Result<GateOutcome> {
        let pre_outside = context.content_outside(&step.window)?;
        let post_outside = context.proposed_content_outside(step)?;
        
        if pre_outside != post_outside {
            return Ok(GateOutcome::fail("C701-52", 
                "Content modified outside window boundary"));
        }
        Ok(GateOutcome::pass())
    }
}

/// C701-60: Concurrency check
pub struct CurrentMatchesPreimage;
impl Gate for CurrentMatchesPreimage {
    fn name(&self) -> &str { "current_matches_preimage" }
    fn check(&self, step: &Step, context: &Context) -> Result<GateOutcome> {
        let current_sha1 = context.document.compute_sha1()?;
        if current_sha1 != step.pre_sha1 {
            return Ok(GateOutcome::fail("C701-60", 
                "Document changed since pre_sha1 captured (concurrency conflict)"));
        }
        Ok(GateOutcome::pass())
    }
}

/// Create the full gate pipeline
pub fn create_gate_pipeline() -> Vec<Box<dyn Gate>> {
    vec![
        Box::new(AnchorsPresent),              // C701-50
        Box::new(WindowMatchesPlan),           // C701-51
        Box::new(ContentUntouchedOutsideWindow), // C701-52
        Box::new(ExactlyOneTarget),            // C701-53
        Box::new(PathCanonicalAndAccessible),  // C701-54
        Box::new(PreSha1Captured),             // C701-55
        Box::new(PostSha1Captured),            // C701-56
        Box::new(SizeDeltaMatchesExpected),    // C701-57
        Box::new(AllReferencesResolvable),     // C701-58
        Box::new(ManifestWritten),             // C701-59
        Box::new(CurrentMatchesPreimage),      // C701-60
    ]
}
```

---

### 2.9.4 Micro-Steps

| Clause | Text | Implementation |
|--------|------|----------------|
| C701-40 | Locate canonical document | Document resolution |
| C701-41 | Identify target; compute pre_sha1 | Target identification |
| C701-42 | Resolve Window coordinates; assert boundaries | Window resolution |
| C701-43 | Re-read and assert SHA1 equals pre_sha1 before edit | Concurrency check |
| C701-44 | Apply edit; compute post_sha1; compute size_delta | Edit application |
| C701-45 | Validate outside-Window content unchanged | Boundary check |
| C701-46 | Emit Step Manifest; append to Flight Recorder | Recording |
| C701-47 | On failure, revert to original and HALT | Rollback |

```rust
// src/edit/executor.rs

pub struct MicroStepExecutor {
    gates: Vec<Box<dyn Gate>>,
}

impl MicroStepExecutor {
    pub fn execute(&self, step: &Step, context: &mut EditContext) -> Result<Manifest> {
        // C701-40: Locate canonical document
        let document = context.resolve_document(&step.document_id)?;
        
        // C701-41: Identify target and compute pre_sha1
        let target = context.find_target(&step.anchors)?;
        let pre_sha1 = document.compute_sha1()?;
        
        // C701-42: Resolve Window, assert boundaries valid
        let window = self.resolve_window(step, &target)?;
        
        // C701-43: Re-read and verify pre_sha1 (concurrency check)
        let current_sha1 = document.reread_and_hash()?;
        if current_sha1 != pre_sha1 {
            // C701-47: HALT on mismatch
            return Err(ConcurrencyError::new("C701-43/60", 
                "Document changed during operation").into());
        }
        
        // Save original for potential rollback
        let original_content = document.content().to_vec();
        
        // C701-44: Apply edit
        let apply_result = document.apply_edit(step);
        
        match apply_result {
            Ok(()) => {
                // Compute post_sha1 and size_delta
                let post_sha1 = document.compute_sha1()?;
                let size_delta = step.after_content.len() as i64 
                    - step.before_content.as_ref().map(|b| b.len()).unwrap_or(0) as i64;
                
                // C701-45: Validate outside-Window unchanged
                if !self.validate_boundary_integrity(&original_content, document.content(), &window) {
                    // C701-47: Revert and HALT
                    document.restore(&original_content);
                    return Err(BoundaryError::new("C701-45", 
                        "Content outside window was modified").into());
                }
                
                // C701-46: Emit manifest
                let manifest = Manifest {
                    step_id: StepId::new(),
                    pre_sha1,
                    post_sha1,
                    window: window.clone(),
                    size_delta,
                    gate_outcomes: self.collect_gate_outcomes(),
                    timestamp: Utc::now(),
                };
                
                context.flight_recorder.append(&manifest)?;
                
                Ok(manifest)
            }
            Err(e) => {
                // C701-47: Revert and HALT
                document.restore(&original_content);
                Err(e)
            }
        }
    }
    
    fn validate_boundary_integrity(
        &self, 
        original: &[u8], 
        current: &[u8], 
        window: &Window
    ) -> bool {
        // Content before window must match
        if original[..window.start] != current[..window.start] {
            return false;
        }
        // Content after window must match (adjusted for size change)
        let original_after_start = window.end;
        let size_change = current.len() as i64 - original.len() as i64;
        let current_after_start = (window.end as i64 + size_change) as usize;
        
        original[original_after_start..] == current[current_after_start..]
    }
}
```

---

### 2.9.5 Assistant Behavior

| Clause | Text | Implementation |
|--------|------|----------------|
| C701-180 | Governed assistant treats COR-701 as binding | Binding flag |
| C701-181 | No edit without valid Manifest and passing gates | Validation requirement |
| C701-182 | After approval, Batch Steps execute without per-Step confirmation | Batch execution |
| C701-183 | Surface summary of Steps, gates, HALT conditions | Summary generation |
| C701-184 | If cannot satisfy COR-701, refuse edit and emit HALT Manifest | Refusal protocol |
| C701-185 | No ignoring or bypassing COR-701 | Compliance check |

```rust
// src/assistant/edit_behavior.rs

/// C701-180: Assistant edit behavior
pub struct EditBehavior;

impl EditBehavior {
    /// C701-180: Check if operation is governed
    pub fn is_governed(&self, operation: &PlannedOperation) -> bool {
        operation.targets_governed_content()
    }
    
    /// C701-181: Validation requirement
    pub fn validate_has_manifest(&self, operation: &PlannedOperation) -> Result<()> {
        if self.is_governed(operation) && operation.manifest.is_none() {
            return Err(BehaviorError::new("C701-181", 
                "Governed edit requires valid Manifest"));
        }
        Ok(())
    }
    
    /// C701-184: Refusal protocol
    pub fn refuse_with_halt(&self, reason: &str) -> HaltManifest {
        HaltManifest {
            step_id: None,
            error_codes: vec!["C701-184".to_string()],
            reason: reason.to_string(),
            evidence: "Cannot satisfy COR-701 requirements".to_string(),
        }
    }
    
    /// C701-183: Generate summary
    pub fn summarize_batch(&self, batch: &Batch, results: &[StepResult]) -> BatchSummary {
        BatchSummary {
            total_steps: batch.steps.len(),
            completed: results.iter().filter(|r| r.success).count(),
            failed: results.iter().filter(|r| !r.success).count(),
            gates_checked: results.iter().map(|r| r.gates_passed.len()).sum(),
            halt_conditions: results.iter()
                .filter_map(|r| r.halt_manifest.as_ref())
                .cloned()
                .collect(),
        }
    }
}
```

---

### 2.9.6 Reply Format Requirements

| Clause | Text | Implementation |
|--------|------|----------------|
| C701-190 | Interactive reply needs structural context | Context requirement |
| C701-191 | Provide full view OR deterministic patch | Reply format |
| C701-192 | Patch must include Window coordinates and exact content | Patch completeness |
| C701-193 | Free-floating content without context violates COR-701 | Rejection |
| C701-194 | When length constrained, prefer structured patch | Format preference |

```rust
// src/assistant/reply_format.rs

/// C701-191: Reply format options
pub enum ReplyFormat {
    /// Full document view with context
    FullView {
        document_id: DocumentId,
        content: String,
    },
    /// Deterministic patch (C701-192)
    Patch {
        document_id: DocumentId,
        window: Window,
        old_content: Option<String>,
        new_content: String,
    },
}

impl ReplyFormat {
    /// C701-193: Reject free-floating content
    pub fn validate(&self) -> Result<()> {
        match self {
            ReplyFormat::FullView { .. } => Ok(()),
            ReplyFormat::Patch { window, .. } => {
                if window.start == 0 && window.end == 0 {
                    Err(FormatError::new("C701-193", 
                        "Free-floating content without context"))
                } else {
                    Ok(())
                }
            }
        }
    }
    
    /// C701-194: Choose format based on constraints
    pub fn choose_format(
        context_available: bool, 
        length_constrained: bool
    ) -> ReplyFormatPreference {
        if length_constrained {
            ReplyFormatPreference::Patch
        } else if context_available {
            ReplyFormatPreference::FullView
        } else {
            ReplyFormatPreference::Patch
        }
    }
}
```

---

**Key Takeaways (2.9)**
- âœ“ Every edit produces a **Manifest** proving what happened
- âœ“ **11 gates** validate edits before they're applied
- âœ“ **SHA1 integrity chain** prevents undetected corruption
- âœ“ **Concurrency check** (C701-60) prevents race conditions
- âœ“ **Automatic rollback** on any failure (C701-47)
- âœ“ Assistants **cannot bypass** COR-701 (C701-185)


---

## 2.10 Session Logging (LOG-001)

**Why**  
LOG-001 defines the session state model and logging hygiene that powers the Flight Recorder. Without structured session state, audit trails become unqueryable noise.

**What**  
Defines SessionState fields, hygiene rules, and Task Ledger schema that make the Flight Recorder useful for debugging, compliance, and recovery.

**Jargon**  
- **SessionState**: The current state of a governed session (step, task, layer, intent).
- **KLOG**: One-line knowledge log entry for quick reference.
- **Task Ledger**: Structured log of task execution with outcomes.
- **Session Anchor**: The root reference for a session's audit trail.

---

### 2.10.1 Session State (L001-50 to L001-58)

| Clause | Text | Implementation |
|--------|------|----------------|
| L001-50 | sa_step: strictly increasing integer | `sa_step: u64` |
| L001-51 | sa_task: short literal label | `sa_task: String` |
| L001-52 | sa_rid: RID or 'None' | `sa_rid: Option<String>` |
| L001-53 | sa_layer: L1\|L2\|L3\|L4 | `sa_layer: SessionLayer` |
| L001-54 | sa_window: short literal window label | `sa_window: String` |
| L001-55 | sa_intent: literal user intent | `sa_intent: String` |
| L001-56 | sa_risk: risk category | `sa_risk: SessionRisk` |
| L001-57 | sa_growth: optional list of Growth_Item | `sa_growth: Vec<GrowthItem>` |
| L001-58 | No fields outside this list | `#[serde(deny_unknown_fields)]` |

```rust
// src/observability/session_state.rs

/// L001-50 to L001-58: Session_State fields
#[clause("L001-50", "sa_step: strictly increasing integer")]
#[clause("L001-51", "sa_task: short literal label")]
#[clause("L001-52", "sa_rid: RID or 'None'")]
#[clause("L001-53", "sa_layer: L1|L2|L3|L4")]
#[clause("L001-54", "sa_window: short literal window label")]
#[clause("L001-55", "sa_intent: literal user intent")]
#[clause("L001-56", "sa_risk: risk category")]
#[clause("L001-57", "sa_growth: optional list of Growth_Item")]
#[clause("L001-58", "No fields outside this list")]
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(deny_unknown_fields)]  // L001-58
pub struct SessionState {
    pub sa_step: u64,               // L001-50: strictly increasing
    pub sa_task: String,            // L001-51
    pub sa_rid: Option<String>,     // L001-52
    pub sa_layer: SessionLayer,     // L001-53
    pub sa_window: String,          // L001-54
    pub sa_intent: String,          // L001-55
    pub sa_risk: SessionRisk,       // L001-56
    pub sa_growth: Vec<GrowthItem>, // L001-57
}

/// L001-53: Layer values
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SessionLayer {
    L1,
    L2,
    L3,
    L4,
}

/// L001-56: Risk categories
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SessionRisk {
    None,
    Low,
    Medium,
    High,
    Critical,
}

/// L001-57: Growth item
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GrowthItem {
    pub category: String,
    pub description: String,
    pub timestamp: DateTime<Utc>,
}

impl SessionState {
    /// L001-50: Step must increase
    pub fn advance_step(&mut self) {
        self.sa_step += 1;
    }
    
    /// Validate state consistency
    pub fn validate(&self) -> Result<()> {
        // L001-50: Step must be positive
        if self.sa_step == 0 {
            return Err(Error::InvalidStep { clause: "L001-50" });
        }
        
        // L001-51: Task must not be empty
        if self.sa_task.is_empty() {
            return Err(Error::EmptyTask { clause: "L001-51" });
        }
        
        Ok(())
    }
}
```

---

### 2.10.2 Hygiene Rules (L001-40 to L001-51)

| Clause | Text | Implementation |
|--------|------|----------------|
| L001-40 | Session starts with clean state | `SessionState::new()` |
| L001-41 | State transitions logged | Transition logging |
| L001-42 | Invalid transitions rejected | Transition validation |
| L001-43 | State recovery from log | Recovery mechanism |
| L001-44 | No orphan states | Orphan detection |
| L001-45 | State serialization deterministic | Deterministic serde |
| L001-46 | State compression optional | Compression config |
| L001-47 | State encryption optional | Encryption config |
| L001-48 | State backup periodic | Backup scheduler |
| L001-49 | State pruning policy | Pruning rules |
| L001-51 | Task label format enforced | Format validation |

```rust
// src/observability/session_hygiene.rs

/// L001-40 to L001-51: Session hygiene
pub struct SessionHygiene {
    config: HygieneConfig,
}

impl SessionHygiene {
    /// L001-40: Clean initial state
    pub fn new_session() -> SessionState {
        SessionState {
            sa_step: 1,
            sa_task: "init".to_string(),
            sa_rid: None,
            sa_layer: SessionLayer::L4,
            sa_window: "startup".to_string(),
            sa_intent: "session initialization".to_string(),
            sa_risk: SessionRisk::None,
            sa_growth: vec![],
        }
    }
    
    /// L001-42: Validate transition
    pub fn validate_transition(
        &self, 
        from: &SessionState, 
        to: &SessionState
    ) -> Result<()> {
        // Step must increase
        if to.sa_step <= from.sa_step {
            return Err(Error::StepMustIncrease { 
                clause: "L001-50",
                from: from.sa_step,
                to: to.sa_step 
            });
        }
        
        Ok(())
    }
    
    /// L001-43: Recover from log
    pub fn recover_state(&self, log: &FlightRecorder) -> Result<SessionState> {
        log.entries()
            .filter_map(|e| e.session_state())
            .last()
            .cloned()
            .ok_or(Error::NoStateToRecover)
    }
    
    /// L001-44: Detect orphan states
    pub fn detect_orphans(&self, states: &[SessionState]) -> Vec<&SessionState> {
        states.iter()
            .filter(|s| s.sa_rid.is_none() && s.sa_task == "orphan")
            .collect()
    }
}
```

---

### 2.10.3 Task Ledger (L001-110 to L001-145)

| Clause | Text | Implementation |
|--------|------|----------------|
| L001-110 | Task Ledger is append-only | Append-only storage |
| L001-111 | Each entry has unique task_id | `task_id: TaskId` |
| L001-112 | Entry includes start_time | `start_time: DateTime<Utc>` |
| L001-113 | Entry includes end_time | `end_time: Option<DateTime<Utc>>` |
| L001-114 | Entry includes outcome | `outcome: TaskOutcome` |
| L001-115 | Entry includes artifacts | `artifacts: Vec<ArtifactRef>` |
| L001-120 | Task states: Pending, Running, Complete, Failed, Cancelled | `TaskState` enum |
| L001-121 | State transitions validated | Transition rules |
| L001-130 | Task duration computed | Duration calculation |
| L001-131 | Task dependencies tracked | Dependency graph |
| L001-140 | Task retry policy | Retry config |
| L001-141 | Task timeout policy | Timeout config |
| L001-145 | Task Ledger queryable | Query interface |

```rust
// src/observability/task_ledger.rs

/// L001-110 to L001-145: Task Ledger
#[clause("L001-110", "Task Ledger is append-only")]
pub struct TaskLedger {
    storage: AppendOnlyStorage,
}

/// L001-111 to L001-115: Task entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskEntry {
    /// L001-111: Unique task ID
    pub task_id: TaskId,
    /// L001-112: Start time
    pub start_time: DateTime<Utc>,
    /// L001-113: End time (None if running)
    pub end_time: Option<DateTime<Utc>>,
    /// L001-114: Outcome
    pub outcome: TaskOutcome,
    /// L001-115: Artifacts produced
    pub artifacts: Vec<ArtifactRef>,
    /// Current state
    pub state: TaskState,
    /// L001-131: Dependencies
    pub dependencies: Vec<TaskId>,
}

/// L001-120: Task states
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum TaskState {
    Pending,
    Running,
    Complete,
    Failed,
    Cancelled,
}

/// L001-114: Task outcomes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TaskOutcome {
    Success { result: Value },
    Failure { error: String, recoverable: bool },
    Cancelled { reason: String },
    Timeout,
}

impl TaskLedger {
    /// L001-110: Append only
    pub fn record(&mut self, entry: TaskEntry) -> Result<TaskId> {
        self.storage.append(&entry)?;
        Ok(entry.task_id)
    }
    
    /// L001-121: Validate state transition
    pub fn validate_transition(&self, from: TaskState, to: TaskState) -> Result<()> {
        let valid = match (from, to) {
            (TaskState::Pending, TaskState::Running) => true,
            (TaskState::Running, TaskState::Complete) => true,
            (TaskState::Running, TaskState::Failed) => true,
            (TaskState::Running, TaskState::Cancelled) => true,
            (TaskState::Pending, TaskState::Cancelled) => true,
            _ => false,
        };
        
        if !valid {
            return Err(Error::InvalidTaskTransition { 
                clause: "L001-121",
                from, to 
            });
        }
        
        Ok(())
    }
    
    /// L001-130: Compute duration
    pub fn duration(&self, entry: &TaskEntry) -> Option<Duration> {
        entry.end_time.map(|end| end - entry.start_time)
    }
    
    /// L001-145: Query interface
    pub fn query(&self) -> TaskLedgerQuery {
        TaskLedgerQuery::new(&self.storage)
    }
}

/// L001-145: Query builder
pub struct TaskLedgerQuery<'a> {
    storage: &'a AppendOnlyStorage,
    filters: Vec<TaskFilter>,
}

impl<'a> TaskLedgerQuery<'a> {
    pub fn by_state(mut self, state: TaskState) -> Self {
        self.filters.push(TaskFilter::State(state));
        self
    }
    
    pub fn by_time_range(mut self, start: DateTime<Utc>, end: DateTime<Utc>) -> Self {
        self.filters.push(TaskFilter::TimeRange { start, end });
        self
    }
    
    pub fn execute(self) -> Vec<TaskEntry> {
        self.storage.query_with_filters(&self.filters)
    }
}
```

---

**Key Takeaways (2.10)**
- âœ“ **SessionState** has exactly 8 fields (L001-50 to L001-58), no more
- âœ“ **sa_step** is strictly increasing (monotonic)
- âœ“ **Task Ledger** is append-only like Flight Recorder
- âœ“ **Task states** follow defined transitions only
- âœ“ All session data is **queryable** for debugging and compliance


---

# 3. Local-First Infrastructure

## 3.1 Local-First Data Fundamentals

**Why**  
Local-first is a core principle, not just a feature. Understanding what it means technically prevents design mistakes that would compromise user sovereignty.

**What**  
Explains what "local-first" really means, why concurrent editing is hard, how CRDTs solve it, and what CRDTs don't solve.

**Jargon**  
- **Local-first**: Data lives on your device first; cloud is optional.
- **Concurrent Editing**: Multiple participants modifying the same data simultaneously.
- **CRDT (Conflict-free Replicated Data Type)**: Data structure that can be merged without conflicts.
- **Eventual Consistency**: All replicas converge to the same state given enough time.

---

#### 3.1.0.1 The Promise

**Local-first software keeps your data on your devices, with optional cloud sync.** This gives you:

- **Ownership:** Your files are literally on your computer
- **Speed:** No network round-trip for every action
- **Offline:** Works without internet
- **Privacy:** Data doesn't have to touch company servers

#### 3.1.0.2 The Contrast

```
CLOUD-FIRST (Google Docs):                LOCAL-FIRST (What we're building):
                                          
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google's Servers  â”‚ â† "Real" data     â”‚   YOUR Computer     â”‚ â† "Real" data
â”‚   (the cloud)       â”‚                   â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                         â”‚
          â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Browser      â”‚ â† Just a window   â”‚   Cloud (optional)  â”‚ â† Backup/sync
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.1.0.3 Why Local-First for This Project?

ğŸ“Œ **Privacy:** AI processes your documents locally. Your private notes never leave your machine.

ğŸ“Œ **Speed:** No waiting for server round-trips. The AI model is right there on your GPU.

ğŸ“Œ **Ownership:** Your data is literally files on your computer. No company can lock you out.

ğŸ“Œ **Offline:** Works on airplanes, in basements, anywhere. No "you're offline" errors.

âš ï¸ **The Tradeoff:** Syncing between devices becomes much harder. When two devices edit the same document offline, we need special technology (CRDTs) to merge the changes.

---

### 3.1.1 The Problem: Concurrent Editing

#### 3.1.1.1 Why This is Hard

**Traditional databases assume one "source of truth."** When you save a document, you overwrite what was there. If two people edit simultaneously, one overwrites the other.

```
Traditional Approach (Google Docs style):
  
  [Device A]                    [Server]                    [Device B]
      â”‚                            â”‚                            â”‚
      â”‚â”€â”€â”€â”€ Edit: "Hello" â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                            â”‚
      â”‚                            â”‚<â”€â”€â”€â”€ Edit: "World" â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚                            â”‚                            â”‚
      â”‚                   Server decides order                  â”‚
      â”‚                   "Hello" then "World"                  â”‚
      â”‚                   OR "World" then "Hello"               â”‚
      â”‚                            â”‚                            â”‚
      
  Problem: Server is required. No offline support.
```

```
Local-First Challenge:
  
  [Device A - Offline]                              [Device B - Offline]
      â”‚                                                  â”‚
      â”‚ Edit: "Hello"                                    â”‚ Edit: "World"
      â”‚     (no server to ask!)                          â”‚     (no server!)
      â”‚                                                  â”‚
      â–¼                                                  â–¼
  Local state: "Hello"                          Local state: "World"
  
  Later, when both reconnect... now what?
```

#### 3.1.1.2 Timeline of a Conflict

```
Monday 9am:  Both laptop and tablet sync â†’ same document state
Monday 10am: You go offline on both devices
Monday 11am: On laptop, you add paragraph A
Monday 11am: On tablet, you add paragraph B
Monday 2pm:  Both come online again

QUESTION: What should the document look like now?

  Option 1: Last-write-wins â†’ One person's work is LOST âŒ
  Option 2: Keep both versions, ask user to choose â†’ Annoying âŒ  
  Option 3: Automatically merge both changes â†’ âœ“ This is what CRDTs do
```

---

### 3.1.2 Solution: CRDTs Explained

#### 3.1.2.1 Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **CRDT** | Conflict-free Replicated Data Typeâ€”a special data structure that can merge automatically | The technology that makes local-first sync possible |
| **Merge** | Combining two versions into one | CRDTs guarantee merges always produce the same result |
| **Eventual Consistency** | All devices eventually have the same data, even if they're temporarily different | What CRDTs guarantee |
| **Operation-based (Op-based)** | A CRDT style that syncs by sharing operations ("insert 'A' at position 3") | One approach |
| **State-based** | A CRDT style that syncs by sharing entire state snapshots | Another approach |

#### 3.1.2.2 The Magic of CRDTs

**CRDTs are data structures designed so that merging always works and always produces the same result.**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: How CRDTs Work
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key insight: Instead of storing "the text is Hello", store 
"character H was inserted by device A at time T1, character e 
was inserted by device A at time T2..."

This extra information lets us ALWAYS merge correctly:

Device A's operations:              Device B's operations:
  1. Insert "H" at start             1. Insert "W" at start
  2. Insert "e" after "H"            2. Insert "o" after "W"
  3. Insert "l" after "e"            3. Insert "r" after "o"
  ...                                ...
  
When merging:
  â€¢ Each operation has a unique ID
  â€¢ We can replay ALL operations in a deterministic order
  â€¢ Both devices end up with: "HelloWorld" (or "WorldHello")
  â€¢ The SAME result regardless of which device syncs first!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### 3.1.2.3 Types of CRDT Data Structures

```
For Text Documents:
  â€¢ Tracks each character insertion/deletion
  â€¢ Handles concurrent typing in different places
  â€¢ Libraries: Yjs (Y.Text), Automerge (Text type)

For JSON-like Objects:
  â€¢ Tracks changes to keys and values
  â€¢ Handles concurrent edits to different fields
  â€¢ Libraries: Yjs (Y.Map), Automerge (objects)

For Lists/Arrays:
  â€¢ Tracks insertions, deletions, moves
  â€¢ Handles concurrent list modifications
  â€¢ Libraries: Yjs (Y.Array), Loro (MovableList)

For Rich Text:
  â€¢ Tracks formatting (bold, italic, etc.)
  â€¢ Handles concurrent formatting changes
  â€¢ Libraries: Yjs + editor bindings
```

#### 3.1.2.4 What CRDTs DON'T Solve

âš ï¸ **CRDTs merge automatically, but "automatic" doesn't mean "smart."**

```
Example: Two users both edit the SAME sentence:

Original:        "The quick brown fox"
User A changes:  "The fast brown fox"      (quick â†’ fast)
User B changes:  "The quick red fox"       (brown â†’ red)

CRDT merge:      "The fast red fox"        (both changes applied)

Is this right? Maybe! But maybe User A wanted to keep "brown" and 
User B wanted to keep "quick". The CRDT doesn't understand INTENT,
it just merges the characters.
```

ğŸ’¡ **Key insight:** CRDTs prevent data loss and conflicts, but users may still need to review merged results for semantic correctness.

---

**Key Takeaways**  
- Local-first means your device holds authoritative data; cloud is secondary.
- The core challenge is concurrent editing when devices are offline.
- CRDTs solve this by tracking operations (not just state), enabling deterministic merges.
- CRDTs guarantee eventual consistencyâ€”all devices converge to the same state.
- CRDTs merge mechanically; they don't understand semantic intent, so users may need to review results.

---
## 3.2 CRDT Libraries Comparison

**Why**  
Choosing the right CRDT library affects performance, features, and ecosystem. This comparison helps make an informed decision.

**What**  
Deep dives into Yjs, Automerge, and Loro with pros/cons and recommendations.

**Jargon**  
- **Yjs**: Mature JavaScript CRDT library with rich ecosystem.
- **Automerge**: Rust-first CRDT with strong formal foundations.
- **Loro**: Emerging Rust CRDT combining best features of both.

---

### 3.2.1 Yjs Deep Dive

#### 3.2.1.1 What is Yjs?

**Yjs is the most popular CRDT library for JavaScript/TypeScript applications.** It's battle-tested, fast, and has excellent editor integrations.

#### 3.2.1.2 Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Yjs                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     JavaScript/TypeScript                         â”‚
â”‚ Also:         Rust port (Yrs), Python, Swift, Ruby          â”‚
â”‚ Data Types:   Y.Text, Y.Map, Y.Array, Y.XmlFragment         â”‚
â”‚ Performance:  Excellent (~260K inserts: 1s, 10MB memory)    â”‚
â”‚ History:      No full history (snapshots optional)          â”‚
â”‚ Sync:         WebSocket, WebRTC, custom providers           â”‚
â”‚ Editors:      ProseMirror, TipTap, Monaco, Quill, more      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2.1.3 How Yjs Works

```javascript
// Basic Yjs usage
import * as Y from 'yjs'

// Create a document
const doc = new Y.Doc()

// Get a shared text type
const text = doc.getText('content')

// Make changes
text.insert(0, 'Hello World')

// Observe changes (for updating UI)
text.observe(event => {
  console.log('Text changed:', text.toString())
})

// Export for sync/storage
const update = Y.encodeStateAsUpdate(doc)  // Binary format
```

#### 3.2.1.4 Pros and Cons

**Pros:**
- â­ Best performance and memory efficiency
- â­ Rich editor integrations (drop-in for popular editors)
- â­ Large community, many examples
- â­ Multiple sync options (WebSocket, WebRTC, file-based)
- â­ Cross-platform via ports (Yrs for Rust/Tauri)

**Cons:**
- âš ï¸ No built-in full history (only current state)
- âš ï¸ Learning curve for understanding shared types
- âš ï¸ Need to manually handle persistence

---

### 3.2.2 Automerge Deep Dive

#### 3.2.2.1 What is Automerge?

**Automerge is an academically rigorous CRDT library with full history tracking.** Version 2 is written in Rust with JavaScript bindings.

#### 3.2.2.2 Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Automerge                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     Rust core, JS/WASM bindings                   â”‚
â”‚ Data Types:   JSON-like objects, lists, text, counters      â”‚
â”‚ Performance:  Slower (~260K inserts: 1.8s, 44MB memory)     â”‚
â”‚ History:      Full operation history (like Git)             â”‚
â”‚ Sync:         Custom sync protocol                          â”‚
â”‚ Best For:     When you need complete version history        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2.2.3 Pros and Cons

**Pros:**
- â­ Full version historyâ€”can reconstruct any past state
- â­ Cleaner APIâ€”works like normal JS objects
- â­ Academic backingâ€”provably correct
- â­ Good for debugging (can replay history)

**Cons:**
- âš ï¸ Higher memory usage (~4x more than Yjs)
- âš ï¸ Slower for large documents
- âš ï¸ Larger storage requirements (keeps all operations)
- âš ï¸ Fewer editor integrations

---

### 3.2.3 Loro and Emerging Options

#### 3.2.3.1 What is Loro?

**Loro is a new CRDT library aiming to combine the best of Yjs and Automerge.** It offers high performance AND full history.

#### 3.2.3.2 Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Loro                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     Rust core, JS/WASM bindings                   â”‚
â”‚ Data Types:   MovableList, Map, Tree, Text, Counter         â”‚
â”‚ Performance:  Very high (designed to beat both Yjs/Automerge)â”‚
â”‚ History:      Full version DAG (like Git)                   â”‚
â”‚ Unique:       Movable trees (great for outlines/kanban)     â”‚
â”‚ Maturity:     Newer, less battle-tested                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2.3.3 Pros and Cons

**Pros:**
- â­ Rust-native (great for Tauri)
- â­ Full history like Automerge, speed like Yjs (claimed)
- â­ Movable trees perfect for hierarchical data (outlines, kanban)
- â­ Time-travel debugging possible

**Cons:**
- âš ï¸ Newer, less proven in production
- âš ï¸ Smaller community and fewer integrations
- âš ï¸ API may still change

---

### 3.2.4 Recommendation: Which CRDT Library?

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: CRDT Library Choice
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FOR ELECTRON + TypeScript:
  â””â”€â”€ Use Yjs
      â€¢ Best performance and editor integrations
      â€¢ Largest community, most resources
      â€¢ Add snapshots for version history if needed

FOR TAURI + Rust:
  â””â”€â”€ Consider Loro or Yrs (Yjs Rust port)
      â€¢ Loro: If you need movable trees and version history
      â€¢ Yrs: If you want Yjs compatibility across platforms

RECOMMENDATION FOR THIS PROJECT (starting):
  â””â”€â”€ Start with Yjs
      â€¢ Proven, fast, well-documented
      â€¢ Works in both Electron and Tauri (via Yrs)
      â€¢ Easiest path to editor integration
      â€¢ Migrate to Loro later if needed for hierarchical data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### 3.2.4.1 Comparison Summary Table

| Aspect | Yjs | Automerge | Loro |
|--------|-----|-----------|------|
| Performance | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ (claimed) |
| Memory | â­â­â­â­â­ (10MB) | â­â­â­ (44MB) | â­â­â­â­ |
| Full History | âŒ (snapshots only) | âœ… | âœ… |
| Editor Integration | â­â­â­â­â­ | â­â­ | â­â­ |
| Rust Native | Via Yrs | Via WASM | âœ… Native |
| Maturity | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| Movable Trees | âŒ | âŒ | âœ… |

---

**Key Takeaways**  
- Yjs is the performance and ecosystem leaderâ€”best choice for starting out.
- Automerge offers full history but at 4Ã— memory cost and slower performance.
- Loro is a promising emerging option combining Yjs speed with Automerge history, plus movable trees for hierarchical data.
- For Tauri/Rust: use Yrs (Yjs port) or Loro; for TypeScript: use Yjs directly.
- Start with Yjs; consider migration to Loro later if hierarchical sync or version DAG becomes critical.

---
## 3.3 Database & Sync Patterns

**Why**  
Understanding how CRDTs integrate with databases enables efficient local storage and sync.

**What**  
Covers SQLite integration, combining CRDT and database, and sync topologies.

**Jargon**  
- **SQLite**: Embedded relational database.
- **Sync Topology**: How devices connect for data synchronization (peer-to-peer, hub, hybrid).

---

#### 3.3.0.1 Why Use a Database with CRDT?

**CRDTs handle sync, but databases handle queries.** You often need both:

```
CRDT alone:
  âœ“ Sync across devices
  âœ“ Merge concurrent edits
  âœ— "Find all documents containing 'budget'" â†’ Slow (must scan all)
  âœ— "Sort documents by date" â†’ Not built-in
  âœ— Complex queries â†’ Difficult

Database alone:
  âœ“ Fast queries with indexes
  âœ“ Sort, filter, aggregate
  âœ— Sync across devices â†’ Conflicts!
  âœ— Offline merge â†’ Data loss

CRDT + Database:
  âœ“ Sync via CRDT
  âœ“ Query via database
  âœ“ Best of both worlds
```

#### 3.3.0.2 SQLite: The Recommended Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SQLite                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:         Embedded SQL database                         â”‚
â”‚ Storage:      Single file on disk                           â”‚
â”‚ Performance:  Very fast for local operations                â”‚
â”‚ Features:     Full SQL, indexes, full-text search (FTS)     â”‚
â”‚ Size:         Tiny (library is ~1MB)                        â”‚
â”‚ Reliability:  Extremely battle-tested                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why SQLite for this project:**
- â­ Standard across all platforms (Windows, macOS, Linux)
- â­ Works great with Electron AND Tauri
- â­ Full-text search for finding documents
- â­ ACID guarantees (data integrity)
- â­ Single file = easy backup

---

### 3.3.1 Combining CRDT and Database

#### 3.3.1.1 Architecture Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚    User Edit                                                â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚   CRDT    â”‚  â—„â”€â”€â”€ Handles: Sync, Merge, Collaboration   â”‚
â”‚  â”‚  (Yjs)    â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â”‚ On every CRDT change:                              â”‚
â”‚        â”‚ â€¢ Update SQLite                                    â”‚
â”‚        â”‚ â€¢ Update indexes                                   â”‚
â”‚        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  SQLite   â”‚  â—„â”€â”€â”€ Handles: Queries, Search, Indexes     â”‚
â”‚  â”‚           â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â”‚ Query results                                      â”‚
â”‚        â–¼                                                    â”‚
â”‚    UI Display                                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.3.1.2 What Goes Where?

```
In CRDT (Yjs):
  â€¢ Document content (text, rich text)
  â€¢ Board/canvas positions
  â€¢ List ordering
  â€¢ Everything that needs to sync and merge

In SQLite:
  â€¢ Document metadata (title, dates, tags)
  â€¢ Search indexes
  â€¢ User preferences
  â€¢ Derived/computed data
  â€¢ Anything that needs fast querying

Example Schema:
  documents:
    - id (primary key)
    - title (indexed)
    - created_at
    - updated_at
    - tags (indexed)
    - crdt_id (reference to CRDT document)
    - content_preview (first 200 chars for search)
```

#### 3.3.1.3 Sync Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE SYNC FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. USER MAKES EDIT (Device A)
   â”‚
   â”œâ”€â”€â–º CRDT update applied locally
   â”‚
   â”œâ”€â”€â–º SQLite updated with new content/metadata
   â”‚
   â””â”€â”€â–º CRDT update sent to sync server (or peer)

2. SYNC UPDATE RECEIVED (Device B)
   â”‚
   â”œâ”€â”€â–º CRDT merges incoming update
   â”‚
   â”œâ”€â”€â–º SQLite updated to reflect merged state
   â”‚
   â””â”€â”€â–º UI refreshes to show changes

3. CONFLICT HANDLED AUTOMATICALLY
   â”‚
   â””â”€â”€â–º CRDT merge is deterministic
       â”‚
       â””â”€â”€â–º Same SQLite state on all devices (eventually)
```

---

### 3.3.2 Sync Topologies

#### 3.3.2.1 Options for Syncing Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNC TOPOLOGY OPTIONS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION A: Peer-to-Peer                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚Dev Aâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Dev Bâ”‚   Direct device-to-device          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚     â”‚               â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚  Pros: No server needed, private                             â”‚
â”‚  Cons: Both devices must be online simultaneously            â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION B: Central Server                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚Dev Aâ”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚Serverâ”‚â—„â”€â”€â”€â”€â”€â”€â”‚Dev Bâ”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚  Pros: Works when only one device online                    â”‚
â”‚  Cons: Requires running/paying for server                   â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION C: File Sync (OneDrive/Dropbox)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚Dev Aâ”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚OneDriveâ”‚â—„â”€â”€â”€â”€â”€â”€â”‚Dev Bâ”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  Pros: No custom server, leverages existing sync             â”‚
â”‚  Cons: File-level conflicts, coarse merging                  â”‚
â”‚        (need CRDT on top to handle conflicts)               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.3.2.2 Recommendation

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Sync Topology
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 1 (MVP): File Sync + CRDT
  â€¢ Store CRDT updates as files in a synced folder
  â€¢ Let OneDrive/Dropbox/iCloud handle file sync
  â€¢ CRDT handles merge when files conflict
  â€¢ Zero server infrastructure needed

PHASE 2 (Multi-user): Central Sync Server
  â€¢ Build or use WebSocket sync server
  â€¢ Real-time collaboration possible
  â€¢ More complex but better UX

Libraries that help:
  â€¢ y-indexeddb: Local persistence for Yjs
  â€¢ y-websocket: WebSocket sync for Yjs
  â€¢ ElectricSQL: Postgres â†” SQLite sync
  â€¢ Replicache: Client-server sync framework

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

**Key Takeaways**  
- CRDTs handle sync/merge; SQLite handles queries/indexesâ€”use both together.
- CRDT stores content that needs to sync; SQLite stores metadata, search indexes, and derived data.
- On every CRDT change, update SQLite to keep the query layer in sync.
- Start with file-based sync (OneDrive/Dropbox + CRDT); add WebSocket server later for real-time collaboration.
- SQLite is the recommended local database: portable, reliable, full-featured, single-file.

---

## 3.4 Conflict Resolution UX

**Why**  
Even with CRDTs, users sometimes need to understand what changed. Good conflict UX builds trust.

**What**  
Patterns for showing sync status, version history, and when to surface conflicts to users.

**Jargon**  
- **Version History**: Record of document states over time.
- **Sync Status**: Visual indicator of synchronization state.

---

### 3.4.1 User-Facing Conflict Patterns

#### 3.4.1.1 The Good News

**Most of the time, users shouldn't see conflicts at all.** CRDTs merge automatically, and if users edit different parts of a document, everything "just works."

#### 3.4.1.2 When to Show Something

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHEN TO SHOW SYNC FEEDBACK                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ALWAYS SHOW:                                                â”‚
â”‚    â€¢ Sync status indicator (synced âœ“, syncing â†», offline âš¡)â”‚
â”‚    â€¢ "X minutes ago" last sync time                         â”‚
â”‚                                                              â”‚
â”‚  SHOW ON EVENT:                                              â”‚
â”‚    â€¢ "Document updated by another device" notification      â”‚
â”‚    â€¢ Highlight recently changed sections (briefly)          â”‚
â”‚                                                              â”‚
â”‚  SHOW ON POTENTIAL ISSUE:                                    â”‚
â”‚    â€¢ "This section was edited while you were offline.       â”‚
â”‚       Review the changes?" (when same paragraph edited)     â”‚
â”‚                                                              â”‚
â”‚  DON'T BOTHER USER WITH:                                     â”‚
â”‚    â€¢ Every automatic merge (too noisy)                      â”‚
â”‚    â€¢ Technical details ("CRDT vector clock updated")        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.4.1.3 Simple Sync Status UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    âœ“ Synced 2 min ago    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

OR when syncing:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    â†» Syncing...          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

OR when offline:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    âš¡ Working offline     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

---

### 3.4.2 Version History UI

#### 3.4.2.1 Why Provide Version History

Even with automatic merging, users want:
- **Safety net:** "I accidentally deleted something, can I get it back?"
- **Audit trail:** "What changed since yesterday?"
- **Comparison:** "What's different from the old version?"

#### 3.4.2.2 Implementation Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 VERSION HISTORY PANEL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â—„ Document History                                          â”‚
â”‚                                                              â”‚
â”‚  TODAY                                                       â”‚
â”‚  â”œâ”€â”€ 3:45 PM - You edited (current)                        â”‚
â”‚  â”œâ”€â”€ 2:30 PM - Synced from MacBook                         â”‚
â”‚  â””â”€â”€ 10:15 AM - You edited                                  â”‚
â”‚                                                              â”‚
â”‚  YESTERDAY                                                   â”‚
â”‚  â”œâ”€â”€ 8:00 PM - You edited                                   â”‚
â”‚  â””â”€â”€ 2:00 PM - Created                                      â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  [Preview Selected] [Restore to This Version]               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.4.2.3 Technical Implementation

```
For Yjs (no built-in history):
  â€¢ Take periodic snapshots (every N minutes or on significant changes)
  â€¢ Store snapshots in SQLite with timestamps
  â€¢ To restore: load old snapshot, create new CRDT state
  
For Automerge/Loro (built-in history):
  â€¢ History is automatically tracked
  â€¢ Can "time travel" to any past state
  â€¢ Trade-off: larger storage requirements
```

---

**Key Takeaways**  
- CRDTs handle most conflicts invisiblyâ€”don't over-communicate to users.
- Always show sync status (synced/syncing/offline) and last sync time.
- Highlight concurrent edits from other devices briefly; prompt review only when the same paragraph was edited.
- Provide version history as a safety net: periodic snapshots for Yjs, built-in history for Automerge/Loro.
- Version history UI enables restore, audit, and comparison without burdening users with technical details.

---


# 4. LLM Infrastructure

## 4.1 LLM Infrastructure

**Why**  
Running AI models locally requires understanding how they work, how much resource they consume, and what trade-offs exist. This section provides the foundational knowledge for all model-related decisions.

**What**  
Explains how LLMs work at a practical level (parameters, inference vs training), key concepts (tokens, context windows, quantization, GGUF format), and sizing guidance for what fits on a 24GB RTX 3090.

**Jargon**  
- **LLM**: Large Language Modelâ€”AI that generates text by predicting the next word.
- **Parameters**: The "knowledge" of a model stored as numbers; more parameters = more capability but more memory.
- **Inference**: Using a trained model to generate outputs (we do inference, not training).
- **Token**: A chunk of text (~0.75 words); models think in tokens, not characters.
- **Context Window**: How many tokens a model can "see" at onceâ€”its working memory.
- **Quantization**: Compressing a model by reducing number precision (e.g., 16-bit â†’ 4-bit).
- **Q4/Q5/Q8**: Quantization levels; Q4 = smallest/fastest, Q8 = highest quality.
- **GGUF**: Standard file format for quantized local models (used by llama.cpp, Ollama).
- **KV Cache**: Memory used to store conversation context; grows with conversation length.

---

### 4.1.1 How LLMs Work (Simplified)

#### 4.1.1.1 The Basic Idea

**An LLM is a very sophisticated autocomplete.** Given some text, it predicts what text should come nextâ€”but it's so good at this that it can write essays, code, answer questions, and more.

```
You type:       "Write a haiku about programming"
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   LLM Model     â”‚
                  â”‚  (Billions of   â”‚
                  â”‚   parameters)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
Model outputs:  "Code flows like water
                 Bugs emerge from the depths below
                 Debug, rinse, repeat"
```

#### 4.1.1.2 What "Parameters" Mean

Think of parameters as the model's "brain cells"â€”connections that store patterns learned from training data.

```
Model Size Guide:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  3B-4B   â”‚  Small  â”‚  Fast, limited capability     
  7B-8B   â”‚  Medium â”‚  Good balance, our sweet spot 
  13B     â”‚  Large  â”‚  Better quality, slower       
  27B-30B â”‚  XL     â”‚  Near-GPT-3.5 quality         
  70B+    â”‚  XXL    â”‚  Best quality, very demanding 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

ğŸ’¡ **For our project:** 7B-13B models hit the sweet spot of quality vs. resource usage on a 3090.

---

### 4.1.2 Key Concepts: Tokens, VRAM, Quantization

#### 4.1.2.1 Understanding Tokens

**Tokens are how models measure text.** One token â‰ˆ 4 characters â‰ˆ 0.75 words.

```
Example tokenization:
"Hello, how are you today?" 
â†’ ["Hello", ",", " how", " are", " you", " today", "?"]
â†’ 7 tokens

Rough conversion:
  100 tokens  â‰ˆ 75 words   â‰ˆ 1 short paragraph
  1000 tokens â‰ˆ 750 words  â‰ˆ 1.5 pages
  4000 tokens â‰ˆ 3000 words â‰ˆ 6 pages
```

ğŸ“Œ **Why tokens matter:** 
- Models have a maximum context window (e.g., 4096 or 8192 tokens)
- Cloud APIs charge per token
- More tokens = slower responses and more memory

#### 4.1.2.2 Understanding Context Windows

**The context window is the model's "working memory."** It includes BOTH your prompt AND the model's response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4096 TOKEN CONTEXT WINDOW                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  System prompt (instructions)     â”‚  ~200 tokens       â”‚
â”‚  Conversation history             â”‚  ~2000 tokens      â”‚
â”‚  Current user message             â”‚  ~300 tokens       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Space for model's response       â”‚  ~1596 tokens      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âš ï¸ **Warning:** Long conversations eventually "forget" earlier messages when context fills up.

#### 4.1.2.3 Understanding Quantization

**Quantization shrinks models by reducing number precision.** Like saving a photo as JPEG instead of RAWâ€”smaller file, slight quality loss.

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: QUANTIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  Original model: 7B parameters at 16-bit = ~14 GB
  
  Quantized versions:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Format   â”‚ Bits     â”‚ Size        â”‚ Quality Loss       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Q8_0     â”‚ 8-bit    â”‚ ~7 GB       â”‚ Minimal (<1%)      â”‚
  â”‚ Q5_K_M   â”‚ 5-bit    â”‚ ~5 GB       â”‚ Very small (~1-2%) â”‚
  â”‚ Q4_K_M   â”‚ 4-bit    â”‚ ~4 GB       â”‚ Small (~2-3%)      â”‚ â† Sweet spot
  â”‚ Q3_K_M   â”‚ 3-bit    â”‚ ~3 GB       â”‚ Noticeable (~5%)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  ğŸ“Œ Q4_K_M is the most common choice: good quality, big savings

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

ğŸ’¡ **For our project:** We'll primarily use Q4_K_M quantized models in GGUF format.

#### 4.1.2.4 VRAM Usage: Putting It Together

```
Formula for VRAM estimate:
  VRAM â‰ˆ (Parameters in billions) Ã— (Bits Ã· 2) GB
  
  Examples with Q4 (4-bit):
  â€¢ 7B model:  7 Ã— (4Ã·2) = 7 Ã— 2 = ~3.5-4 GB
  â€¢ 13B model: 13 Ã— (4Ã·2) = 13 Ã— 2 = ~6.5-8 GB  
  â€¢ 70B model: 70 Ã— (4Ã·2) = 70 Ã— 2 = ~35 GB... but actually fits in ~17-18GB 
                          (due to efficient formats)
```

---

### 4.1.3 Model Sizes and What Fits

#### 4.1.3.1 Quick Reference Table

| Model Size | Quantization | VRAM Needed | Speed (tokens/sec) | Quality Level |
|------------|--------------|-------------|-------------------|---------------|
| 3-4B | Q4 | ~2-3 GB | 60-200 | Basic tasks |
| 7-8B | Q4 | ~4-5 GB | 50-130 | Good general use |
| 13B | Q4 | ~7-9 GB | 30-70 | Very good |
| 27B | Q4 | ~14 GB | 20-30 | Excellent |
| 70B | Q4 | ~17-18 GB | 10-15 | Near GPT-3.5 |

#### 4.1.3.2 What Fits on Our 24GB RTX 3090?

```
Scenario Planning for 24 GB VRAM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ COMFORTABLE (with headroom):
  â€¢ 3Ã— 7B models (12 GB) + context buffer
  â€¢ 2Ã— 13B models (16 GB) + some headroom  
  â€¢ 1Ã— 7B + 1Ã— 13B + 1Ã— 4B (15 GB)

âš¡ TIGHT (works but careful):
  â€¢ 1Ã— 70B model (17-18 GB) alone
  â€¢ 1Ã— 27B + 1Ã— 7B (18 GB)
  â€¢ 2Ã— 7B + SDXL image generation (8 + 10 = 18 GB)

âœ— WON'T FIT:
  â€¢ 2Ã— 70B models (34+ GB)
  â€¢ 70B + any substantial other model
  â€¢ Multiple 27B+ models
```

#### 4.1.3.3 The Speed Difference: GPU vs CPU

âš¡ **Critical:** Running models from GPU VRAM is approximately 6x faster than running them from system RAM.

| Where Model Lives | Speed | When to Use |
|-------------------|-------|-------------|
| GPU VRAM | ~50-130 tokens/sec | Always prefer this |
| System RAM (CPU) | ~8-20 tokens/sec | Last resort / fallback |

---

**Key Takeaways**  
- LLMs predict "what text comes next" so well they seem intelligent; we do inference, not training.
- Tokens â‰ˆ 0.75 words; context window limits total conversation length.
- Quantization (Q4/Q5) shrinks models 3-4Ã— with minimal quality loss; GGUF is the standard format.
- 7B Q4 model â‰ˆ 4GB VRAMâ€”this is our planning baseline.
- On 24GB VRAM: 2-3 small models (7B-13B) comfortably, or one 70B model alone.
- GPU is ~6Ã— faster than CPU; avoid CPU fallback for user-facing tasks.

---
**Key Takeaways (10.2 - Tokens/VRAM/Quantization)**
- Tokens â‰ˆ 0.75 words; context window limits total conversation length
- Quantization (Q4/Q5) shrinks models 3-4x with minimal quality loss
- GGUF is the standard format for local quantized models
- 7B Q4 model â‰ˆ 4GB VRAM; this is our planning baseline

## 4.2 LLM Inference Runtimes

**Why**  
The runtime software determines how efficiently models execute, how many requests can be handled concurrently, and how easily models can be managed. This section guides runtime selection.

**What**  
Defines what an inference runtime does, compares major options (Ollama, vLLM, TGI, LM Studio, llamafile, llama.cpp), and recommends a phased strategy starting with Ollama for development.

**Jargon**  
- **Runtime**: Software that loads and runs AI models.
- **API**: Application Programming Interfaceâ€”how our app communicates with the runtime.
- **OpenAI-compatible API**: An API matching OpenAI's format, so code written for ChatGPT works locally.
- **Streaming**: Sending response tokens one at a time as generated (better UX).
- **Batching**: Processing multiple requests together for efficiency.
- **Continuous Batching**: Advanced batching that dynamically adds/removes requests mid-generation.
- **PagedAttention**: vLLM's memory optimization technique for efficient KV cache management.

---

### 4.2.1 What is an Inference Runtime?

#### 4.2.1.1 The Role of an Inference Runtime

**A runtime is the software layer between your application and the AI model.** It handles:

```
Your App                    Runtime                     GPU
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP API    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   CUDA/GPU    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Write  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ â€¢ Load   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Matrix  â”‚
â”‚  me a   â”‚                â”‚   model  â”‚              â”‚ math on â”‚
â”‚  poem"  â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â€¢ Run    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ tensors â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Streaming    â”‚   infer  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Response     â”‚ â€¢ Manage â”‚
                          â”‚   memory â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2.1.2 Why Runtime Choice Matters

Different runtimes optimize for different things:

| Priority | Best Runtime | Trade-off |
|----------|-------------|-----------|
| Ease of use | Ollama | Lower max throughput |
| Maximum speed | vLLM | More complex setup |
| Enterprise features | TGI | Heavier infrastructure |
| Simplicity (single model) | llamafile | Very limited features |

---

### 4.2.2 Runtime Comparison: Ollama vs vLLM vs TGI vs Others

#### 4.2.2.1 Overview Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Runtime     â”‚ Multi-Model â”‚ Performance  â”‚ Ease of Use   â”‚ Best For      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ollama      â”‚ Yes (swap)  â”‚ Moderate     â”‚ â­â­â­â­â­ Easy   â”‚ Development   â”‚
â”‚ vLLM        â”‚ No (1 each) â”‚ â­â­â­â­â­ Best  â”‚ â­â­ Complex   â”‚ Production    â”‚
â”‚ TGI         â”‚ No (1 each) â”‚ Very Good    â”‚ â­â­â­ Medium   â”‚ Enterprise    â”‚
â”‚ LM Studio   â”‚ Yes (GUI)   â”‚ Moderate     â”‚ â­â­â­â­â­ Easy   â”‚ Exploration   â”‚
â”‚ llamafile   â”‚ No          â”‚ Low          â”‚ â­â­â­â­â­ Easy   â”‚ Distribution  â”‚
â”‚ llama.cpp   â”‚ No          â”‚ Good         â”‚ â­â­â­ Medium   â”‚ Embedding     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2.2.2 Ollama â€” The Easy Choice

**What it is:** A user-friendly CLI tool and server for running local LLMs. Think "Docker for AI models."

```bash
```
#  Install and run a model in one command
ollama run mistral

```
#  Or start as a server
ollama serve
```
#  Then call via API at localhost:11434
```

**Pros:**
- â­ Incredibly easy to set up (one-line install)
- â­ Built-in model management (download, update, delete)
- â­ OpenAI-compatible API out of the box
- â­ Automatic GPU/CPU fallback
- â­ Supports multiple models (swaps them in/out of VRAM)

**Cons:**
- âš ï¸ Not optimized for high concurrency (~41 tokens/sec under load vs vLLM's ~793)
- âš ï¸ No advanced batching (processes one request fully before next)
- âš ï¸ Model switching has latency (unload/load takes seconds)

**Performance Numbers:**
```
Ollama on RTX 3090 (single user):
  â€¢ Mistral-7B Q4:  ~100-130 tokens/sec
  â€¢ Llama2-13B Q4:  ~40-50 tokens/sec
  â€¢ Under heavy load: drops to ~41 tokens/sec (no batching)
```

**Best for:** Development, personal use, low-concurrency production

### vLLM â€” The Performance Champion

**What it is:** A high-performance inference engine from UC Berkeley, optimized for throughput.

```bash
```
#  Start vLLM server
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-v0.1 \
  --port 8000
```

**Pros:**
- â­ Extremely fast: ~793 tokens/sec under load (vs Ollama's ~41)
- â­ Continuous batching: efficiently handles many concurrent requests
- â­ PagedAttention: optimizes memory usage
- â­ Scales almost linearly with more requests
- â­ OpenAI-compatible API

**Cons:**
- âš ï¸ One model per process (need multiple processes for multiple models)
- âš ï¸ More complex setup than Ollama
- âš ï¸ GPU-only (no CPU fallback)
- âš ï¸ Python-based (adds some overhead to embed)

**Performance Numbers:**
```
vLLM on RTX 3090:
  â€¢ Single request:   Similar to Ollama
  â€¢ 10 concurrent:    ~793 tokens/sec total (vs Ollama's ~41)
  â€¢ Scales to 100s of concurrent requests efficiently
```

**Best for:** High-concurrency production, batch processing, when speed matters most

#### 4.2.2.3 HuggingFace TGI (Text Generation Inference)

**What it is:** HuggingFace's production-grade inference server, used in their cloud offerings.

```bash
```
#  Run via Docker
docker run --gpus all -p 8080:80 \
  ghcr.io/huggingface/text-generation-inference \
  --model-id mistralai/Mistral-7B-v0.1
```

**Pros:**
- â­ Production-tested at scale (powers HuggingFace Inference Endpoints)
- â­ Continuous batching like vLLM
- â­ Built-in metrics (Prometheus) and tracing
- â­ Supports many quantization formats (GPTQ, AWQ, bitsandbytes)
- â­ OpenAI-compatible API

**Cons:**
- âš ï¸ One model per container
- âš ï¸ Requires Docker (adds complexity on Windows)
- âš ï¸ Heavier setup than Ollama

**Best for:** Enterprise production, when you need built-in observability

#### 4.2.2.4 Other Options (Brief)

**LM Studio:**
- GUI application for exploring models
- Has a server mode with OpenAI API
- Great for testing, not ideal for production automation
- Closed-source

**llamafile:**
- Single executable per model (bundles model + runtime)
- Just download and runâ€”no installation
- Limited features, single-threaded
- Best for distributing a pre-packaged model to end users

**llama.cpp (via Python bindings):**
- The engine under Ollama and many others
- Can embed directly in your code
- More control, more complexity
- Good for custom integrations

---

### 4.2.3 Recommended Runtime Strategy

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Runtime Strategy
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED APPROACH: Ollama Primary + vLLM for Heavy Loads

Phase 1 (Development & MVP):
  â””â”€â”€ Use Ollama exclusively
      â€¢ Fastest to set up
      â€¢ Easy model management
      â€¢ Good enough for single-user
      
Phase 2 (Multi-user or batch processing):
  â””â”€â”€ Add vLLM for specific high-throughput needs
      â€¢ Route "fast lane" traffic to Ollama
      â€¢ Route "batch" or "heavy" jobs to vLLM

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### 4.2.3.1 Integration Pattern

```python
```
#  Conceptual routing logic
def route_request(request):
    if request.type == "interactive_chat":
        # Quick responses, single user
        return call_ollama(request)
    elif request.type == "batch_process":
        # Processing many documents
        return call_vllm(request)
    elif request.type == "code_generation":
        # Code needs fast iteration
        return call_ollama(request, model="codellama")
    else:
        # Fallback
        return call_ollama(request)
```

---

**Key Takeaways**  
- Runtime = software that loads and runs your AI models; all major options now support OpenAI-compatible APIs.
- Ollama: easiest setup, built-in model management, good for development and single-userâ€”start here.
- vLLM: highest throughput (~20Ã— faster under load), but one model per process and more complex setup.
- TGI: production-grade with built-in observability, best for enterprise deployments.
- Recommended strategy: Ollama for development/MVP, add vLLM later for high-concurrency needs.

---
**Key Takeaways (11.2 - Inference Runtimes)**
- Ollama is the easiest way to run LLMs locally
- llama.cpp is the underlying engine (Ollama wraps it)
- vLLM is faster but more complex to set up
- All expose HTTP APIs that our Python orchestrator calls

## 4.3 Model Selection & Roles

**Why**  
Using specialized models for specific tasks outperforms one large generalist, especially on constrained hardware. This section guides model selection for each role.

**What**  
Explains why specialized models beat generalists, defines role categories (orchestrator, code, creative, utility), recommends specific models for each role, and covers GPU memory management and scheduling strategies.

**Jargon**  
- **Orchestrator Model**: General-purpose model for reasoning, routing, and conversation.
- **Code Model**: Model fine-tuned specifically for programming tasks.
- **Creative Model**: Model optimized for long-form writing and creative generation.
- **Utility/Fast Model**: Small, fast model for classification, extraction, and simple tasks.
- **Hot Model**: A model kept loaded in VRAM for instant response.
- **On-Demand Model**: A model loaded only when specifically needed.
- **KV Cache**: Memory storing conversation context; grows with conversation length.

---

### 4.3.1 Specialized Models for Different Tasks

#### 4.3.1.1 Why Not One Model for Everything?

**Specialized models outperform generalists at specific tasks while using less resources.**

```
Analogy: Hiring Staff

Option A: One expensive expert who does everything "pretty well"
  â””â”€â”€ 70B generalist model (17GB VRAM, slow)

Option B: Team of specialists, each excellent at their job
  â””â”€â”€ 7B code model (4GB) + 7B chat model (4GB) + 7B creative (4GB)
  â””â”€â”€ Total: 12GB, all running simultaneously, each faster at their specialty

For our project: Option B is better
```

#### 4.3.1.2 Role Categories

| Role | What It Does | Characteristics Needed |
|------|--------------|------------------------|
| **Orchestrator** | General reasoning, routing decisions, conversation | Fast, good instruction-following |
| **Code Assistant** | Writing and explaining code | Trained on code, good at syntax |
| **Creative Writer** | Long-form content, stories, marketing | Larger context, creative outputs |
| **Utility/Fast** | Simple tasks: classification, extraction, yes/no | Tiny, extremely fast |

---

### 4.3.2 Model Recommendations by Role

#### 4.3.2.1 Orchestrator / General Purpose

**Primary Pick: Mistral-7B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MISTRAL-7B (Q4_K_M)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters:  7.3B                                         â”‚
â”‚  VRAM:        ~4.1 GB                                      â”‚
â”‚  Speed:       ~130 tokens/sec on 3090                      â”‚
â”‚  Context:     4K tokens (limited) or 8K with some variantsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Strengths:                                                â”‚
â”‚    â€¢ Outperforms Llama2-13B despite being smaller         â”‚
â”‚    â€¢ Excellent instruction following                       â”‚
â”‚    â€¢ Very fast inference                                   â”‚
â”‚  Weaknesses:                                               â”‚
â”‚    â€¢ 4K context can be limiting for long conversations    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alternative: Llama2-13B** (when you need more capability or longer context)
- ~9 GB VRAM, ~40-50 tokens/sec
- 8K context window
- Better for complex reasoning

#### 4.3.2.2 Code Generation

**Primary Pick: CodeLlama-7B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODELLAMA-7B (Q4_K_M)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters:  7B                                           â”‚
â”‚  VRAM:        ~3.8 GB                                      â”‚
â”‚  Speed:       ~100 tokens/sec on 3090                      â”‚
â”‚  Context:     16K tokens                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Strengths:                                                â”‚
â”‚    â€¢ Fine-tuned specifically for code                      â”‚
â”‚    â€¢ Supports Python, JS, C++, and more                   â”‚
â”‚    â€¢ Large context for reading whole files                â”‚
â”‚  Weaknesses:                                               â”‚
â”‚    â€¢ Less capable at general conversation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alternatives:**
- **StarCoder-7B:** Open-source, 16K context
- **WizardCoder-15B:** Higher quality (~8-9GB), better for complex tasks

#### 4.3.2.3 Creative / Long-Form Writing

**Primary Pick: Llama2-13B or Mistral-7B**

For most creative tasks, the orchestrator model works fine. For serious long-form writing:

**Consider: Llama2-70B (4-bit)** â€” Best quality, but uses ~17-18GB
- Only load when specifically needed for creative work
- Unload other models first
- ~15 tokens/sec (slower but higher quality)

#### 4.3.2.4 Utility / Fast Tasks

**Primary Pick: Phi-4 Mini (3.8B) or Gemma-3-4B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SMALL UTILITY MODELS                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phi-4 Mini (3.8B Q4):    ~2.5 GB, ~60 tokens/sec         â”‚
â”‚  Gemma-3 4B (4-bit):      ~2.6 GB, ~200+ tokens/sec       â”‚
â”‚  Qwen2.5-3B (Q4):         ~2-3 GB, ~40 tokens/sec         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Use for:                                                  â”‚
â”‚    â€¢ Classification ("is this spam?")                      â”‚
â”‚    â€¢ Extraction ("find the date in this text")            â”‚
â”‚    â€¢ Simple Q&A                                            â”‚
â”‚    â€¢ Routing decisions                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.3.2.5 Recommended Starting Configuration

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Initial Model Setup
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: "Always Hot" + "On-Demand" Strategy

Always Loaded ("Hot"):
  â”œâ”€â”€ Mistral-7B (4GB)      â†’ General orchestrator, fast chat
  â””â”€â”€ CodeLlama-7B (4GB)    â†’ Code assistance
  Total: ~8 GB (leaves 14GB free)

Load On-Demand:
  â”œâ”€â”€ Llama2-13B (9GB)      â†’ Complex reasoning when needed
  â”œâ”€â”€ Llama2-70B (17GB)     â†’ Best quality (swap out others first)
  â””â”€â”€ SDXL (7-10GB)         â†’ Image generation

Rationale:
  â€¢ Two 7B models handle 90% of tasks
  â€¢ Fast switching between chat and code
  â€¢ Load larger models only for complex work
  â€¢ Preserves VRAM for image generation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### 4.3.3 GPU Memory Management

#### 4.3.3.1 The Loading Problem

**Models must be in VRAM to run fast.** Loading a model takes time:
- 7B model: ~3-5 seconds
- 13B model: ~5-10 seconds  
- 70B model: ~15-30 seconds

This creates a user experience challenge: if users request a model that isn't loaded, they wait.

#### 4.3.3.2 Strategies

**1. Keep "Hot" Models Resident**
```
Always keep your most-used models in VRAM:
  â€¢ Set Ollama: OLLAMA_MAX_LOADED_MODELS=2
  â€¢ These stay loaded even when idle
  â€¢ Instant response for common tasks
```

**2. On-Demand Loading with Feedback**
```
When user needs a different model:
  â€¢ Show loading indicator: "Loading creative writing model..."
  â€¢ Expected wait: 5-15 seconds
  â€¢ Consider preloading if you can predict need
```

**3. Never Use CPU Fallback for Primary Tasks**
```
CPU inference is ~6x slower:
  â€¢ GPU: 100 tokens/sec
  â€¢ CPU: ~15 tokens/sec
  
Only use CPU for:
  â€¢ Truly background tasks
  â€¢ When GPU is fully occupied with priority work
  â€¢ Emergency fallback (better slow than nothing)
```

#### 4.3.3.3 KV Cache: The Hidden Memory User

**Context uses extra VRAM beyond model weights.**

```
VRAM breakdown for a 7B model with long conversation:

  Model weights:        ~4 GB
  KV cache (context):   +2-4 GB for 4K tokens
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:                ~6-8 GB actual usage

âš ï¸ Long conversations can DOUBLE your VRAM usage!
```

ğŸ’¡ **Tip:** For multi-model setups, keep conversations shorter or implement context summarization.

---

### 4.3.4 Scheduling & Contention

#### 4.3.4.1 The Core Problem

**Only one heavy task can use the GPU efficiently at a time.** Running two things simultaneously doesn't make each run at half speedâ€”it makes both run poorly or crash.

#### 4.3.4.2 Priority Rules

```
Priority Queue (highest to lowest):

  1. Interactive Chat    â†’ User is waiting, <100ms latency matters
  2. Code Generation     â†’ User is waiting, but can tolerate 1-2sec
  3. Image Generation    â†’ User expects to wait (5-30 seconds)
  4. Background Tasks    â†’ Batch processing, can run overnight
```

#### 4.3.4.3 Practical Scheduling Pattern

```python
```
#  Pseudocode for GPU scheduling
class GPUScheduler:
    def handle_request(self, request):
        if request.priority == "interactive":
            # Pause any batch jobs
            self.pause_background_tasks()
            # Run immediately
            return self.run_now(request)
            
        elif request.priority == "image":
            if self.vram_available() < 10_GB:
                # Not enough VRAM, queue it
                return self.queue(request, 
                    message="Waiting for VRAM...")
            else:
                return self.run_now(request)
                
        else:  # background
            # Only run if GPU is idle
            if self.gpu_is_idle():
                return self.run_now(request)
            else:
                return self.queue(request)
```

---

**Key Takeaways (12.5)**
- âœ“ **Llama 3 13B** is the recommended default general model
- âœ“ **Code Llama 13B** for code tasks, 7B for autocomplete
- âœ“ **SDXL 1.0** via ComfyUI for image generation
- âœ“ Models swap in/out of VRAM based on current task
- âœ“ The 24GB RTX 3090 can handle most scenarios with smart scheduling

â”‚  â”‚ REASONING/      â”‚    â”‚ CREATIVE        â”‚                         â”‚
â”‚  â”‚ PLANNING        â”‚    â”‚ WRITING         â”‚                         â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚ Task breakdown  â”‚    â”‚ Fiction         â”‚                         â”‚
â”‚  â”‚ Decision making â”‚    â”‚ Storytelling    â”‚                         â”‚
â”‚  â”‚ Multi-step      â”‚    â”‚ Brainstorming   â”‚                         â”‚
â”‚  â”‚ planning        â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚ GPT-OSS-20B     â”‚    â”‚ NeuralStar      â”‚                         â”‚
â”‚  â”‚ DeepSeek        â”‚    â”‚ 4x7B MoE        â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.3.5.2 Expanded Model Recommendations

##### 4.3.5.2.1 General Writing & Reasoning

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **Llama 3 13B** | 13B | ~14GB (Q4) | Balanced quality/speed | Default text tasks |
| **Mistral 7B** | 7B | ~8GB (Q4) | Fast, efficient | Quick responses |
| **GPT-OSS-20B** | 20B | ~16GB | Strong reasoning | Complex planning |

ğŸ“Œ **Recommendation:** Start with **Llama 3 13B** as the default general model. Use Mistral 7B for fast, simple tasks.

##### 4.3.5.2.2 Code Generation

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **Code Llama 13B** | 13B | ~14GB (Q4) | Multi-language | Primary code model |
| **Code Llama 7B** | 7B | ~7GB (Q4) | Fast completion | Autocomplete |
| **StarCoder 15B** | 15B | ~15GB | Broad language support | Alternative |

ğŸ“Œ **Recommendation:** **Code Llama 13B** for code generation, 7B variant for real-time autocomplete.

##### 4.3.5.2.3 Image Generation

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **SDXL 1.0** | ~3B | ~10GB | Best quality | Primary image gen |
| **SD 1.5** | ~1B | ~4GB | Faster, lighter | Quick drafts |

ğŸ“Œ **Recommendation:** **SDXL 1.0** via ComfyUI for quality image generation.

##### 4.3.5.2.4 Creative Writing (Specialized)

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **NeuralStar AlphaWriter 4x7B** | 24B MoE | ~20GB (Q4) | Fiction-tuned | Stories, creative |

â”€â”€â”€ Nice to Know â”€â”€â”€

> **MoE (Mixture of Experts)** means the model has multiple "expert" sub-models inside. Only some experts activate for each request, making it more efficient than a dense 24B model.

---

#### 4.3.5.3 Memory Budget Planning

â•â•â• CORE CONCEPT â•â•â•

> **You can't run all models at once.** With 24GB VRAM on an RTX 3090, plan which models are loaded when:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VRAM BUDGET (24GB RTX 3090)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SCENARIO A: Text-focused work                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚   Llama 3 13B (14GB)        â”‚     Free (10GB)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  SCENARIO B: Image generation                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚         SDXL (10GB)         â”‚ Mistral 7B â”‚ Free  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  SCENARIO C: Code + Chat                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚   Code Llama 13B    â”‚   Mistral 7B   â”‚   Free    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  âš¡ Models swap in/out based on task                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.3.6 Local Model Runtimes

**A "runtime" is the software that loads AI models and runs them. Different runtimes have different strengths.**

#### 4.3.6.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Ollama** | Easy-to-use model runner, like "Docker for AI models" | Simplest way to run local LLMs |
| **vLLM** | High-performance model server from Berkeley | Best for production, supports batching |
| **llama.cpp** | Efficient CPU/GPU inference, uses GGUF format | Most flexible for quantized models |
| **ComfyUI** | Node-based UI for Stable Diffusion | Best for image generation workflows |
| **TGI** | HuggingFace's text generation server | Alternative to vLLM |

---

#### 4.3.6.2 Runtime Comparison

| Runtime | Ease of Use | Performance | Flexibility | Best For |
|---------|-------------|-------------|-------------|----------|
| **Ollama** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | Quick start, development |
| **vLLM** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Production, high throughput |
| **llama.cpp** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Custom setups, edge cases |
| **ComfyUI** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Image generation (required) |

**Key Takeaways (12.7)**
- âœ“ Cloud APIs for planning and complex reasoning (paid but smart)
- âœ“ Local models for execution and bulk work (free)
- âœ“ Automatic fallback when local quality is insufficient
- âœ“ User can override to force local or cloud

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ CLOUD (GPT-4)   â”‚            â”‚ LOCAL (Llama)   â”‚        â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚        â”‚
â”‚  â”‚ Create outline  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Write sections  â”‚        â”‚
â”‚  â”‚ and strategy    â”‚            â”‚ based on        â”‚        â”‚
â”‚  â”‚ framework       â”‚            â”‚ outline         â”‚        â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚        â”‚
â”‚  â”‚ Cost: ~$0.10    â”‚            â”‚ Cost: $0.00     â”‚        â”‚
â”‚  â”‚ (one-time)      â”‚            â”‚ (unlimited)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.3.8 ComfyUI Workflow Integration

**ComfyUI is a node-based tool for creating images with AI. Instead of just typing a prompt, you can build complex image processing pipelines.**

#### 4.3.8.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **ComfyUI** | Visual tool for building AI image generation workflows | Our image generation backend |
| **Workflow** | A saved pipeline of image processing steps | Can be triggered programmatically |
| **Node** | One step in the pipeline (like "load model" or "apply style") | Building blocks of workflows |
| **Checkpoint** | A saved AI model file | SDXL base, custom fine-tunes |
| **ControlNet** | Guides image generation with poses, edges, etc. | Advanced control over output |

---

#### 4.3.8.2 Why ComfyUI?

â•â•â• CORE CONCEPT â•â•â•

> ComfyUI workflows are **saved as JSON** and can be **triggered via API**. This means:
> 1. Design complex pipelines visually
> 2. Save them as templates
> 3. Trigger from Handshake with different prompts
> 4. Receive generated images back

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMFYUI INTEGRATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  USER IN HANDSHAKE                                          â”‚
â”‚  "Generate a logo for my startup"                           â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PYTHON ORCHESTRATOR                     â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  1. Pick workflow: "logo_generation.json"           â”‚    â”‚
â”‚  â”‚  2. Insert prompt into workflow                     â”‚    â”‚
â”‚  â”‚  3. POST to ComfyUI API                             â”‚    â”‚
â”‚  â”‚  4. Poll for completion                             â”‚    â”‚
â”‚  â”‚  5. Retrieve generated image                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              COMFYUI (localhost:8188)                â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  [Load SDXL]â”€â”€â–¶[CLIP Encode]â”€â”€â–¶[KSampler]â”€â”€â–¶[Save] â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  IMAGE RETURNED + SAVED WITH METADATA                       â”‚
â”‚  (prompt, seed, settings stored in sidecar JSON)           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 4.3.8.3 Pre-Built Workflows to Create

| Workflow | Description | Use Case |
|----------|-------------|----------|
| **txt2img_basic** | Simple text to image | Quick generations |
| **txt2img_quality** | High quality with refiner | Final outputs |
| **img2img** | Modify existing image | Variations |
| **inpaint** | Edit parts of image | Touch-ups |
| **upscale** | Increase resolution | Print-ready |

---
**Key Takeaways (12.8)**
- âœ“ **ComfyUI** runs as a separate service, controlled via API
- âœ“ Workflows are JSON files that can be version controlled
- âœ“ Generated images stored with full metadata for reproducibility
- âœ“ Can build progressively complex workflows over time

## 4.4 Image Generation (Stable Diffusion)

**Why**  
Image generation is a key capability for creative workflows. This section covers how to integrate Stable Diffusion alongside LLM workloads without resource conflicts.

**What**  
Compares SD 1.5 vs SDXL (speed, quality, VRAM), details VRAM requirements and performance, and provides strategies for integrating image generation with LLM workloads.

**Jargon**  
- **Stable Diffusion (SD)**: Open-source AI that generates images from text descriptions.
- **SD 1.5/2.1**: Older versions; smaller, faster, 512Ã—512 output.
- **SDXL**: Newest version; higher quality, 1024Ã—1024 output, heavier.
- **ComfyUI**: Visual workflow tool for Stable Diffusion; more efficient than Automatic1111.
- **Automatic1111**: Popular SD web interface; less efficient but feature-rich.
- **Steps**: Number of denoising iterations; more steps = higher quality but slower.
- **Refiner**: SDXL's second-stage model that adds fine details.

---

### 4.4.1 SD vs SDXL Overview

#### 4.4.1.1 Quick Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚    SD 1.5/2.1    â”‚         SDXL             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output Size    â”‚ 512Ã—512          â”‚ 1024Ã—1024                â”‚
â”‚ VRAM Needed    â”‚ 6-8 GB           â”‚ 7-16 GB (varies)         â”‚
â”‚ Speed (3090)   â”‚ ~0.2-0.3s/image  â”‚ ~4-10s/image             â”‚
â”‚ Quality        â”‚ Good             â”‚ Excellent                â”‚
â”‚ Best For       â”‚ Quick previews   â”‚ Final outputs            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.4.2 VRAM Requirements & Performance

#### 4.4.2.1 Detailed VRAM Breakdown

```
SD 1.5 (512Ã—512, 25 steps):
  â€¢ VRAM:    ~6-8 GB
  â€¢ Speed:   ~0.2-0.3 seconds per image
  â€¢ Rate:    ~4-5 images/second possible on 3090

SDXL Base (1024Ã—1024, 30 steps):
  â€¢ VRAM:    ~6-14 GB (depends on optimizations)
  â€¢ Speed:   ~4-10 seconds per image
  â€¢ With optimizations (OneDiff + Tiny VAE): 
    - VRAM drops to ~6.9 GB
    - Speed improves to ~4 seconds

SDXL with Refiner:
  â€¢ VRAM:    ~7-16 GB
  â€¢ Speed:   ~6-12 seconds per image
  â€¢ Higher quality details
```

âš¡ **Key Finding:** With optimizations, SDXL can run alongside a 7B LLM (4GB + 7GB = 11GB total).

---

### 4.4.3 Integrating with LLM Workloads

#### 4.4.3.1 The Contention Problem

**Image generation and LLM inference compete for the same GPU.**

```
Scenario: User chatting while generating an image

WRONG approach (simultaneous):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Mistral-7B (4GB) + SDXL (10GB) = 14GB â”‚
  â”‚  Both running = GPU thrashing          â”‚
  â”‚  Result: Both slow, possible crash     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RIGHT approach (serialized + priority):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. Chat request arrives               â”‚
  â”‚  2. Pause/queue image generation       â”‚
  â”‚  3. Process chat (fast, <1 sec)        â”‚
  â”‚  4. Resume image generation            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.4.3.2 Recommended Strategy

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Image Generation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: ComfyUI + Sequential Processing

Setup:
  â€¢ Run ComfyUI as a separate process
  â€¢ Call it via HTTP API when images needed
  â€¢ Keep LLM models hot; unload for big image jobs

Priority:
  â€¢ Chat/code requests ALWAYS preempt image generation
  â€¢ Queue images, show progress to user
  â€¢ Run image generation when GPU is otherwise idle

VRAM Management:
  â€¢ For quick SD 1.5: Can run alongside 7B model
  â€¢ For quality SDXL: Unload secondary LLM, keep orchestrator

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

**Key Takeaways**  
- SD 1.5: Fast (~0.3s/image), lower VRAM (~6-8GB), good for quick previews.
- SDXL: Higher quality, larger output (1024Ã—1024), but needs ~7-14GB and 4-10 seconds per image.
- Never run heavy image generation and LLM inference simultaneouslyâ€”serialize with priority.
- Use ComfyUI (more efficient than Automatic1111) as a separate process with HTTP API.
- Chat/code requests always preempt image generation; queue images and show progress.

---
## 5.1 Plugin Architecture

**Why**  
Plugins transform a static application into a living platform. Understanding plugin architecture patternsâ€”both successful and cautionaryâ€”informs how to build extensibility that balances power with safety.

**What**  
Analyzes existing plugin systems (VS Code, Figma, Browser Extensions, Obsidian), designs manifest format with permission declarations, defines plugin types (automation, UI, AI tool), and specifies API patterns for registration and workspace access.

**Jargon**  
- **Plugin Manifest**: JSON file declaring plugin metadata, permissions, and contributions.
- **Contributes**: Section of manifest declaring what UI elements (commands, menus) the plugin adds.
- **Activation Events**: Triggers that cause a plugin to load (lazy loading).
- **Declarative Contributions**: UI elements defined in manifest rather than code (commands, menus, views).
- **Extension Host**: Separate process running plugin code (VS Code pattern).
- **Capability Model**: Permission system where plugins declare required access and users consent.

---

### 5.1.1 Why Plugins Matter

#### 5.1.1.1 The Power of Extensibility

**Plugins let your users (and you) add features without changing the core application.**

```
Without Plugins:
  â€¢ Every feature request requires core development
  â€¢ One-size-fits-all: everyone gets everything or nothing
  â€¢ Slow iteration: changes go through your release cycle
  â€¢ Limited: can only do what YOU thought of

With Plugins:
  â€¢ Users can add their own integrations
  â€¢ Personalization: each user's setup is unique
  â€¢ Community innovation: features you never imagined
  â€¢ Faster: plugins ship independently of core app
```

#### 5.1.1.2 Examples of Plugin Value

```
Your app with no plugins:
  â””â”€â”€ Basic AI chat + documents
  
Your app with plugins:
  â”œâ”€â”€ Todoist integration (someone's plugin)
  â”œâ”€â”€ Custom AI model loader (power user)
  â”œâ”€â”€ Citation manager (academic user)
  â”œâ”€â”€ Code formatter (developer)
  â”œâ”€â”€ Voice commands (accessibility)
  â””â”€â”€ [Hundreds more possibilities]
```

---

### 5.1.2 Learning from Existing Systems

#### 5.1.2.1 VS Code: The Gold Standard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                VS CODE EXTENSION MODEL                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Separate "Extension Host" process              â”‚
â”‚  Language:    JavaScript/TypeScript                          â”‚
â”‚  Manifest:    package.json with "contributes" section        â”‚
â”‚  Security:    No sandboxâ€”full Node.js access                â”‚
â”‚  Trust:       "Trust this publisher?" prompt                 â”‚
â”‚                                                              â”‚
â”‚  What they got right:                                        â”‚
â”‚    âœ“ Rich API for extending UI                              â”‚
â”‚    âœ“ Lazy loading (activation events)                       â”‚
â”‚    âœ“ Declarative contributions (commands, menus)            â”‚
â”‚    âœ“ Huge ecosystem (50,000+ extensions)                    â”‚
â”‚                                                              â”‚
â”‚  What we'd do differently:                                   â”‚
â”‚    â€¢ Add sandboxing (they have none)                        â”‚
â”‚    â€¢ Require permission declarations                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.1.2.2 Figma: Security-First

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FIGMA PLUGIN MODEL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Sandboxed JavaScript (no DOM, no XHR)          â”‚
â”‚  UI:          Separate iframe for plugin UI                  â”‚
â”‚  API:         Only Figma document access via figma.*         â”‚
â”‚  Network:     Must whitelist domains in manifest             â”‚
â”‚                                                              â”‚
â”‚  What they got right:                                        â”‚
â”‚    âœ“ True sandboxâ€”plugins can't escape                      â”‚
â”‚    âœ“ UI separated from logic                                â”‚
â”‚    âœ“ Explicit network permissions                           â”‚
â”‚    âœ“ User can cancel runaway plugins                        â”‚
â”‚                                                              â”‚
â”‚  What we'd adapt:                                            â”‚
â”‚    â€¢ Similar sandbox model                                   â”‚
â”‚    â€¢ Manifest-declared network permissions                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.1.2.3 Browser Extensions: Permission Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             BROWSER EXTENSION MODEL (Manifest V3)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Key Innovation: Explicit permissions                        â”‚
â”‚                                                              â”‚
â”‚  manifest.json:                                              â”‚
â”‚  {                                                           â”‚
â”‚    "permissions": ["storage", "tabs"],                      â”‚
â”‚    "host_permissions": ["https://api.example.com/*"]        â”‚
â”‚  }                                                           â”‚
â”‚                                                              â”‚
â”‚  User sees at install:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ "MyExtension" wants to:              â”‚                   â”‚
â”‚  â”‚ â€¢ Read and change your browsing data â”‚                   â”‚
â”‚  â”‚   on api.example.com                 â”‚                   â”‚
â”‚  â”‚ â€¢ Store data locally                 â”‚                   â”‚
â”‚  â”‚                                      â”‚                   â”‚
â”‚  â”‚  [Add Extension]  [Cancel]           â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  What we'd copy:                                             â”‚
â”‚    âœ“ Manifest-declared permissions                          â”‚
â”‚    âœ“ User consent at install                                â”‚
â”‚    âœ“ Clear permission descriptions                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.1.2.4 Obsidian: Cautionary Tale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OBSIDIAN PLUGIN MODEL                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Main Electron process (no isolation!)          â”‚
â”‚  Access:      Full Node.jsâ€”plugins can do ANYTHING          â”‚
â”‚  Trust:       Community ratings + open source review         â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ Security Issue:                                         â”‚
â”‚  "Obsidian plugins have all the same permissions you do     â”‚
â”‚  to read/write all the files in your vault"                 â”‚
â”‚                                                              â”‚
â”‚  A malicious plugin could:                                   â”‚
â”‚    â€¢ Read any file on your computer                         â”‚
â”‚    â€¢ Send data to external servers                          â”‚
â”‚    â€¢ Install malware                                         â”‚
â”‚    â€¢ Encrypt your files (ransomware)                        â”‚
â”‚                                                              â”‚
â”‚  What NOT to copy:                                           â”‚
â”‚    âœ— No sandboxing                                          â”‚
â”‚    âœ— Full system access                                     â”‚
â”‚    âœ— Trust based only on community review                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.1.3 Plugin Manifest & Registration

#### 5.1.3.1 Plugin Manifest Format

```json
{
  "manifestVersion": 1,
  "id": "com.example.my-plugin",
  "name": "My Awesome Plugin",
  "version": "1.2.3",
  "description": "Does something useful",
  "author": "Your Name",
  "homepage": "https://github.com/you/plugin",
  
  "minAppVersion": "2.0.0",
  "main": "dist/index.js",
  "ui": "dist/ui.html",
  
  "type": ["automation", "ui"],
  
  "permissions": {
    "readData": ["documents", "boards"],
    "writeData": ["documents"],
    "filesystem": false,
    "network": ["https://api.myservice.com"],
    "ai": {
      "models": ["local"],
      "maxTokensPerDay": 10000
    }
  },
  
  "contributes": {
    "commands": [
      {
        "id": "myplugin.doThing",
        "title": "Do the Thing",
        "shortcut": "Ctrl+Shift+T"
      }
    ],
    "menus": [
      {
        "location": "tools",
        "items": [{ "command": "myplugin.doThing" }]
      }
    ]
  }
}
```

#### 5.1.3.2 Key Manifest Sections Explained

| Section | Purpose |
|---------|---------|
| `id` | Unique identifier (reverse domain style) |
| `main` | Entry point JavaScript file |
| `ui` | Optional HTML file for plugin UI panel |
| `permissions` | What the plugin is allowed to access |
| `contributes` | What UI elements the plugin adds |

---

### 5.1.4 Plugin Types & Categories

#### 5.1.4.1 Three Main Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PLUGIN TYPES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. AUTOMATION PLUGINS                                       â”‚
â”‚     â€¢ Background tasks and macros                           â”‚
â”‚     â€¢ Triggered by events or commands                       â”‚
â”‚     â€¢ May not have UI                                        â”‚
â”‚     Example: "Auto-backup to Dropbox"                       â”‚
â”‚                                                              â”‚
â”‚  2. UI PLUGINS                                               â”‚
â”‚     â€¢ Add panels, views, or widgets                         â”‚
â”‚     â€¢ Render custom interfaces                               â”‚
â”‚     â€¢ Interact with user directly                           â”‚
â”‚     Example: "Kanban board view"                            â”‚
â”‚                                                              â”‚
â”‚  3. AI TOOL PLUGINS                                          â”‚
â”‚     â€¢ Add new AI capabilities                               â”‚
â”‚     â€¢ May integrate external models or APIs                 â”‚
â”‚     â€¢ Often combine UI + automation                         â”‚
â”‚     Example: "AI image generator", "Translation tool"       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.1.5 API Design Patterns

#### 5.1.5.1 Registration API Example

```javascript
// Plugin code (index.js)
export function activate(api) {
  // Register a command
  api.registerCommand("myplugin.sayHello", {
    title: "Say Hello",
    handler: async () => {
      api.showNotification("Hello from my plugin!");
    }
  });
  
  // Register a view
  api.registerView("myplugin.dashboard", {
    title: "My Dashboard",
    location: "sidebar",
    render: (container) => {
      container.innerHTML = "<h1>Dashboard</h1>";
    }
  });
  
  // Subscribe to events
  api.onDocumentSaved((doc) => {
    console.log("Document saved:", doc.id);
  });
}

export function deactivate() {
  // Cleanup when plugin is disabled
}
```

#### 5.1.5.2 Workspace Data API

```javascript
// Reading data
const docs = await api.workspace.query({
  type: "document",
  where: { tags: { contains: "important" } },
  limit: 10
});

// Writing data
await api.workspace.update("document", docId, {
  title: "New Title"
});

// Subscribing to changes
api.workspace.onDidChange((change) => {
  if (change.type === "document") {
    // Handle document change
  }
});
```

#### 5.1.5.3 Key Design Principles

ğŸ“Œ **Explicit Registration:** Plugins declare what they contribute via manifest AND register at runtime

ğŸ“Œ **Namespaced:** All plugin commands/views prefixed with plugin ID (`myplugin.command`)

ğŸ“Œ **Promise-based:** All async operations return Promises

ğŸ“Œ **Observable:** Plugins can subscribe to app events

ğŸ“Œ **Permission-gated:** API calls check permissions before executing

---

**Key Takeaways**  
- Plugins transform applications into platforms with community innovation and personalization.
- VS Code shows rich APIs and lazy loading; Figma shows true sandboxing; Browser extensions show permission models.
- Obsidian is a cautionary tale: no sandboxing means plugins can do anything, including ransomware.
- Plugin manifest declares metadata, permissions, and UI contributions; user consents at install.
- Three plugin types: automation (background tasks), UI (views/widgets), AI tools (model integrations).
- API design: explicit registration, namespaced commands, promise-based, observable, permission-gated.

---

## 5.2 Sandboxing & Security

**Why**  
Plugins are a major attack vector. Without sandboxing, any plugin can read files, steal data, or install malware. This section specifies how to run untrusted code safely.

**What**  
Explains why sandboxing is essential, compares sandboxing technologies (WASM, Pyodide, OS subprocess, containers), defines permission categories (filesystem, network, AI, workspace), and recommends a phased security architecture.

**Jargon**  
- **Sandbox**: Isolated environment that restricts what code can do.
- **WASM (WebAssembly)**: Binary instruction format that runs in a secure sandbox with no system access unless explicitly granted.
- **Pyodide**: Full Python interpreter compiled to WASM, enabling Python plugins with sandbox security.
- **seccomp/AppArmor/AppContainer**: OS-level sandboxing mechanisms (Linux/Windows).
- **Capability Model**: Security pattern where code receives explicit permission tokens for specific resources.
- **Default Deny**: Security stance where nothing is permitted unless explicitly granted.

---

### 5.2.1 Why Sandbox Untrusted Code

#### 5.2.1.1 The Risk

**Any code you run can do anything your user can do** (unless sandboxed).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHAT UNSANDBOXED CODE CAN DO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  âš ï¸ A malicious plugin WITHOUT sandboxing could:            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Read ANY file on the computer                            â”‚
â”‚    - Browser passwords                                       â”‚
â”‚    - SSH keys                                                â”‚
â”‚    - Financial documents                                     â”‚
â”‚                                                              â”‚
â”‚  â€¢ Send data to external servers                            â”‚
â”‚    - Steal personal information                             â”‚
â”‚    - Exfiltrate business documents                          â”‚
â”‚                                                              â”‚
â”‚  â€¢ Modify or delete files                                   â”‚
â”‚    - Ransomware (encrypt and demand payment)                â”‚
â”‚    - Destroy data                                            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Install malware                                          â”‚
â”‚    - Keyloggers                                              â”‚
â”‚    - Cryptocurrency miners                                   â”‚
â”‚                                                              â”‚
â”‚  This is NOT hypotheticalâ€”it happens regularly              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.2.1.2 Defense Layers

```
Security = Multiple Layers

Layer 1: PERMISSION MODEL
  â€¢ Plugin declares what it needs
  â€¢ User consents at install
  â€¢ App only grants what was approved

Layer 2: SANDBOX
  â€¢ Plugin code runs in isolation
  â€¢ Cannot access system outside sandbox
  â€¢ Even if code is malicious, damage is limited

Layer 3: REVIEW PROCESS
  â€¢ Marketplace review before listing
  â€¢ Automated security scanning
  â€¢ Community reporting

Layer 4: MONITORING
  â€¢ Track plugin behavior
  â€¢ Alert on suspicious activity
  â€¢ Ability to remotely disable malicious plugins
```

---

### 5.2.2 Sandboxing Technologies Compared

#### 5.2.2.1 Overview Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Technology   â”‚ Security   â”‚ Performance â”‚ Complexity   â”‚ Best For     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WASM         â”‚ â­â­â­â­â­    â”‚ â­â­â­â­      â”‚ â­â­â­ Medium  â”‚ Most plugins â”‚
â”‚ Pyodide      â”‚ â­â­â­â­â­    â”‚ â­â­â­        â”‚ â­â­â­ Medium  â”‚ Python AI    â”‚
â”‚ OS Subprocessâ”‚ â­â­â­â­      â”‚ â­â­â­â­â­     â”‚ â­â­ Complex  â”‚ Legacy code  â”‚
â”‚ Containers   â”‚ â­â­â­â­â­    â”‚ â­â­          â”‚ â­ Very High  â”‚ Heavy/risky  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.2.2.2 WebAssembly (WASM) â€” Recommended

**What it is:** A binary instruction format that runs in a secure sandbox.

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: WASM Sandbox
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Plugin code compiles to WASM (from Rust, C++, AssemblyScript)
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    YOUR APPLICATION                      â”‚
  â”‚                                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚              WASM SANDBOX                        â”‚    â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
  â”‚  â”‚  â”‚         PLUGIN CODE                       â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot access filesystem               â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot make network requests           â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot read memory outside sandbox     â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Can ONLY call functions YOU expose     â”‚  â”‚    â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
  â”‚  â”‚                                                  â”‚    â”‚
  â”‚  â”‚  Exposed Functions (your API):                  â”‚    â”‚
  â”‚  â”‚  â€¢ readDocument(id) â†’ document                  â”‚    â”‚
  â”‚  â”‚  â€¢ saveDocument(id, content)                    â”‚    â”‚
  â”‚  â”‚  â€¢ showUI(html)                                 â”‚    â”‚
  â”‚  â”‚  â€¢ [nothing elseâ€”no system access]             â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why WASM is secure:**
- Memory is completely isolated (can't read/write outside sandbox)
- No system calls unless explicitly provided
- Even buggy code can't escape
- Industry-proven (used by Figma, Cloudflare, etc.)

**Performance:**
- Near-native speed (JIT compiled)
- Fast startup (milliseconds)
- Small overhead

#### 5.2.2.3 Pyodide (Python in WASM)

**What it is:** Full Python interpreter compiled to WASM.

```
Pyodide gives you Python plugins with WASM security.

Pros:
  âœ“ Full Python ecosystem (numpy, pandas, etc.)
  âœ“ Inherits WASM sandbox properties
  âœ“ Plugin authors write normal Python

Cons:
  âœ— Slower than native Python
  âœ— Large initial download (~10MB+)
  âœ— Startup time can be significant
```

**Best for:** AI/data plugins that need Python libraries.

#### 5.2.2.4 OS Subprocess Sandboxing

**What it is:** Running plugins as separate OS processes with restricted permissions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 OS-LEVEL SANDBOXING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Main App                    Plugin Process                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           â”‚  IPC/Pipes   â”‚ Restricted by:            â”‚   â”‚
â”‚  â”‚   Your    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ seccomp (Linux)         â”‚   â”‚
â”‚  â”‚   App     â”‚              â”‚ â€¢ AppArmor (Linux)        â”‚   â”‚
â”‚  â”‚           â”‚              â”‚ â€¢ sandbox-exec (macOS)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â€¢ AppContainer (Windows)  â”‚   â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Can block:                                                  â”‚
â”‚    â€¢ File access outside allowed paths                      â”‚
â”‚    â€¢ Network access                                          â”‚
â”‚    â€¢ Process spawning                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.2.3 Permission Models

#### 5.2.3.1 Capability Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PERMISSION CATEGORIES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  FILESYSTEM SCOPES                                           â”‚
â”‚  â”œâ”€â”€ fs.read[/workspace/*]     Read specific paths          â”‚
â”‚  â”œâ”€â”€ fs.write[/workspace/out]  Write to specific paths      â”‚
â”‚  â””â”€â”€ fs.none                   No filesystem access         â”‚
â”‚                                                              â”‚
â”‚  NETWORK SCOPES                                              â”‚
â”‚  â”œâ”€â”€ net.none                  No network (default)         â”‚
â”‚  â”œâ”€â”€ net.host[api.example.com] Specific domains only        â”‚
â”‚  â””â”€â”€ net.any                   Unrestricted (dangerous)     â”‚
â”‚                                                              â”‚
â”‚  AI/MODEL SCOPES                                             â”‚
â”‚  â”œâ”€â”€ ai.none                   Cannot use AI                â”‚
â”‚  â”œâ”€â”€ ai.local                  Local models only            â”‚
â”‚  â”œâ”€â”€ ai.cloud                  Can call cloud APIs          â”‚
â”‚  â””â”€â”€ ai.budget[10000]          Token limit per day          â”‚
â”‚                                                              â”‚
â”‚  WORKSPACE DATA SCOPES                                       â”‚
â”‚  â”œâ”€â”€ workspace.read            Read documents/boards        â”‚
â”‚  â”œâ”€â”€ workspace.write           Modify data                  â”‚
â”‚  â””â”€â”€ workspace.none            No access to user data       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.2.3.2 Install-Time Permission Dialog

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Install "AI Writing Assistant"?                            â”‚
â”‚                                                              â”‚
â”‚  This plugin requests:                                       â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ Read your documents                                      â”‚
â”‚     To analyze and improve your writing                     â”‚
â”‚                                                              â”‚
â”‚  ğŸŒ Network access to api.grammarly.com                     â”‚
â”‚     To check grammar and spelling                           â”‚
â”‚                                                              â”‚
â”‚  ğŸ¤– Use local AI models                                      â”‚
â”‚     To generate writing suggestions                         â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ This plugin cannot:                                     â”‚
â”‚     â€¢ Access files outside your workspace                   â”‚
â”‚     â€¢ Access other websites                                 â”‚
â”‚     â€¢ Modify system settings                                â”‚
â”‚                                                              â”‚
â”‚         [Cancel]                [Install]                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.2.4 Recommended Security Architecture

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Security Architecture
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: WASM-First with Permission Model

Phase 1 (Internal Plugins):
  â””â”€â”€ Simple process isolation
      â€¢ Run plugins as subprocesses
      â€¢ Limit via OS mechanisms where easy
      â€¢ Internal plugins are trusted (from your team)

Phase 2 (Community Plugins):
  â””â”€â”€ WASM sandbox for all third-party code
      â€¢ Compile plugins to WASM
      â€¢ Expose only necessary APIs
      â€¢ Manifest-declared permissions
      â€¢ User consent dialog at install

Phase 3 (Marketplace):
  â””â”€â”€ Full security pipeline
      â€¢ Automated security scanning
      â€¢ Manual review for sensitive permissions
      â€¢ Code signing
      â€¢ Remote disable capability

DEFAULT STANCE: Deny Everything
  â€¢ No filesystem access by default
  â€¢ No network by default
  â€¢ No AI access by default
  â€¢ Plugin must request; user must grant

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

**Key Takeaways**  
- Unsandboxed plugins can read any file, steal data, install malware, or ransomware your files.
- Defense requires multiple layers: permission model + sandbox + review process + monitoring.
- WASM provides strong, proven isolation with near-native performanceâ€”recommended for most plugins.
- Permission categories: filesystem, network, AI/models, workspace dataâ€”each with scoped grants.
- Default deny: plugins get nothing unless they request it in manifest and user approves.
- Phased approach: start with subprocess isolation for internal plugins, add WASM for community plugins.

---

## 5.3 AI Observability

**Why**  
AI systems are probabilisticâ€”the same input can produce different outputs. Traditional debugging doesn't apply. This section defines what to monitor and how to debug AI behavior.

**What**  
Explains why AI needs different observability, defines key metrics (performance, resource, quality, cost), compares tools (OpenTelemetry + Prometheus vs Langfuse vs LangSmith), covers privacy-sensitive logging, and provides dashboard/instrumentation examples.

**Jargon**  
- **Observability**: The ability to understand internal system state from external outputs (metrics, logs, traces).
- **OpenTelemetry (OTel)**: Vendor-neutral standard for collecting metrics, logs, and traces.
- **Prometheus**: Time-series database for storing metrics.
- **Grafana**: Visualization tool for metrics dashboards.
- **Langfuse**: Open-source LLM observability platform (self-hostable).
- **Trace**: End-to-end record of a request's path through the system.
- **Span**: A single unit of work within a trace.

---

### 5.3.1 What to Monitor in AI Apps

#### 5.3.1.1 Why AI Needs Different Observability

**Traditional apps are deterministic; AI apps are probabilistic.** The same input might produce different outputs. This makes debugging harder.

```
Traditional App:
  Input: login("user", "pass")
  Output: Always same result (success or specific error)
  
AI App:
  Input: "Write me a poem about cats"
  Output: Different poem every time
  Problem: How do you know if it's working "correctly"?
```

#### 5.3.1.2 Key Metrics to Track

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI OBSERVABILITY METRICS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PERFORMANCE METRICS                                         â”‚
â”‚  â”œâ”€â”€ Latency (p50, p95, p99)   How long requests take       â”‚
â”‚  â”œâ”€â”€ Tokens per second         Throughput measure           â”‚
â”‚  â”œâ”€â”€ Time to first token       Perceived responsiveness     â”‚
â”‚  â””â”€â”€ Queue depth               Backlog of requests          â”‚
â”‚                                                              â”‚
â”‚  RESOURCE METRICS                                            â”‚
â”‚  â”œâ”€â”€ GPU memory usage          Are we close to OOM?         â”‚
â”‚  â”œâ”€â”€ GPU utilization %         Is GPU being used?           â”‚
â”‚  â”œâ”€â”€ CPU/RAM usage             System health                â”‚
â”‚  â””â”€â”€ Model load/unload events  Memory management working?   â”‚
â”‚                                                              â”‚
â”‚  QUALITY SIGNALS                                             â”‚
â”‚  â”œâ”€â”€ Error rate                Model failures               â”‚
â”‚  â”œâ”€â”€ Retry rate                Had to try again             â”‚
â”‚  â”œâ”€â”€ Fallback rate             Localâ†’cloud switches         â”‚
â”‚  â”œâ”€â”€ User feedback             Thumbs up/down               â”‚
â”‚  â””â”€â”€ Task completion           Did user accomplish goal?    â”‚
â”‚                                                              â”‚
â”‚  COST METRICS (if using cloud APIs)                         â”‚
â”‚  â”œâ”€â”€ Tokens consumed           Input + output               â”‚
â”‚  â”œâ”€â”€ API spend                 Actual money                 â”‚
â”‚  â””â”€â”€ Local vs cloud ratio      How much offloaded?         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.3.2 Tools Comparison

#### 5.3.2.1 Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool        â”‚ Type           â”‚ Local-First? â”‚ Best For      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OTel+Prom   â”‚ General obs.   â”‚ âœ“ Yes        â”‚ Core metrics  â”‚
â”‚ Langfuse    â”‚ LLM-specific   â”‚ Self-hosted  â”‚ Full tracing  â”‚
â”‚ LangSmith   â”‚ LLM-specific   â”‚ Cloud only   â”‚ LangChain     â”‚
â”‚ Helicone    â”‚ LLM proxy      â”‚ Self-hosted  â”‚ Caching       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.3.2.2 OpenTelemetry + Prometheus + Grafana â€” Recommended Core

**What it is:** Industry-standard observability stack.

```
The "boring but reliable" choice:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Your App â”€â”€â–º OpenTelemetry â”€â”€â–º Prometheus â”€â”€â–º Grafana     â”‚
â”‚  (metrics)    (collection)      (storage)     (dashboards) â”‚
â”‚                                                              â”‚
â”‚  Also:                                                       â”‚
â”‚  Your App â”€â”€â–º OTel â”€â”€â–º Jaeger/Tempo â”€â”€â–º Grafana            â”‚
â”‚  (traces)                (storage)      (visualization)     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- â­ Fully localâ€”no data leaves your machine
- â­ Vendor-neutral standard
- â­ Works with any backend (vLLM, TGI expose Prometheus metrics)
- â­ Flexibleâ€”you define what to track

**Cons:**
- âš ï¸ No LLM-specific features out of box
- âš ï¸ Must design your own metrics/spans
- âš ï¸ Setup requires several components

#### 5.3.2.3 Langfuse â€” Best LLM-Specific (Self-Hosted)

**What it is:** Open-source LLM observability platform.

```
Langfuse tracks:
  â€¢ Every prompt and response
  â€¢ Token counts and costs
  â€¢ Latency breakdowns
  â€¢ Tool calls within agents
  â€¢ User feedback
```

**Pros:**
- â­ Open-source, self-hostable
- â­ Purpose-built for LLM debugging
- â­ Tracks costs and tokens automatically
- â­ Integrates via OpenTelemetry

**Cons:**
- âš ï¸ Requires running Postgres + Langfuse server
- âš ï¸ Heavier setup than plain OTel

---

### 5.3.3 Privacy-Sensitive Logging

#### 5.3.3.1 The Problem

**LLM logs contain user prompts, which may contain sensitive information.**

```
Example dangerous log:

{
  "timestamp": "2024-01-15T10:30:00Z",
  "prompt": "Write an email to john.doe@company.com about 
             my salary negotiation. My current salary is 
             $85,000 and I want to ask for $100,000",
  "response": "..."
}

This log contains:
  â€¢ Email address (PII)
  â€¢ Salary information (sensitive)
  â€¢ Professional context (private)
```

#### 5.3.3.2 Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PRIVACY-SAFE LOGGING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. REDACT BEFORE LOGGING                                    â”‚
â”‚     â€¢ Use regex/libraries to detect PII                     â”‚
â”‚     â€¢ Replace: "john.doe@company.com" â†’ "[EMAIL]"           â”‚
â”‚     â€¢ Tools: llm-guard Anonymize scanner                    â”‚
â”‚                                                              â”‚
â”‚  2. LOG METADATA, NOT CONTENT                                â”‚
â”‚     Good: { task: "email_draft", tokens_in: 50, success: T }â”‚
â”‚     Bad:  { prompt: "Write email to john...", ... }         â”‚
â”‚                                                              â”‚
â”‚  3. SAMPLE, DON'T LOG EVERYTHING                             â”‚
â”‚     â€¢ Log 10% of interactions for debugging                 â”‚
â”‚     â€¢ Full logs only with explicit user consent             â”‚
â”‚                                                              â”‚
â”‚  4. SHORT RETENTION                                          â”‚
â”‚     â€¢ Delete detailed logs after 7-30 days                  â”‚
â”‚     â€¢ Keep aggregated metrics longer                        â”‚
â”‚                                                              â”‚
â”‚  5. LOCAL ONLY                                               â”‚
â”‚     â€¢ Never send raw prompts to cloud services              â”‚
â”‚     â€¢ If cloud needed, anonymize first                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.3.3.3 Safe Logging Schema

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "task_type": "email_draft",
  "agent": "writing_assistant",
  "model": "mistral-7b",
  "tokens_in": 50,
  "tokens_out": 120,
  "latency_ms": 850,
  "success": true,
  "error": null,
  "pii_detected": false,
  "user_feedback": null
}
```

Note: No actual prompt or response content logged.

---

### 5.3.4 Metrics & Dashboards

#### 5.3.4.1 Essential Dashboard Panels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRAFANA DASHBOARD                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ROW 1: HEALTH AT A GLANCE                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Requests/minâ”‚ â”‚ Error Rate  â”‚ â”‚ p95 Latency â”‚            â”‚
â”‚  â”‚    42       â”‚ â”‚   0.5%      â”‚ â”‚   850ms     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  ROW 2: LATENCY OVER TIME                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  â”€â”€â”€â”€p50   â”€â”€â”€â”€p95   â”€â”€â”€â”€p99                 â”‚           â”‚
â”‚  â”‚     â•­â”€â”€â”€â”€â”€â”€â•®      â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                â”‚           â”‚
â”‚  â”‚  â”€â”€â”€â•¯      â•°â”€â”€â”€â”€â”€â”€â•¯         â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â”‚  ROW 3: RESOURCES                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ GPU Memory           â”‚ â”‚ GPU Utilization      â”‚          â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 75%    â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 67%     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚  ROW 4: BY MODEL                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Model      â”‚ Requests â”‚ Avg Latency â”‚ Errors â”‚           â”‚
â”‚  â”‚ mistral-7b â”‚ 1,234    â”‚ 340ms       â”‚ 0.2%   â”‚           â”‚
â”‚  â”‚ codellama  â”‚ 567      â”‚ 520ms       â”‚ 0.8%   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.3.4.2 Instrumentation Example

```python
from opentelemetry import trace, metrics

tracer = trace.get_tracer(__name__)
meter = metrics.get_meter(__name__)

```
#  Define metrics
request_counter = meter.create_counter(
    "llm_requests_total",
    description="Total LLM requests"
)
latency_histogram = meter.create_histogram(
    "llm_latency_seconds",
    description="LLM request latency"
)

```
#  Instrument a function
async def call_llm(prompt, model):
    with tracer.start_as_current_span("llm_call") as span:
        span.set_attribute("model", model)
        
        start = time.time()
        try:
            response = await model.generate(prompt)
            
            request_counter.add(1, {"model": model, "status": "success"})
            latency_histogram.record(time.time() - start, {"model": model})
            
            return response
        except Exception as e:
            request_counter.add(1, {"model": model, "status": "error"})
            span.record_exception(e)
            raise
```

---

**Key Takeaways**  
- AI apps are probabilistic; traditional debugging doesn't workâ€”observability is essential.
- Track four metric categories: performance (latency, throughput), resources (GPU/memory), quality signals (errors, feedback), cost (tokens, API spend).
- OpenTelemetry + Prometheus + Grafana is the recommended local-first stack; add Langfuse for LLM-specific tracing.
- Privacy: log metadata not content; redact PII; sample instead of logging everything; short retention.
- Build dashboards showing health at a glance, latency over time, resource usage, and per-model breakdowns.

---

## 5.4 Evaluation & Quality

**Why**  
LLM outputs are non-deterministicâ€”traditional unit tests with exact expected values don't work. This section defines testing strategies for AI systems.

**What**  
Addresses the challenge of testing non-deterministic outputs, introduces testing strategies (golden test suites, property-based tests, LLM-as-judge), and covers multi-agent tracing for complex workflows.

**Jargon**  
- **Golden Test Suite**: Set of representative prompts with expected properties or keywords to verify.
- **Property-Based Test**: Test that checks structural properties (valid JSON, contains keys) rather than exact content.
- **LLM-as-Judge**: Using another LLM to rate output quality on criteria like correctness and helpfulness.
- **Multi-Agent Tracing**: Tracking request flow through multiple agents/models to debug complex systems.

---

### 5.4.1 Testing LLM Outputs

#### 5.4.1.1 The Challenge

**LLM outputs are non-deterministic.** Traditional unit tests expect exact outputs:

```python
```
#  Traditional test (deterministic)
def test_add():
    assert add(2, 3) == 5  # Always passes or fails consistently

```
#  LLM test (non-deterministic)
def test_poem():
    poem = llm("Write a haiku about code")
    assert poem == "???"  # What do we check?
```

#### 5.4.1.2 Testing Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM TESTING STRATEGIES                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. GOLDEN TEST SUITES                                       â”‚
â”‚     â€¢ Define representative test prompts                    â”‚
â”‚     â€¢ For deterministic tasks: check exact output           â”‚
â”‚     â€¢ For generative tasks: check key properties            â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Prompt: "What is 2+2?"                                   â”‚
â”‚    Assert: "4" in response.lower()                          â”‚
â”‚                                                              â”‚
â”‚  2. PROPERTY-BASED TESTS                                     â”‚
â”‚     â€¢ Check structural properties, not exact content        â”‚
â”‚     â€¢ Response length in expected range                     â”‚
â”‚     â€¢ Contains required keywords                            â”‚
â”‚     â€¢ Valid JSON/format                                     â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Prompt: "Write JSON with name and age"                   â”‚
â”‚    Assert: valid JSON, has "name" key, has "age" key        â”‚
â”‚                                                              â”‚
â”‚  3. LLM-AS-JUDGE                                             â”‚
â”‚     â€¢ Use another LLM to evaluate output quality            â”‚
â”‚     â€¢ Rate on criteria: correctness, coherence, helpfulness â”‚
â”‚     â€¢ Scalable but adds latency/cost                        â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Ask GPT-4: "Rate this response 1-5 for helpfulness: ..." â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.4.1.3 Golden Test Example

```python
```
#  tests/test_llm_golden.py

GOLDEN_TESTS = [
    {
        "name": "math_simple",
        "prompt": "What is 15 + 27?",
        "expected_contains": ["42"],
    },
    {
        "name": "code_function",
        "prompt": "Write a Python function that adds two numbers",
        "expected_contains": ["def ", "return"],
    },
    {
        "name": "json_extraction",
        "prompt": "Extract the name and date from: 'Meeting with Alice on Jan 5th'",
        "validate": lambda r: "alice" in r.lower() and "jan" in r.lower(),
    },
]

def test_golden_suite():
    for test in GOLDEN_TESTS:
        response = call_llm(test["prompt"])
        
        if "expected_contains" in test:
            for expected in test["expected_contains"]:
                assert expected in response, f"Failed {test['name']}"
        
        if "validate" in test:
            assert test["validate"](response), f"Failed {test['name']}"
```

---

### 5.4.2 Multi-Agent Tracing

#### 5.4.2.1 The Complexity

**Multi-agent systems have many components talking to each other.** Debugging requires seeing the full flow.

```
User Request: "Summarize this document and create action items"

Agent Flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Orchestratorâ”‚â”€â”€â–º "This needs summarization + extraction"
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Summaryâ”‚ â”‚Extractorâ”‚
â”‚ Agent â”‚ â”‚  Agent  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Mistralâ”‚ â”‚CodeLlamaâ”‚
â”‚  LLM  â”‚ â”‚   LLM   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Combine â”‚
    â”‚ Results â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.4.2.2 Tracing with OpenTelemetry

```python
```
#  Each agent action becomes a span
with tracer.start_as_current_span("user_request") as root:
    root.set_attribute("request_type", "summarize_and_extract")
    
    with tracer.start_as_current_span("orchestrator_decision") as span:
        span.set_attribute("decision", "parallel_agents")
    
    # These run in parallel but are child spans
    with tracer.start_as_current_span("summary_agent") as span:
        with tracer.start_as_current_span("llm_call_mistral") as llm:
            summary = await call_mistral(document)
            
    with tracer.start_as_current_span("extractor_agent") as span:
        with tracer.start_as_current_span("llm_call_codellama") as llm:
            actions = await call_codellama(document)
    
    with tracer.start_as_current_span("combine_results") as span:
        result = combine(summary, actions)
```

#### 5.4.2.3 Trace Visualization

```
In Jaeger/Tempo, you'd see:

user_request                     [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•] 2.5s
  â””â”€ orchestrator_decision       [â•â•]                               0.1s
  â””â”€ summary_agent               [â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                   1.2s
       â””â”€ llm_call_mistral       [â•â•â•â•â•â•â•â•â•â•â•â•]                     1.0s
  â””â”€ extractor_agent             [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                 1.5s
       â””â”€ llm_call_codellama     [â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                   1.3s
  â””â”€ combine_results             [â•â•]                               0.1s
```

---

**Key Takeaways**  
- LLM outputs are non-deterministic; traditional exact-match tests don't work.
- Three strategies: golden test suites (check key properties/keywords), property-based tests (check structure), LLM-as-judge (rate quality).
- Multi-agent systems need full flow tracingâ€”use OpenTelemetry spans to track each agent and model call.
- Trace visualization shows timing breakdown and helps identify bottlenecks in complex workflows.

---


### 5.4.6 Governance Compliance Tests

These tests verify that Handshake correctly implements the governance rules from the Bootloader and Execution Charter.

#### 5.4.6.1 Bootloader Compliance Tests

```rust
// tests/bootloader_compliance.rs

#[cfg(test)]
mod bootloader_tests {
    use super::*;

    /// BL-30: Challenge hidden assumptions before planning
    #[test]
    fn test_bl_30_challenge_assumptions() {
        let job = Job::non_trivial().with_no_assumptions();
        let validator = ChallengeFirstValidator;
        let result = validator.validate(&job, &context());
        
        assert!(result.has_warning("BL-30"));
    }
    
    /// BL-104: Flight Recorder is append-only
    #[test]
    fn test_bl_104_append_only() {
        let mut recorder = FlightRecorder::new();
        recorder.append(test_entry()).unwrap();
        
        // These methods should not exist (compile-time guarantee)
        // recorder.delete(step_id);  // Compile error
        // recorder.update(step_id, new_entry);  // Compile error
        
        // Can only read and append
        assert!(recorder.entries().len() == 1);
    }
    
    /// BL-272: No unanchored operations
    #[test]
    fn test_bl_272_no_unanchored() {
        let result = PlannedOperation::new(
            OperationType::Insert,
            EntityRef::empty(),  // Unanchored - should fail
            LocationSelector::default(),
            None,
            ContentSnapshot::new("content"),
            "reason",
        );
        
        assert!(matches!(result, Err(Error::UnanchoredOperation { clause: "BL-272" })));
    }
    
    /// BL-274: Deletions must be explicit with before content
    #[test]
    fn test_bl_274_explicit_delete() {
        let op = PlannedOperation::delete(
            entity_ref(),
            location(),
            ContentSnapshot::new("old content"),
            "reason for deletion",
        );
        
        assert_eq!(op.operation_type, OperationType::Delete);
        assert!(op.before.is_some());
        assert!(op.after.is_empty());
        assert!(!op.risks.is_empty());
    }
    
    /// BL-24: Flight Recorder cannot be disabled
    #[test]
    fn test_bl_24_recorder_mandatory() {
        let runtime = Runtime::new(config());
        
        // Flight Recorder is always present
        assert!(runtime.flight_recorder().is_some());
        
        // Cannot disable it
        let result = runtime.disable_flight_recorder();
        assert!(result.is_err());
    }
}
```

#### 5.4.6.2 Execution Charter Compliance Tests

```rust
// tests/execution_charter_compliance.rs

#[cfg(test)]
mod execution_charter_tests {
    use super::*;

    /// EXEC-003: Default to activated mode
    #[test]
    fn test_exec_003_default_activated() {
        let mode = ProjectMode::default();
        assert_eq!(mode, ProjectMode::Activated);
    }
    
    /// EXEC-024/028: Layer 1 is immutable
    #[test]
    fn test_exec_024_l1_immutable() {
        let guard = LayerGuard;
        let operation = PlannedOperation::write_to(Layer::L1);
        
        let result = guard.check_write(Layer::L1, &operation);
        assert!(result.is_err());
        assert!(result.unwrap_err().code == "EXEC-024/028");
    }
    
    /// EXEC-043: No structural edits in non-structural modes
    #[test]
    fn test_exec_043_no_structural_in_chat() {
        let engine = EscalationEngine;
        let operation = PlannedOperation::structural();
        
        for mode in [WorkMode::Chat, WorkMode::Data, WorkMode::Brainstorm] {
            let result = engine.validate_mode_allows_structural(mode, &operation);
            assert!(result.is_err());
            assert!(result.unwrap_err().code == "EXEC-043");
        }
    }
    
    /// EXEC-050: Cannot fabricate non-existent references
    #[test]
    fn test_exec_050_no_fabrication() {
        let mut discovery = ReferenceDiscovery::new();
        discovery.full_scan(&empty_index()).unwrap();
        
        let result = discovery.get_reference(&RefId::new("FAKE-001"));
        assert!(result.is_err());
        assert!(result.unwrap_err().code == "EXEC-050");
    }
    
    /// EXEC-073: No governed tasks during drift state
    #[test]
    fn test_exec_073_drift_blocks_tasks() {
        let mut handler = DriftHandler::new();
        handler.set_drift_detected(true);
        
        let result = handler.can_execute_governed_task();
        assert!(result.is_err());
        assert!(result.unwrap_err().code == "EXEC-073");
    }
}
```

#### 5.4.6.3 COR-701 Compliance Tests

```rust
// tests/cor701_compliance.rs

#[cfg(test)]
mod cor701_tests {
    use super::*;

    /// C701-50: Anchors must be present
    #[test]
    fn test_c701_50_anchors_present() {
        let step = Step::new()
            .with_anchor("nonexistent-anchor");
        let context = Context::with_empty_document();
        
        let gate = AnchorsPresent;
        let result = gate.check(&step, &context);
        
        assert!(!result.unwrap().passed);
    }
    
    /// C701-52: Content outside window unchanged
    #[test]
    fn test_c701_52_boundary_protection() {
        let mut document = Document::with_content("AAABBBCCC");
        let step = Step::new()
            .with_window(Window::new(3, 6))  // Target "BBB"
            .with_after("XXX");
        
        // Simulate edit that also modifies outside window
        document.apply_corrupt_edit(&step, true);  // Corrupts boundary
        
        let gate = ContentUntouchedOutsideWindow;
        let result = gate.check(&step, &document.context());
        
        assert!(!result.unwrap().passed);
        assert!(result.unwrap().error_code == "C701-52");
    }
    
    /// C701-60: Concurrency check
    #[test]
    fn test_c701_60_concurrency_check() {
        let document = Document::with_content("original");
        let pre_sha1 = document.compute_sha1();
        
        let step = Step::new()
            .with_pre_sha1(&pre_sha1);
        
        // Simulate concurrent modification
        document.modify_externally("modified");
        
        let gate = CurrentMatchesPreimage;
        let result = gate.check(&step, &document.context());
        
        assert!(!result.unwrap().passed);
        assert!(result.unwrap().error_code == "C701-60");
    }
    
    /// C701-47: Automatic rollback on failure
    #[test]
    fn test_c701_47_rollback() {
        let mut document = Document::with_content("original content");
        let original_sha1 = document.compute_sha1();
        
        let step = Step::new()
            .with_after("new content")
            .with_failing_gate();  // Will fail a gate
        
        let executor = MicroStepExecutor::new();
        let result = executor.execute(&step, &mut document.context());
        
        // Edit failed
        assert!(result.is_err());
        
        // But content was rolled back
        assert_eq!(document.compute_sha1(), original_sha1);
    }
    
    /// C701-181: Governed edit requires manifest
    #[test]
    fn test_c701_181_manifest_required() {
        let operation = PlannedOperation::governed()
            .without_manifest();
        
        let behavior = EditBehavior;
        let result = behavior.validate_has_manifest(&operation);
        
        assert!(result.is_err());
        assert!(result.unwrap_err().code == "C701-181");
    }
}
```


---

## 5.5 Benchmark Harness

**Why**  
Reproducible performance testing enables informed decisions about runtimes, models, and configurations. This section specifies a systematic benchmarking system.

**What**  
Describes benchmark harness architecture (config files, adapters, runners, output), provides example configurations and adapter interface, and shows reporting format for comparing runtimes/models.

**Jargon**  
- **Benchmark Harness**: Framework for running reproducible performance tests.
- **Adapter**: Code that translates generic benchmark calls to specific runtime APIs (Ollama, vLLM, TGI).
- **Scenario**: A defined test configuration (prompts, models, concurrency levels, iterations).
- **Load Sweep**: Running tests at increasing concurrency levels to measure scaling behavior.

---

### 5.5.1 Benchmark Architecture

#### 5.5.1.1 Why Build a Benchmark Harness?

**Reproducible performance testing** lets you:
- Compare runtimes (Ollama vs vLLM)
- Compare models (Mistral-7B vs Llama2-7B)
- Measure impact of configuration changes
- Track performance over time

#### 5.5.1.2 System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BENCHMARK HARNESS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  CONFIG FILES (YAML)                                         â”‚
â”‚  â”œâ”€â”€ models.yml      Model endpoints and settings           â”‚
â”‚  â”œâ”€â”€ scenarios.yml   Test scenarios to run                  â”‚
â”‚  â””â”€â”€ prompts.yml     Standard prompts for testing           â”‚
â”‚                                                              â”‚
â”‚  ADAPTERS                                                    â”‚
â”‚  â”œâ”€â”€ OllamaAdapter   Talks to Ollama                        â”‚
â”‚  â”œâ”€â”€ VLLMAdapter     Talks to vLLM                          â”‚
â”‚  â”œâ”€â”€ TGIAdapter      Talks to TGI                           â”‚
â”‚  â””â”€â”€ ImageAdapter    Talks to ComfyUI                       â”‚
â”‚                                                              â”‚
â”‚  RUNNERS                                                     â”‚
â”‚  â”œâ”€â”€ SingleLLMRunner      One model, one prompt             â”‚
â”‚  â”œâ”€â”€ ConcurrentRunner     Multiple parallel requests        â”‚
â”‚  â””â”€â”€ MixedWorkloadRunner  LLM + Image together              â”‚
â”‚                                                              â”‚
â”‚  OUTPUT                                                      â”‚
â”‚  â”œâ”€â”€ results.jsonl   Raw timing data                        â”‚
â”‚  â””â”€â”€ report.md       Summary statistics                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.5.2 Scenarios & Adapters

#### 5.5.2.1 Example Configuration

```yaml
```
#  models.yml
models:
  - id: mistral-7b-ollama
    type: ollama
    endpoint: http://localhost:11434
    model_name: mistral
    
  - id: mistral-7b-vllm
    type: vllm
    endpoint: http://localhost:8000
    model_name: mistralai/Mistral-7B-v0.1

```
#  scenarios.yml
scenarios:
  - id: single_chat
    type: single_llm
    models: [mistral-7b-ollama, mistral-7b-vllm]
    prompts: [short_qa, medium_qa, long_generation]
    iterations: 10
    
  - id: concurrent_load
    type: load_sweep
    models: [mistral-7b-vllm]
    prompts: [medium_qa]
    concurrency_levels: [1, 2, 4, 8, 16]
    iterations: 5

```
#  prompts.yml
prompts:
  - id: short_qa
    text: "What is the capital of France?"
    max_tokens: 50
    
  - id: medium_qa
    text: "Explain how photosynthesis works in 3 paragraphs."
    max_tokens: 300
```

#### 5.5.2.2 Adapter Interface

```python
```
#  adapters.py
class LLMAdapter:
    """Base class for model adapters"""
    
    async def generate(self, prompt: str, params: dict) -> Result:
        raise NotImplementedError

class OllamaAdapter(LLMAdapter):
    async def generate(self, prompt: str, params: dict) -> Result:
        start = time.time()
        response = await httpx.post(
            f"{self.endpoint}/api/generate",
            json={"model": self.model, "prompt": prompt, **params}
        )
        elapsed = time.time() - start
        
        data = response.json()
        return Result(
            text=data["response"],
            tokens_in=data["prompt_eval_count"],
            tokens_out=data["eval_count"],
            latency=elapsed
        )
```

---

### 5.5.3 Reporting & Analysis

#### 5.5.3.1 Output Format

```
```
#  Benchmark Report - 2024-01-15

### 5.5.4 Summary

| Scenario       | Model             | Avg Latency | p50    | p95    | Tokens/sec |
|----------------|-------------------|-------------|--------|--------|------------|
| single_chat    | mistral-7b-ollama | 340ms       | 320ms  | 450ms  | 88         |
| single_chat    | mistral-7b-vllm   | 310ms       | 300ms  | 420ms  | 97         |
| concurrent_8   | mistral-7b-vllm   | 180ms       | 170ms  | 250ms  | 620        |

### 5.5.5 Findings

- vLLM is ~10% faster for single requests
- vLLM scales much better under load (620 vs ~100 tokens/sec at 8 concurrent)
- Ollama shows consistent latency regardless of load (no batching)

### 5.5.6 Recommendations

- Use Ollama for development/single-user
- Use vLLM for production/multi-user scenarios
```

---

**Key Takeaways**  
- Benchmark harness enables reproducible comparison of runtimes (Ollama vs vLLM), models, and configurations.
- Architecture: YAML config files (models, scenarios, prompts) + adapters (translate to runtime APIs) + runners (execute tests) + output (raw data + summary report).
- Scenarios include single requests, concurrent load sweeps, and mixed workloads (LLM + image).
- Output reports provide summary tables with latency percentiles and tokens/sec, plus findings and recommendations.
- Key insight from example: vLLM scales ~15Ã— better under load (620 vs 41 tokens/sec at 8 concurrent).

---

---


# 6. Mechanical Integrations

## 6.1 Document Ingestion: Docling Subsystem

**Why**  
Handshake needs to ingest documents from various formats (PDF, DOCX, PPTX, etc.) and convert them into structured blocks for editing and AI processing. Docling provides MIT-licensed, layout-aware document understanding.

**What**  
Integrates IBM Docling as the primary document processor; covers media support, licensing, architecture, alternatives, performance, and RAG enhancement. This section consolidates three research artefacts on Docling.

**Jargon**  
- **Docling**: IBM's MIT-licensed document understanding library, now part of LF AI & Data foundation.
- **DoclingDocument**: Hierarchical JSON schema representing parsed document structure.
- **TableFormer**: Docling's specialized table extraction model.
- **DocLayNet**: Deep learning model for page layout segmentation.
- **VLM (Vision Language Model)**: Models that understand both images and text (Granite-Docling, SmolDocling).

---
This document bundles and preserves three related research artefacts on Docling and its integration into the Handshake workspace:

- Part I: GPT-generated Docling evaluation and integration research
- Part II: Spec-style Docling integration assessment for Handshake
- Part III: Architectural evaluation essay for Docling in Handshake

All original tables, ASCII diagrams, and schemas are preserved as-is; only heading levels were adjusted for nesting.

---

### 6.1.1 Part I â€“ Docling Evaluation for Project Handshake (GPT research v1)

### 6.1.2 Docling Evaluation for Project Handshake

**Date:** 2025-12-02  
**Target project:** Handshake (local-first Tauri + Rust + React desktop workspace)

---

#### 6.1.2.1 Executive Summary

Docling is a strong fit as the primary on-device document preprocessor for Handshake. It is MIT-licensed, actively maintained by IBM and the Docling community, and now part of the LF AI & Data foundation, which makes it suitable for commercial, closed-source desktop distribution.

Technically, Docling covers the core formats Handshake cares about (PDF, DOCX, PPTX, XLSX, HTML, Markdown, EPUB, images, CSV/JSON/XML, ZIP), with advanced PDF layout and table recovery via DocLayNet and TableFormer, OCR through multiple engines, optional ASR for audio, and VLM pipelines using Granite-Docling and related models.

However, Docling does **not** aim for â€œeverything under the sunâ€ format coverage (no native email formats, no legacy Office, no direct video container support), so it should be paired with Unstructured and/or Apache Tika for fringe formats and email ingestion.

For Handshakeâ€™s architecture, the best integration path is to run Docling as a **separate Python service** (Docling/Docling-Serve or a thin custom worker) orchestrated from Rust via HTTP or a job queue, with outputs converted into Tiptap/Yjs blocks and fed into the CRDT engine as AI-authored ops. This keeps the Rust/Tauri shell lean, isolates Python/ML dependencies, and aligns with the projectâ€™s â€œAI as CRDT participantâ€ model.

---

#### 6.1.2.2 Media Type Support Matrix

Support levels:

- âœ… Fully supported / first-class in current Docling release  
- âš ï¸ Indirect / requires extra configuration, conversion, or is not clearly documented  
- âŒ Not supported (or no credible evidence of support)  
- ğŸ—“ï¸ On roadmap / planned (when clearly stated)

All Docling support statements are based primarily on official â€œSupported formatsâ€ docs, feature descriptions, and examples.

##### 6.1.2.2.1 Document Formats

| Format            | Extension          | Support Level | Notes |
|------------------|--------------------|--------------|-------|
| PDF (native)     | `.pdf`             | âœ…            | Primary target; deep layout + table understanding (DocLayNet + TableFormer). |
| PDF (scanned)    | `.pdf`             | âœ…            | Supported via OCR; multiple OCR backends + â€œforce full page OCRâ€ pipeline. |
| Word (modern)    | `.docx`            | âœ…            | Explicitly listed as supported input; structure preserved into DoclingDocument. |
| PowerPoint       | `.pptx`            | âœ…            | Supported; slides converted into blocks and images. |
| Excel            | `.xlsx`            | âœ…            | Supported; sheets/tables extracted into structured table elements. |
| Legacy Word      | `.doc`             | âŒ            | Not listed; require pre-conversion (e.g. LibreOffice â†’ DOCX or PDF) or a fallback tool. |
| Legacy PowerPoint| `.ppt`             | âŒ            | Same as `.doc`; use external conversion or fallback. |
| Legacy Excel     | `.xls`             | âŒ            | Same as `.doc`; convert upstream. |
| OpenDocument Text| `.odt`             | âŒ            | Not mentioned in supported formats; use Unstructured/Tika if needed. |
| OpenDocument Pres| `.odp`             | âŒ            | Same as `.odt`. |
| OpenDocument Calc| `.ods`             | âŒ            | Same as `.odt`. |
| Rich Text        | `.rtf`             | âŒ            | No explicit support; use fallback. |

##### 6.1.2.2.2 Markup & Text Formats

| Format              | Extension              | Support Level | Notes |
|--------------------|------------------------|--------------|-------|
| HTML               | `.html`, `.htm`        | âœ…            | First-class input; DOM parsed into structural blocks. |
| XHTML              | `.xhtml`               | âš ï¸           | Likely handled as HTML if parsed, but not explicitly called out. |
| Markdown           | `.md`                  | âœ…            | Explicitly supported; both input and output. |
| AsciiDoc           | `.adoc`                | âŒ            | No mention; treat via external converter or LLM if needed. |
| reStructuredText   | `.rst`                 | âŒ            | Same as AsciiDoc. |
| LaTeX              | `.tex`                 | âŒ            | Not supported directly; better handled by LLM or external tools. |
| Plain Text         | `.txt`                 | âš ï¸           | Not listed as a first-class InputFormat but can be handled via â€œcustom text inputâ€ / pipeline utilities; no layout semantics. |

##### 6.1.2.2.3 Image Formats (OCR / Vision)

| Format | Extension           | Support Level | Notes |
|--------|---------------------|--------------|-------|
| PNG    | `.png`              | âœ…            | Explicitly supported for OCR and figure extraction. |
| JPEG   | `.jpg`, `.jpeg`     | âœ…            | Explicitly supported. |
| TIFF   | `.tiff`, `.tif`     | âœ…            | Explicitly supported; important for multipage scans. |
| BMP    | `.bmp`              | âš ï¸           | Not documented; may work via Pillow/ffmpeg in practice but not guaranteed. |
| WEBP   | `.webp`             | âš ï¸           | Same as BMP; not listed. |
| GIF    | `.gif`              | âš ï¸           | Static images might work if converted; no docled guarantee. |
| HEIC/HEIF | `.heic`, `.heif` | âŒ            | Not mentioned; likely require upstream conversion. |
| SVG    | `.svg`              | âŒ            | Vector; Docling focuses on raster inputs and PDF. Treat as unsupported. |

##### 6.1.2.2.4 Audio Formats (ASR)

Docling supports audio via an ASR pipeline built on Whisper; internally it uses a generic audio input type and ffmpeg for decoding.

| Format    | Extension | Support Level | Notes |
|-----------|-----------|--------------|-------|
| WAV       | `.wav`    | âš ï¸           | Likely supported via ffmpeg; not explicitly called out but ffmpeg covers it. |
| MP3       | `.mp3`    | âœ…            | Explicit example input in ASR docs and marketing text (â€œsupports audio via ASRâ€). |
| FLAC      | `.flac`   | âš ï¸           | ffmpeg-dependent; not explicitly documented. |
| OGG       | `.ogg`    | âš ï¸           | Same as FLAC. |
| M4A/AAC   | `.m4a`    | âš ï¸           | Same as FLAC; rely on ffmpeg. |
| WebM Audio| `.webm`   | âš ï¸           | If ffmpeg decodes it, ASR pipeline can consume it; not guaranteed in docs. |

Whisper gives multilingual transcription; Doclingâ€™s example focuses on English but Whisper itself supports many languages.

##### 6.1.2.2.5 Video Formats

Docling **does not** directly treat video containers as an InputFormat. Official messaging mentions â€œaudioâ€ but not video.

| Format | Extension | Support Level | Notes |
|--------|-----------|--------------|-------|
| MP4    | `.mp4`    | âŒ            | No direct support; recommended pattern: extract audio via ffmpeg â†’ feed into ASR pipeline. |
| WebM   | `.webm`   | âŒ            | Same pattern as MP4. |
| MKV    | `.mkv`    | âŒ            | Same pattern as MP4. |
| MOV    | `.mov`    | âŒ            | Same pattern as MP4. |

##### 6.1.2.2.6 Subtitle / Caption Formats

| Format | Extension | Support Level | Notes |
|--------|-----------|--------------|-------|
| WebVTT | `.vtt`    | âœ…            | Explicitly supported; treated as text with timing metadata. |
| SRT    | `.srt`    | âŒ            | Not listed; trivial to parse externally if needed. |
| ASS/SSA| `.ass`, `.ssa` | âŒ      | Not supported. |

##### 6.1.2.2.7 Data & Structured Formats

| Format | Extension | Support Level | Notes |
|--------|-----------|--------------|-------|
| CSV    | `.csv`    | âœ…            | Explicit example (â€œConversion of CSV filesâ€); integrated into pipeline. |
| TSV    | `.tsv`    | âš ï¸           | Can usually be treated as CSV with tab delimiter, but not explicitly documented. |
| JSON   | `.json`   | âœ…            | Listed as input; content from JSON values. |
| XML    | `.xml`    | âœ…            | Generic XML plus â€œCustom XMLâ€ example; needs configuration for schema-aware mapping. |
| YAML   | `.yaml`   | âŒ            | Not mentioned; convert to JSON or use separate parser. |

##### 6.1.2.2.8 Specialized / Domain-Specific

| Format       | Extension | Support Level | Notes |
|-------------|-----------|--------------|-------|
| USPTO XML   | â€“         | âš ï¸           | Can be handled as custom XML with a mapping plugin; not first-class. |
| JATS XML    | â€“         | âš ï¸           | Same as USPTO XML. |
| EPUB        | `.epub`   | âœ…            | Explicit input format. |
| MOBI        | `.mobi`   | âŒ            | Not supported; convert upstream. |
| DjVu        | `.djvu`   | âŒ            | Not supported; convert via external tools to PDF. |
| XPS         | `.xps`    | âŒ            | Not supported; convert upstream. |

##### 6.1.2.2.9 Email Formats

| Format | Extension | Support Level | Notes |
|--------|-----------|--------------|-------|
| EML    | `.eml`    | âŒ            | Not supported; Unstructured and Tika fill this gap. |
| MSG    | `.msg`    | âŒ            | Same as EML. |
| MBOX   | `.mbox`   | âŒ            | Same as EML. |

##### 6.1.2.2.10 Archive Formats

| Format        | Extension | Support Level | Notes |
|---------------|-----------|--------------|-------|
| ZIP (archive) | `.zip`    | âœ…            | Explicitly supported as container; Docling processes supported files inside. |
| PDF Portfolio | `.pdf`    | âš ï¸           | Not treated specially; likely seen as normal PDF pages; embedded attachments not handled generically. |

---

#### 6.1.2.3 Handshake Integration Assessment

This section covers both Doclingâ€™s capabilities and how they map into Handshakeâ€™s architecture.

##### 6.1.2.3.1 Licensing & Open-Source Position

- **Core Docling** (`docling`): MIT license.  
- **Docling-IBM models** (DocLayNet, TableFormer, Granite-Docling integrations): MIT license for the integration library; individual models hosted on Hugging Face carry their own model licenses (Apache-2.0 or custom IBM terms) which must be checked per model.  
- **Docling-Serve**: MIT license (server wrapper).  
- **Docling-MCP**: MIT license.  

Implications for Handshake:

- Safe for **closed-source commercial desktop distribution**; no copyleft/AGPL contamination from Docling itself.
- Care is required when adding **optional dependencies** (e.g. PyMuPDF, which is AGPL/commercial) â€“ but Doclingâ€™s main pipeline does not require PyMuPDF; its own stack is designed to be commercially friendly.  
- Model licenses (Granite-Docling, SmolDocling, Qwen2.5-VL, Pixtral, etc.) must be reviewed but typically allow commercial use with attribution.  

Conclusion: **No licensing blocker** for embedding Docling into a proprietary Tauri desktop app, provided you track model licenses and avoid AGPL-only dependencies.

##### 6.1.2.3.2 Docling Output Structure & Semantics

Doclingâ€™s core output is the `DoclingDocument` schema: a hierarchical representation of the document with pages, blocks, inlines, tables, figures, and metadata.

Key properties:

- **Hierarchy**: Document â†’ Sections â†’ Pages â†’ Blocks (paragraphs, headings, lists, tables, figures, formulas, code, etc.).  
- **Geometry**: Each block carries page index + bounding boxes (coordinates) for layout-aware features and visual canvases.  
- **Provenance**: References to source page and element IDs enable round-tripping, anchor links, and precise citations.  
- **Confidence scores**: Documented concept; per-element confidence values can be used for quality thresholds and flight recorder metrics.  
- **Serialization formats**: Markdown, HTML, plain text, JSON, DocTags (a token-friendly structured representation).  

How this helps Handshake:

- Straightforward mapping into **block-based editors** (Tiptap/ProseMirror).
- Bounding boxes provide coordinates for **canvases** (Milanote/Miro-style) and figure thumbnails.
- Provenance + confidence feed into **Raw/Derived/Display separation** and into **flight recorder** metrics.

###### Representation of specific elements

- **Tables**: Represented as structured objects with rows, columns, and cell spans, including merged cells and header hierarchies.  
- **Images/Figures**: Extracted as separate figure objects with coordinates and optional captions.  
- **Lists**: Ordered and unordered lists with proper nesting.  
- **Code blocks**: Preserved, with scope for language detection using enrichment pipelines.  
- **Footnotes/endnotes/citations**: Represented in metadata and blocks; technical report explicitly covers reference extraction (citations and bibliography).  
- **Mathematical formulas**: Identified and extractable; enrichment examples for formulas exist.  

##### 6.1.2.3.3 Table Extraction Deep Dive

Doclingâ€™s table pipeline uses a tailored TableFormer model plus layout cues:

- Recovers **complex tables** including merged cells, multi-row headers, sparsely bordered tables, and text-heavy tables.  
- Handles **multi-column documents** by pairing layout analysis (DocLayNet) with table detection.  
- Outputs structured table objects that can be exported as Markdown, HTML, CSV, or JSON.  

Technical report benchmarks show strong table accuracy across complex PDFs and improved performance vs. baseline PDF parsers; limitations still exist with extremely irregular and marketing-style tables (e.g., the â€œbank advertisementâ€ example where formatting was incorrect even though raw content was captured).  

Compared to pdfplumber, Camelot, and Tabula:

- Docling/TableFormer performs better on **unstructured, non-grid tables** and mixed layout PDFs; those tools excel on regular, gridlike tables but lack global layout context.  
- For Handshake, Docling should be the default table extractor; pdfplumber/Camelot can be optional niche tools (e.g. highly regular invoices) if ever needed.

##### 6.1.2.3.4 Layout & Visual Understanding

Doclingâ€™s core strength is **layout-aware PDF understanding**:

- Uses DocLayNet for page segmentation (text boxes, tables, figures, headings).  
- Computes **reading order**, correcting for multi-column layouts, sidebars, footers/headers.  
- Identifies figures, tables, and code blocks as distinct elements in DoclingDocument.  

Diagram/chart understanding is currently limited to figure detection + caption extraction; full semantic chart parsing is on the longer-term roadmap.  

This is highly aligned with Handshakeâ€™s needs:

- Layout metadata drives **canvas placement** and **reference views**.
- Reading order is crucial for chunking and RAG to avoid scrambled paragraphs.

##### 6.1.2.3.5 OCR Capabilities

Docling offers **pluggable OCR backends**:

- Examples for Tesseract (with automatic language detection), RapidOCR, and Surya OCR.  
- Supports OCR for scanned PDFs and stand-alone images; can force full page OCR even where embedded text exists.  
- Multilingual OCR support depends on the selected engine and installed language packs (Tesseract/Surya/RapidOCR are multilingual).  

The technical report notes that OCR is **quality-sensitive and slower** than pure text extraction, but integrated into the same pipeline and benefiting from the layout model for text placement.  

For Handshake:

- OCR output should be treated as **DerivedContent**, not canonical RawContent, and clearly marked as such (confidence + OCR flag).
- For worst-case scans, you can route through VLM (Granite-Docling / SmolDocling) instead of or in addition to OCR.

##### 6.1.2.3.6 ASR (Audio / Speech) Capabilities

Docling provides an ASR pipeline using Whisper:

- Example pipelines show `InputFormat.AUDIO` + Whisper-based transcription, producing timestamped segments.  
- Official website states â€œsupports audio via automatic speech recognition (ASR)â€.  

Current visible capabilities:

- **Multilingual** (via Whisper itself).
- **Timestamped segments**; no strong evidence of built-in speaker diarization â€“ diarization would likely need a separate tool (PyAnnote or similar).
- Same DoclingDocument abstraction is used to capture transcripts and link them to source media.

For Handshake (which already needs ASR for lectures):

- Doclingâ€™s ASR pipeline can be used for **basic transcription + segmentation**, but if you require diarization or advanced segmentation, youâ€™ll need complementary tools or your own Whisper integration.
- You can still rely on Docling for **uniform output format** and integration into the same RAG/DoclingDocument world.

##### 6.1.2.3.7 Visual Language Model (VLM) Support

Docling integrates VLMs via the `VlmPipeline`.  

Supported local models (out-of-the-box):

- Granite-Docling-258M (Transformers + MLX variants).
- SmolDocling-256M preview (Transformers + MLX).
- Several generic VLMs (Qwen2.5-VL-3B, Pixtral-12B, Gemma-3-12B, Granite-Vision-2B, Phi-4-multimodal).  

Capabilities:

- End-to-end **VLM pipelines** for PDF â†’ Markdown/DocTags/HTML (no separate OCR/layout â€“ the VLM handles everything).
- Can also route to **remote VLM services** (OpenAI-style endpoints, vLLM, Ollama, etc.).  

Implications for Handshake:

- You can configure Docling to use a **local VLM for â€œhardâ€ pages** (e.g., heavily graphical PDFs) while using the classical layout + OCR pipeline for the rest.
- The VLM pipeline may consume significant GPU resources; Handshake should treat it as a **background/off-peak operation**, scheduled against LLM workloads.

##### 6.1.2.3.8 Technical Architecture (Docling Itself)

From docs + technical report:  

- **Language**: Python, with core ML models in PyTorch and support for GPU (CUDA) and Apple MLX.
- **Key components**:
  - `docling-core`: datamodel and basic utilities.
  - `document_converter`: orchestrates parsing, layout analysis, table detection, OCR, ASR, VLM pipelines into a `DoclingDocument`.
  - Optional `docling-ibm-models` for DocLayNet/TableFormer/Granite integration.
- **Execution modes**:
  - Python API.
  - CLI (`docling`).
  - Docling-Serve (REST API); can run in Docker, K8s, or via Quarkus and other integrations.  
  - MCP server (`docling-mcp`).  

Performance:

- PDF pipeline is highly optimized: CPU-only performance is competitive; GPU acceleration yields substantial speedups in benchmarking (L4/M3 Max).  

For Handshake, the key architectural choice is **how to host Docling** (embedded Python vs sidecar vs remote server). That is addressed in 3.10.

##### 6.1.2.3.9 RAG & Embedding Compatibility

Docling includes dedicated support for chunking and RAG:  

- **Hybrid chunking**: Combines structural (headings, paragraphs) and token-based chunk sizes; tuned for LLM context windows.  
- **Serialization & Chunking examples**: Show how to emit DoclingDocument â†’ JSON/Parquet with chunks and metadata keyed by IDs.  
- **Integrations**:
  - LangChain.
  - LlamaIndex.
  - Haystack.
  - Langflow, txtai, Milvus, Qdrant, MongoDB, etc.  

For Handshake:

- You can reuse Doclingâ€™s **chunking strategies** to generate embedding chunks and store them in `sqlite-vec` / LanceDB.
- Chunk metadata includes **page, coordinates, section IDs**, which can be mapped directly into your Shadow Workspace and Knowledge Graph.

##### 6.1.2.3.10 Detailed Integration Points (10.x)

###### Rust â†” Python Bridge

Options in Handshake context:

| Approach                    | Recommendation for Handshake                                         | Pros | Cons | Latency | Complexity |
|----------------------------|------------------------------------------------------------------------|------|------|---------|-----------|
| PyO3 / embedded Python     | âŒ Avoid as primary path                                              | Very low call overhead | Complex build, GIL issues, shipping Python/runtime inside Tauri, fragile | Low | High |
| Direct subprocess (CLI)    | âš ï¸ Good for prototypes and single-shot conversions                    | Simple, no custom server | Process startup overhead, limited concurrency & observability | Medium | Low |
| **Docling-Serve (HTTP)**   | âœ… Primary recommendation (Rust â†’ HTTP â†’ Docling-Serve)               | Clear API, async, can run as Tauri sidecar; easy to scale | Extra process to manage, HTTP overhead | Medium | Medium |
| Custom FastAPI worker      | âœ… Alternative: embed Docling in same Python orchestrator as LLMs     | Full control, simple JSON API, easy to integrate with orchestrator | Need to design protocol & job queue | Medium | Medium |
| Unix socket / gRPC         | âš ï¸ Overkill initially                                                 | Lower overhead than HTTP | More plumbing/maintenance, no off-the-shelf server | Low | High |

**Recommendation:**

- **Short term**: run a **Python worker** (FastAPI or simple job runner) inside your existing Python orchestrator process, with a minimal REST or local queue API.
- **Medium term**: if usage grows or you want more isolation, adopt **Docling-Serve** as a dedicated sidecar service, managed by Handshake (start/stop, health checks).  

This keeps Rust code clean and places Docling alongside your LLM runtime.

###### Mapping to Tiptap / ProseMirror Blocks

Mapping from `DoclingDocument` â†’ Tiptap nodes:

| Docling Element             | Tiptap Block / Node              | Mapping Complexity |
|-----------------------------|----------------------------------|--------------------|
| Paragraph                   | `paragraph`                      | Low |
| Heading(level)             | `heading` with `level` attribute | Low |
| List (unordered / ordered) | `bulletList` / `orderedList` + `listItem` | Low |
| Table                       | `table` â†’ `tableRow` â†’ `tableCell` | Medium (spans, headers) |
| Figure (image + caption)   | `image` node + custom `figcaption` or `caption` attribute | Medium |
| Code block                 | `codeBlock` (with language attr) | Low |
| Blockquote                 | `blockquote`                     | Low |
| Formula/Equation           | Custom `math_block` node         | Medium |
| Footnote                   | Custom `footnote` inline/block   | Medium |

Implementation plan:

- Define a **Docling â†’ Tiptap transformer** in TypeScript, using a JSON version of DoclingDocument.  
- Use bounding boxes + page references only as **metadata** on blocks (for canvases, citations), not in the visible ProseMirror schema.  
- For very large documents, stream page-by-page and insert blocks in batches.

This mapping is straightforward and well within Tiptapâ€™s extension model.

###### CRDT Integration (Yjs)

Target flow:

```text
[Source file] â†’ [Docling pipeline] â†’ [DoclingDocument]
    â†’ [Block Transformer] â†’ [Yjs ops with AI site ID]
```

Key decisions:

- **AI as CRDT participant**: use a dedicated `siteId` in Yjs for imports performed by Docling (â€œdocling-importerâ€).  
- **Atomic import**:
  - Prefer **page-or section-level transactions** (e.g., one Yjs transaction per page) to keep history chunks manageable and allow progressive UX.  
- **Conflicts**:
  - If importing into an empty doc, simply append blocks.  
  - If re-importing an updated version, you can:
    - Either import into a **new doc** and let the user diff,
    - Or run a block-level diff (ID/anchor-based) and apply Yjs updates selectively.

Provenance:

- Attach metadata per block:
  - `sourceDocumentId`, `page`, `bbox`, `conversionVersion`, `pipelineConfigHash`.
- This supports re-processing strategies and flight recorder integration.

###### Shadow Workspace Integration

For Handshakeâ€™s Shadow Workspace (incremental indexing + embeddings):

- Use Doclingâ€™s **chunking/serialization** utilities to emit stable chunk IDs (e.g., `docId:sectionId:chunkIndex`).  
- Store:
  - Chunk text.
  - Corresponding block IDs / page + bbox.
  - Hash of source content + Docling version + pipeline options.

Dirty detection:

- If the source file changes:
  - Re-run Docling and compare **per-chunk hashes**; only update changed chunks in vector store and knowledge graph.
- For huge PDFs:
  - Use **page-streaming** conversion and store page-level fingerprints to skip unchanged pages.

###### Knowledge Graph Population

Docling by itself does **not** do full NER/relationship extraction, but:

- It provides rich **structure** (sections, headings, tables, figures, citations).  
- There is an â€œInformation extractionâ€ section in docs, oriented around integrating extraction pipelines on top of DoclingDocument.  

Recommended approach:

- Treat Docling as the **structure+text provider**.
- Run Handshakeâ€™s own entity extraction / relation extraction LLMs over:
  - Sections (for hierarchical nodes).
  - Tables (for schema + records).
  - Citations (for edges to external docs).
- Map:

```text
DoclingDocument
â”œâ”€ Metadata      â†’ Document node
â”œâ”€ Sections      â†’ Section nodes (+ parent-child edges)
â”œâ”€ Paragraphs    â†’ Text nodes attached to sections
â”œâ”€ Tables        â†’ Table nodes + row/column/cell nodes
â”œâ”€ Figures       â†’ Figure nodes
â”œâ”€ Citations     â†’ Citation edges
â””â”€ Entities(*)   â†’ Entity nodes (from NER)
```

Doclingâ€™s confidence scores and provenance fields should be stored as node/edge attributes for later auditing.

###### Raw / Derived / Display Separation

Mapping into Handshakeâ€™s 3-way model:

| Docling Output                       | Handshake Category | Notes |
|-------------------------------------|--------------------|-------|
| Original file (PDF, DOCX, etc.)     | RawContent         | Immutable canonical import. |
| Direct text extraction (native PDFs)| RawContent or DerivedContent | For â€œdigital originalsâ€ you might treat as Raw; but you can still regenerate if you trust Docling to be deterministic. |
| OCRâ€™d text                          | DerivedContent     | Must be marked with `ocr=true`, `engine`, `language`, `confidence`. |
| Layout analysis (DocLayNet)         | DerivedContent     | Stored as JSON sidecar; safe to recompute. |
| Table structure (TableFormer)       | DerivedContent     | Sidecar or block metadata. |
| Chunk IDs, embeddings               | DerivedContent     | Vector store only; always recomputable. |
| User-editable imported doc in editor| DisplayContent (+ RawContent) | Once user edits imported blocks, they become user RawContent going forward. |

Reprocessing strategy:

- **Version** every Docling run: `doclingVersion`, `pipeline`, `modelVersions`.
- When upgrading Docling or models:
  - Option 1: Reprocess only when user requests â€œRe-ingest with new parser.â€
  - Option 2: Lazy reprocess when doc is opened, storing new DerivedContent while leaving old RawContent intact.

###### Flight Recorder Integration

Docling itself doesnâ€™t ship a full observability stack, but it exposes enough hooks:

- `ConversionStatus`, error messages, and warning lists.  
- Per-element **confidence scores** for layout, OCR, tables, etc.  

For Handshakeâ€™s DuckDB-based flight recorder:

- Log per document:
  - `docId`, `sourcePath`, size, pages.
  - `startTime`, `endTime`, `duration`.
  - Pipeline options (OCR engine, VLM vs standard).
  - `doclingVersion`, `modelVersions`.
  - `status`, error/warning counts.
  - Aggregate confidence histograms (min/mean/max).
- Log per page (optional):
  - Processing time.
  - Count of tables, figures.
  - OCR percentage (area / tokens).

This supports:

- Performance tuning (detect slow docs).
- Quality monitoring (low confidence threshold alerts).
- Replayability of conversions.

###### Resource Management

Doclingâ€™s performance characteristics:

- CPU-only PDF pipeline is already optimized; GPU acceleration accelerates layout and table models.  
- VLM pipelines with Granite-Docling / SmolDocling can be significantly slower and more GPU-hungry, especially on larger models (Pixtral-12B, Gemma-12B).  

For Handshakeâ€™s hardware (Ryzen 9, 128 GB RAM, 24 GB VRAM):

- Run **standard Docling pipeline on CPU by default**; itâ€™s fast enough for most docs and avoids contention with local LLMs.
- Enable **GPU acceleration selectively**:
  - For large batch ingestions or long PDFs.
  - During â€œmaintenance windowsâ€ when LLMs are idle.
- For VLM pipeline:
  - Treat as **opt-in** (user toggles â€œhigh-fidelity ingestâ€).
  - Limit concurrency to 1â€“2 documents at a time.
- Integrate Docling into your **task scheduler**, giving it a lower priority than interactive LLM sessions.

###### Error Handling & Recovery

Typical failure scenarios and suggested behaviors:

| Scenario                     | Expected Behavior                          | Recovery Strategy |
|------------------------------|-------------------------------------------|-------------------|
| Corrupted PDF                | Docling returns error status              | Mark import as failed, log error; offer â€œdownload corrupted fileâ€ for debugging. |
| Password-protected PDF       | Docling fails to open                     | Detect error string, prompt user for password; retry. |
| 1000+ page document          | Long processing time                      | Use page-streaming; show progress UI; allow cancellation and resumption. |
| Unsupported format           | 415/unsupported                           | Route to Unstructured/Tika fallback; log. |
| OCR failure (low confidence) | Sparse or noisy text, low confidence      | Flag quality issue; allow user to re-run with different OCR engine or VLM pipeline. |
| OOM (GPU/CPU)                | Crash or error from pipeline              | Retry with reduced options (no VLM, CPU only, smaller batch); throttle concurrency. |

###### Connector Integration

Recommended placement of Docling in Handshakeâ€™s connector pipeline:

```text
Connectors: JMAP / CalDAV / MCP / File watcher / etc
      â”‚
      â–¼
[Format detection + routing]
      â”‚
      â”œâ”€ Docling (primary for PDF/Office/HTML/EPUB/images/audio)
      â”‚
      â”œâ”€ Unstructured (email formats, fringe types)
      â”‚
      â””â”€ Tika / others (rare legacy formats)
      â–¼
[Block Transformer â†’ Yjs/CRDT]
      â”‚
      â”œâ”€ Shadow Workspace (chunks + embeddings)
      â””â”€ Knowledge Graph (structured nodes/edges)
```

Docling can be:

- Invoked directly from file watcher for local files.  
- Used as a **tool** in MCP for agentic workflows (agents request conversions through Docling MCP server).  

---

##### 6.1.2.3.11 Integration Point Summary Table

| Integration Point     | Recommendation                                               | Complexity | Risk |
|-----------------------|-------------------------------------------------------------|-----------|------|
| Rust â†” Python bridge  | Docling-Serve or custom Python worker (HTTP/queue)         | Medium    | Low  |
| Tiptap block mapping  | Dedicated Doclingâ†’Tiptap transformer using DoclingDocument | Medium    | Low  |
| CRDT integration      | Yjs ops per page/section with AI site ID                   | Medium    | Medium (large docs/history) |
| Shadow Workspace      | Use Docling chunking + stable chunk IDs + hashes           | Medium    | Low  |
| Knowledge Graph       | Run NER/RE over Docling structure; store provenance        | Medium    | Medium |
| Raw/Derived separation| Treat original files as Raw, Docling outputs as Derived    | Low       | Low  |
| Flight recorder       | Log per-doc/page metrics + confidence + versions           | Medium    | Low  |
| Resource management   | CPU-default; GPU/VLM on demand; scheduler-integrated       | Medium    | Medium |

---

#### 6.1.2.4 Competitor Recommendation

##### 6.1.2.4.1 Short Deep Dives

###### Unstructured.io

- **License**: Apache-2.0.  
- **Strengths**: Very broad format support (including EML/MSG/MBOX, RTF, ODT, many more), rich connectors, built-in chunking and RAG-ready element outputs.  
- **Weaknesses**: Slower and less layout-aware on PDFs than Docling; table extraction is good but not as specialized as TableFormer.  
- **Handshake role**: **Fallback / complement** to Docling, especially for email and fringe formats.

###### Marker

- **License**: GPL-3.0 (copyleft).  
- **Strengths**: Extremely good PDFâ†’Markdown for books and scientific papers; handles equations, figures, tables; highly praised in community.  
- **Weaknesses**: PDF-only; GPL-3.0 is problematic for closed-source Handshake core; focused on text/markdown, not full structured document graph.  
- **Handshake role**: Avoid in core product due to GPL; possible **external tool** or user-managed plugin if ever needed, but not recommended.

###### LlamaParse

- **License / deployment**: Proprietary, cloud-hosted (LlamaCloud).  
- **Strengths**: LLM-driven parsing with excellent handling of complex documents, charts, images, handwriting; strong RAG story with LlamaIndex.  
- **Weaknesses**: Cloud-only, per-page pricing; conflicts with Handshakeâ€™s local-first and privacy goals.  
- **Handshake role**: Maybe a **toggleable cloud fallback** for â€œparsing rescue modeâ€, but not primary.

###### PyMuPDF (fitz)

- **License**: AGPL-3.0 or commercial.  
- **Strengths**: Very fast, low-level PDF manipulation; precise coordinate access; good for building custom pipelines.  
- **Weaknesses**: License problematic unless you buy commercial license; no high-level layout and table understanding like Docling/TableFormer.  
- **Handshake role**: Only if you deliberately buy a commercial license and need low-level PDF operations; otherwise, avoid.

###### Apache Tika

- **License**: Apache-2.0.  
- **Strengths**: Massive format coverage (1000+ formats), robust metadata extraction, Java ecosystem, server mode.  
- **Weaknesses**: Basic text extraction; limited layout understanding; more heavy-weight (JVM).  
- **Handshake role**: Optional **format detection + rare format fallback**.

##### pdfplumber

- **License**: MIT.  
- **Strengths**: Reliable for programmatic PDFs and simple tables; precise coordinates.  
- **Weaknesses**: No OCR, no deep layout model; not as robust on messy tables as TableFormer.  
- **Handshake role**: Optional niche tool for known structured forms/invoices.

###### Camelot / Tabula

- **License**: MIT (Camelot, Tabula core is Apache-style).  
- **Strengths**: Specialized for tables; very good for border-heavy or regular tables.  
- **Weaknesses**: Tables-only, no broader document understanding, no OCR.  
- **Handshake role**: Unnecessary given Docling/TableFormer, unless you hit a very specific table class where they outperform.

##### 6.1.2.4.2 Tool-per-Format Recommendation Matrix

| Format Category | Primary Tool | Fallback Tool(s) | Rationale |
|-----------------|-------------|------------------|-----------|
| PDF (digital + scanned) | **Docling** | Unstructured; optionally LlamaParse cloud for â€œrescue modeâ€ | Docling provides best mix of speed, layout, tables, OCR, licensing. |
| Office (DOCX/PPTX/XLSX) | **Docling** | Unstructured, Tika | Docling supports them natively and outputs structured blocks; Unstructured/Tika can catch edge cases. |
| HTML/Markdown/EPUB | **Docling** | Unstructured | Docling gives DoclingDocument tree and chunking; Unstructured is fallback for odd web formats. |
| Images (scans) | **Docling + OCR engines** | Direct OCR pipeline (e.g., Tesseract via other libs) | Docling unifies OCR with layout and table detection. |
| Audio | **Docling ASR** | Native Whisper integration | Docling wraps Whisper and emits DoclingDocument; you may still prefer controlling Whisper yourself for advanced features. |
| Email (EML/MSG/MBOX) | **Unstructured** | Tika | Docling doesnâ€™t support email formats; Unstructured and Tika do. |
| Legacy Office / RTF / ODT | **Unstructured or Tika** | LibreOffice + Docling (convertâ†’DOCX/PDF) | Better delegated to tools focusing on legacy formats. |
| Arbitrary binary/rare formats | **Tika** | Unstructured | Tikaâ€™s MIME detection + broad coverage. |

##### 6.1.2.4.3 Hybrid Strategy (Recommended)

Docling as **primary engine**, Unstructured + Tika as fallback, Marker + LlamaParse + PyMuPDF explicitly avoided or limited to user-managed plugins for licensing/local-first reasons. This matches the hybrid architecture already suggested by both Docling community examples and third-party integrations.  

---

#### 6.1.2.5 Architecture Diagram (Text)

```text
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            Handshake UI              â”‚
                          â”‚  (Tauri + Rust + React + Tiptap)    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                          CRDT (Yjs)       â”‚   Commands (import file, show status)
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚         Rust Coordinator / Orchestrator    â”‚
                     â”‚  - Starts & monitors sidecars              â”‚
                     â”‚  - Schedules ingestion jobs                â”‚
                     â”‚  - Talks to Python orchestrator / Docling  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚               â”‚
                         HTTP / queueâ”‚               â”‚gRPC/HTTP (models)
                                     â”‚               â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Python Orchestrator +   â”‚       â”‚   Local LLM Runtime(s) â”‚
             â”‚   Docling Worker(s)       â”‚       â”‚ (Ollama / vLLM / etc.) â”‚
             â”‚ - Docling API/CLI         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ - Unstructured / Tika     â”‚
             â”‚ - ASR, OCR, VLM pipelines â”‚
             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚             â”‚
         DoclingDocument JSON    â”‚
                   â”‚             â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”‚
       â”‚ Block Transformerâ”‚      â”‚
       â”‚ (Doclingâ†’Tiptap) â”‚      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
               â”‚ Yjs ops        â”‚ Derived artifacts (chunks, KG nodes)
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â–¼
      â”‚   CRDT Store    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ (Yjs docs)      â”‚   â”‚ Shadow WS   â”‚   â”‚ Knowledge KGâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ (chunks +  â”‚   â”‚ (Cozo/Kuzu) â”‚
               â”‚            â”‚ embeddings)â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚                  â”‚
               â”‚                  â–¼
               â”‚            DuckDB Flight
               â”‚            Recorder (logs)
               â–¼
       SQLite + asset store
       (Raw/Derived separation)
```

---

#### 6.1.2.6 Risk Assessment

| Risk Category | Level | Details & Mitigation |
|---------------|-------|----------------------|
| Technical     | Medium | Docling is evolving quickly; APIs (e.g., Docling-Serve) may change, and some advanced features (charts, equations) are still maturing. Mitigation: pin versions, wrap Docling behind an internal adapter layer, and rely on tested pipelines only in the first release. |
| Licensing     | Lowâ€“Medium | Core Docling stack is MIT and safe. Risk comes from optional dependencies (PyMuPDF) and models with their own terms. Mitigation: avoid AGPL dependencies; track and whitelist models with permissive licenses. |
| Maintenance   | Low | Docling is actively developed with IBM + community backing, integrated into LF AI & Data, with frequent releases and growing ecosystem integrations (LangChain, LlamaIndex, RHEL AI, Quarkus, Apify). |
| Performance   | Medium | For very large docs and VLM pipelines, CPU/GPU usage can be significant. Mitigation: default to CPU pipelines, enable GPU selectively, schedule heavy jobs, monitor via flight recorder. |
| Integration   | Medium | Requires cross-language orchestration (Rust â†” Python), mapping to Tiptap/Yjs, and multi-tool fallback routing. Mitigation: start with a minimal Docling-only path, build small, well-tested adapters, add Unstructured/Tika later as needed. |

---

#### 6.1.2.7 Proof-of-Concept Plan (4 Weeks)

##### 6.1.2.7.1 Week 1 â€“ Minimal Docling Integration

1. **Set up environment**
   - Add Docling (CPU-only) to Python orchestrator environment.
   - Implement a simple FastAPI endpoint: `/convert` â†’ returns DoclingDocument JSON for PDF/DOCX/HTML.  
2. **Rust bridge prototype**
   - From Rust coordinator, call `/convert` for a single local PDF.
   - Store DoclingDocument JSON as a sidecar in SQLite/DuckDB.  
3. **Block transformer v0**
   - Implement minimal Doclingâ†’Tiptap mapping for headings + paragraphs + lists.
   - Insert into a fresh Yjs document as a one-shot transaction.

Success criteria:

- You can drop a PDF into Handshake, see a block-based doc with correct headings/paragraphs.
- Ingestion is logged in DuckDB with basic timing + status.

##### 6.1.2.7.2 Week 2 â€“ Tables, Layout, and Shadow Workspace

1. **Table & image support**
   - Extend mapping to Tiptap for tables and images.
   - Verify merged cells and multi-row headers on a set of complex PDFs.  
2. **Shadow Workspace integration**
   - Use Doclingâ€™s chunking utilities to generate chunks with IDs and metadata.
   - Store chunks and embeddings in your vector store (sqlite-vec / LanceDB).
3. **Flight recorder v1**
   - Log per-document metrics: pages, duration, pipeline options, status.

Success criteria:

- Tables render reasonably; chunking pipeline produces stable IDs.
- You can run simple RAG queries against ingested docs.

##### 6.1.2.7.3 Week 3 â€“ OCR, ASR, and Fallbacks

1. **OCR pipeline**
   - Enable one OCR backend (e.g. Tesseract) and add â€œforce OCRâ€ toggle.
   - Mark OCR-derived text as DerivedContent with confidence metadata.  
2. **ASR prototype**
   - Implement the ASR pipeline for audio (`InputFormat.AUDIO` with Whisper) and map transcripts into block docs (one paragraph per segment or per sentence).  
3. **Introduce Unstructured fallback**
   - For email formats and unsupported types, call Unstructured and wrap its output into a minimal â€œpseudo-DoclingDocumentâ€ structure for consistency.  

Success criteria:

- Scanned PDFs import with usable text and flagged OCR metadata.
- Audio files import as timestamped transcripts into Handshake.
- Email attachments and .eml messages are ingested using Unstructured as a fallback.

##### 6.1.2.7.4 Week 4 â€“ VLM, MCP, and Hardening

1. **VLM pipeline (optional)**
   - Add a â€œhigh fidelityâ€ option using Granite-Docling or SmolDocling for selected pages or documents; measure GPU load and speed.  
2. **MCP integration**
   - Run Docling MCP server and integrate it into your internal agent framework so your own agents can call â€œconvert_documentâ€ and related tools.  
3. **Error handling & UX**
   - Implement clear user messaging for failures (corrupted PDF, password required, low OCR quality).
   - Add a re-ingest UI control that stores a new DerivedContent version while preserving RawContent.  

Success criteria:

- A small end-to-end â€œreference flowâ€:
  - Import a messy PDF, audio file, and email thread.
  - View them in Handshake as structured docs.
  - Run RAG against them.
  - Inspect logs in DuckDB and confirm metrics.

---

**Bottom line:**  
Use **Docling as the primary local document engine** for Handshake, wrapped behind a Python worker or Docling-Serve sidecar, with Unstructured and Tika as targeted fallbacks. This combination respects your local-first constraints, keeps licensing clean, and provides a strong technical base for CRDT-aware, AI-enhanced document workflows.

---

### 6.1.3 Part II â€“ Docling Integration Assessment for Project Handshake (Spec-style)

### 6.1.4 Docling Integration Assessment for Project Handshake

**IBM's open-source document processing library is an excellent fit for Handshake's local-first architecture.** Docling provides MIT-licensed AI-powered document conversion with state-of-the-art layout analysis and table extraction, runs efficiently on commodity hardware, and offers production-ready HTTP API integration via docling-serve. Key risks include memory management for batch processing and the need to bundle ~500MB of Python dependencies as a Tauri sidecar. The license stack is cleanâ€”all models use permissive licenses (MIT, Apache 2.0, CDLA-Permissive-2.0), enabling full commercial use without restrictions.

---

#### 6.1.4.1 Complete media type support matrix

Docling's format support is strong for documents and images but has notable gaps in email, legacy Office, and e-book formats that Handshake may need to address through complementary tools.

##### 6.1.4.1.1 Document formats

| Format | Support | Notes |
|--------|---------|-------|
| PDF (native digital) | âœ… Full | Primary focus; text + layout analysis |
| PDF (scanned/OCR) | âœ… Full | Automatic detection; EasyOCR/Tesseract |
| DOCX | âœ… Full | Via python-docx; hierarchy preserved |
| PPTX | âœ… Full | Slides as pages with layout |
| XLSX | âœ… Full | Via openpyxl |
| Legacy .doc/.ppt/.xls | âŒ None | Office 97-2003 not supported |
| OpenDocument (.odt/.odp/.ods) | âŒ None | Not implemented |
| RTF | âŒ None | Not documented |

##### 6.1.4.1.2 Markup and text formats

| Format | Support | Notes |
|--------|---------|-------|
| HTML/XHTML | âœ… Full | Via BeautifulSoup |
| Markdown | âœ… Full | Via Marko library |
| AsciiDoc | âœ… Full | Recent addition |
| reStructuredText | âŒ None | Not implemented |
| LaTeX (input) | âŒ None | Formula *output* to LaTeX supported |
| Plain text | âš ï¸ Partial | Wrapped as simple document |
| CSV | âœ… Full | Tabular data with structure |

##### 6.1.4.1.3 Image formats (OCR/Vision)

| Format | Support | Notes |
|--------|---------|-------|
| PNG | âœ… Full | Primary image format |
| JPEG/JPG | âœ… Full | Standard support |
| TIFF | âœ… Full | Multi-page supported |
| BMP, WEBP | âœ… Full | Modern format support |
| GIF | âš ï¸ Partial | Static only; animations ignored |
| HEIC | âŒ None | Apple format not supported |
| SVG | âŒ None | Vector format not supported |

##### 6.1.4.1.4 Audio formats (ASR)

| Format | Support | Notes |
|--------|---------|-------|
| WAV, MP3 | âœ… Full | Primary audio formats |
| FLAC, OGG, M4A, WebM | âš ï¸ Via ffmpeg | Requires ffmpeg on PATH |

**ASR models available**: Whisper tiny/base/small/medium/large/turbo with **90+ languages**. MLX acceleration for Apple Silicon. Timestamps preserved as `[time: start-end]` format. Speaker diarization not built-in.

##### 6.1.4.1.5 Video and subtitles

| Format | Support | Notes |
|--------|---------|-------|
| Video frame extraction | âŒ None | Not implemented |
| Audio extraction from video | âŒ None | Use external tools |
| WebVTT | âœ… Full | Recently added |
| SRT, ASS/SSA | âŒ None | Not supported |

##### 6.1.4.1.6 Specialized and data formats

| Format | Support | Notes |
|--------|---------|-------|
| USPTO XML (patents) | âœ… Full | Schema-specific parser |
| JATS XML (academic) | âœ… Full | Journal articles |
| EPUB, MOBI, DjVu | âŒ None | E-book formats not supported |
| JSON, XML, YAML (input) | âŒ None | Output formats only |
| EML, MSG, MBOX (email) | âŒ None | **Gap for Handshake** |

---

#### 6.1.4.2 Licensing analysis confirms commercial viability

The entire Docling stack uses permissive licenses compatible with proprietary software distribution.

| Component | License | Commercial Use |
|-----------|---------|----------------|
| **Docling codebase** | MIT | âœ… Unrestricted |
| **docling-core/parse** | MIT | âœ… Unrestricted |
| **DocLayNet layout model** | CDLA-Permissive-2.0 | âœ… Yes |
| **TableFormer** | CDLA-Permissive-2.0 | âœ… Yes |
| **Granite-Docling-258M** | Apache 2.0 | âœ… Yes |
| **SmolDocling-256M** | CDLA-Permissive-2.0 | âœ… Yes |
| **Heron layout models** | Apache 2.0 | âœ… Yes |

**No enterprise-only features exist**â€”all capabilities are open source. Attribution is requested but not legally required under MIT. The project is governed by the **LF AI & Data Foundation**, ensuring vendor-neutral stewardship.

---

#### 6.1.4.3 Technical architecture enables flexible integration

##### 6.1.4.3.1 Component stack

Docling is organized into four modular packages with clear separation of concerns:

- **docling-core**: Pydantic data models, chunkers, serializers (MIT)
- **docling-parse**: PDF backend using qpdf C++ library via pybind11 (MIT)
- **docling-ibm-models**: AI modelsâ€”RT-DETR layout detector, TableFormer (MIT wrapper, CDLA models)
- **docling-serve**: FastAPI HTTP server with async support, Redis queue backend (MIT)

##### 6.1.4.3.2 AI model performance characteristics

| Model | Architecture | Accuracy | Inference Time (CPU) |
|-------|--------------|----------|---------------------|
| **Layout (Heron-101)** | RT-DETR object detector | 78% mAP | 633ms/page (x86), 271ms (M3) |
| **TableFormer** | Vision Transformer | 97.9% TEDS (complex tables) | 1.7s/table (x86), 704ms (M3) |
| **Granite-Docling-258M** | Idefics3 VLM | 97% TEDS tables, 98.8% F1 code | ~2s/page (GPU) |

TableFormer achieves **93.6% TEDS** on table structure versus Tabula's 67.9% and Camelot's 73.0%, making it the clear choice for complex document tables.

##### 6.1.4.3.3 Resource requirements for Handshake deployment

| Configuration | Memory | Speed | Recommendation |
|---------------|--------|-------|----------------|
| **CPU-only (standard)** | 6GB peak | 0.6â€“1.3 pages/sec | Development, low-volume |
| **CPU-only (pypdfium)** | 2.5GB peak | 0.9â€“2.4 pages/sec | Production batch |
| **Apple Silicon (MLX)** | Shared memory | 1.3â€“2.4 pages/sec | Mac deployment |
| **CUDA GPU** | +1â€“2GB VRAM | 0.49s/page (L4) | High-volume processing |

---

#### 6.1.4.4 Handshake integration assessment

##### 6.1.4.4.1 Recommended architecture: docling-serve as Tauri sidecar

The optimal integration path uses docling-serve's HTTP API bundled as a PyInstaller sidecar with Tauri's shell API for process management.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Handshake (Tauri 2.x)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   React +   â”‚   â”‚     Rust      â”‚   â”‚   docling-serve      â”‚    â”‚
â”‚  â”‚  Tiptap/    â”‚â—„â”€â”€â”¤  Coordinator  â”œâ”€â”€â”€â”¤   (PyInstaller       â”‚    â”‚
â”‚  â”‚  BlockNote  â”‚   â”‚               â”‚   â”‚    sidecar)          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                  â”‚                      â”‚                 â”‚
â”‚     ProseMirror       HTTP Client           localhost:5001          â”‚
â”‚        JSON            (reqwest)              REST API              â”‚
â”‚         â”‚                  â”‚                      â”‚                 â”‚
â”‚         â–¼                  â”‚                      â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                      â”‚                 â”‚
â”‚  â”‚   Yjs       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚                 â”‚
â”‚  â”‚   CRDT      â”‚    DoclingDocument              â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â†’ Tiptap                    â”‚                 â”‚
â”‚         â”‚                                        â”‚                 â”‚
â”‚         â–¼                  â–¼                     â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              SQLite + sqlite-vec + CRDT sidecars            â”‚   â”‚
â”‚  â”‚  RawContent â†’ DerivedContent (DoclingDocument) â†’ Display    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 6.1.4.4.2 Integration points with complexity ratings

| Integration | Complexity | Risk | Implementation Notes |
|-------------|------------|------|---------------------|
| **docling-serve HTTP API** | Low | Low | Production-ready; sync/async endpoints |
| **Tauri sidecar bundling** | Medium | Medium | PyInstaller ~500MB; platform-specific builds |
| **DoclingDocument â†’ Tiptap** | Medium | Low | Tree traversal; handle merged table cells |
| **Yjs CRDT integration** | Medium | Low | Atomic import via transaction; provenance in metadata |
| **HybridChunker for Shadow Workspace** | Low | Low | Built-in tokenization-aware chunking |
| **Knowledge graph population** | High | Medium | NER not built-in; requires spaCy post-processing |
| **Raw/Derived separation** | Medium | Low | OCR text is Derived; track docling_version in schema |

##### 6.1.4.4.3 DoclingDocument to ProseMirror mapping

The DoclingDocument schema maps cleanly to Tiptap/ProseMirror blocks:

| DoclingDocument Type | ProseMirror Node | Special Handling |
|---------------------|------------------|------------------|
| TextItem (paragraph) | `paragraph` | Direct mapping |
| TextItem (heading) | `heading` with level attr | Level 1â€“6 support |
| TextItem (equation) | Custom `mathBlock` | LaTeX content |
| TableItem | `table` â†’ `tableRow` â†’ `tableCell` | Handle colspan/rowspan |
| PictureItem | `image` | Store bbox in data attr |
| ListGroup | `bulletList` / `orderedList` | Recursive nesting |
| CodeItem | `codeBlock` | Language detection included |

**Provenance preservation**: Store `data-docling-ref` (JSON pointer like `#/texts/5`) and `data-page` attributes on nodes for citation tracking.

##### 6.1.4.4.4 CRDT integration pattern

```typescript
// Atomic import with provenance tracking
function importDoclingDocument(docling: DoclingDocument, ydoc: Y.Doc) {
  const prosemirrorJson = convertDoclingToTiptap(docling);
  
  ydoc.transact(() => {
    const fragment = ydoc.get('prosemirror', Y.XmlFragment);
    fragment.delete(0, fragment.length);
    prosemirrorJsonToYXmlFragment(prosemirrorJson, fragment);
  }, 'docling-import');
  
  // Store import metadata
  ydoc.getMap('import-metadata').set('docling', {
    timestamp: Date.now(),
    docling_version: '2.63.0',
    source_hash: computeHash(docling.origin),
    model_versions: { layout: 'heron-101', table: 'tableformer-v2' }
  });
}
```

##### 6.1.4.4.5 Raw/Derived/Display classification

| Content Type | Classification | Rationale |
|--------------|----------------|-----------|
| Original PDF text | **Raw** | Canonical extraction |
| OCR text from images | **Derived** | Regenerable, model-dependent |
| Table structure | **Derived** | Layout analysis dependent |
| Bounding boxes | **Raw** | Fixed from source |
| Chunk embeddings | **Derived** | Model-specific |
| User annotations | **Raw** | User-generated |
| ProseMirror JSON | **Display** | Filtered presentation |

---

#### 6.1.4.5 Competitor recommendation matrix

##### 6.1.4.5.1 License compatibility summary

| Tool | License | Compatible with MIT App? | Best Use Case |
|------|---------|--------------------------|---------------|
| **Docling** | MIT | âœ… Yes | Primary document processing |
| **pdfplumber** | MIT | âœ… Yes | Coordinate extraction, debugging |
| **Camelot-py** | MIT | âœ… Yes | Simple lattice table fallback |
| **Apache Tika** | Apache-2.0 | âœ… Yes | Email formats, format breadth |
| **Unstructured OSS** | Apache-2.0 | âœ… Yes | Data connectors (S3, SharePoint) |
| **Marker** | GPL-3.0 | âŒ No | Research only; copyleft |
| **PyMuPDF** | AGPL-3.0 | âŒ No | Requires commercial license |
| **LlamaParse** | Proprietary | âš ï¸ API only | Cloud prototyping |

##### 6.1.4.5.2 Performance comparison

| Tool | CPU Speed | GPU Speed | Table Accuracy |
|------|-----------|-----------|----------------|
| **Docling** | 0.8â€“3.1 sec/page | 0.49 sec/page | 97.9% TEDS |
| Marker | 16 sec/page | 0.5 sec/page | High (with LLM) |
| Unstructured | 4.2 sec/page | N/A | 75% (complex) |
| pdfplumber | Fast | N/A | Rule-based only |

##### 6.1.4.5.3 Recommended tool combinations for Handshake

**Primary stack (MIT/Apache-2.0 only):**
- **Docling**: PDF, DOCX, PPTX, XLSX, HTML, images, audio
- **Apache Tika** (JVM container): Email (EML, MSG), legacy formats, format detection
- **pdfplumber**: Coordinate debugging, simple table fallback

**Format gap coverage:**

| Missing Format | Recommended Solution |
|----------------|---------------------|
| Email (EML, MSG, MBOX) | Apache Tika or python email/mailbox stdlib |
| Legacy Office (.doc, .xls) | Apache Tika (JVM) or LibreOffice conversion |
| EPUB/MOBI | Calibre CLI or ebooklib |
| OpenDocument | odfpy or Apache Tika |

---

#### 6.1.4.6 Risk assessment

##### 6.1.4.6.1 Technical risks

| Risk | Severity | Likelihood | Mitigation |
|------|----------|------------|------------|
| **Memory leaks in batch processing** | High | High | Use Tesseract instead of EasyOCR; pypdfium backend; container restarts |
| **~500MB sidecar bundle size** | Medium | Certain | CPU-only PyTorch reduces by 200MB; lazy model loading |
| **Cross-platform PyInstaller builds** | Medium | Medium | CI matrix for each platform; test on clean VMs |
| **GPU contention with Ollama/vLLM** | Medium | Medium | CPU-only Docling mode when LLM loaded; sequential processing |

##### 6.1.4.6.2 Licensing risks

| Risk | Severity | Likelihood | Mitigation |
|------|----------|------------|------------|
| Model license changes | Low | Low | Pin model versions; monitor releases |
| Dependency license contamination | Medium | Low | Audit with pip-licenses; avoid GPL deps |

##### 6.1.4.6.3 Maintenance risks

| Risk | Severity | Likelihood | Mitigation |
|------|----------|------------|------------|
| IBM reduces investment | Low | Low | LF AI governance; RHEL AI commitment |
| Breaking API changes | Medium | Medium | Pin docling version; migration scripts |
| Python version requirements | Low | Low | Supports Python 3.9â€“3.14 |

##### 6.1.4.6.4 Known critical issues (as of December 2025)

- **Memory leak in DoclingParseV2DocumentBackend** (#2209): 10GB+ RAM accumulation on long documents
- **EasyOCR memory leak** (#1343): Container OOM in batch mode
- **Workaround**: Use Tesseract OCR, pypdfium backend, periodic restarts

---

#### 6.1.4.7 Four-week proof-of-concept plan

##### 6.1.4.7.1 Week 1: Core integration validation

| Day | Task | Success Criteria |
|-----|------|------------------|
| 1â€“2 | Set up docling-serve locally; test `/v1/convert/file` endpoint | Convert 10 PDFs via HTTP API |
| 3 | Implement Rust HTTP client with reqwest | Type-safe DoclingDocument deserialization |
| 4â€“5 | Build DoclingDocument â†’ Tiptap JSON converter | Paragraph, heading, list, table, image mapping |

**Deliverable**: Rust library that converts uploaded PDF to ProseMirror JSON

##### 6.1.4.7.2 Week 2: Tauri sidecar integration

| Day | Task | Success Criteria |
|-----|------|------------------|
| 1â€“2 | Create PyInstaller bundle for docling-serve | Single executable runs on macOS/Linux |
| 3 | Configure Tauri sidecar with shell API | Sidecar starts/stops with app |
| 4â€“5 | Implement health check and crash recovery | Auto-restart on failure; graceful shutdown |

**Deliverable**: Tauri app that launches docling-serve sidecar automatically

##### 6.1.4.7.3 Week 3: CRDT and Shadow Workspace

| Day | Task | Success Criteria |
|-----|------|------------------|
| 1â€“2 | Integrate Yjs with atomic document import | Import preserves structure; undo works |
| 3 | Store import provenance in Yjs metadata | Track docling version, source hash |
| 4â€“5 | Implement HybridChunker for embeddings | Chunks with headings context; 512 token limit |

**Deliverable**: Imported documents editable in Tiptap with embedding chunks generated

##### 6.1.4.7.4 Week 4: Production hardening

| Day | Task | Success Criteria |
|-----|------|------------------|
| 1â€“2 | Test memory usage on 100-page documents | Peak RAM < 4GB |
| 3 | Benchmark processing speed | < 3 sec/page on M1 Mac |
| 4 | Error handling for corrupted/password PDFs | Graceful errors; partial results |
| 5 | Cross-platform testing (macOS, Windows, Linux) | All platforms functional |

**Deliverable**: Production-ready integration with documented performance characteristics

##### 6.1.4.7.5 Success metrics for PoC

- [ ] PDF â†’ editable Tiptap document in < 5 seconds for 10-page PDF
- [ ] Table cells with merged spans render correctly
- [ ] Images extracted with bounding boxes for canvas placement
- [ ] Memory usage stays < 4GB for 100-page documents
- [ ] Sidecar bundle size < 600MB
- [ ] Works offline (air-gapped)

---

#### 6.1.4.8 Project health validates long-term adoption

**Viability score: 8.5/10 (Highly Viable)**

| Factor | Score | Notes |
|--------|-------|-------|
| GitHub metrics | 9/10 | 42,200+ stars; weekly releases |
| Documentation | 9/10 | Comprehensive docs, tutorials, examples |
| Governance | 10/10 | LF AI & Data Foundation; IBM Distinguished Engineer chairs board |
| Enterprise backing | 9/10 | RHEL AI 1.3+ supported feature; watsonx.ai integration |
| Known issues | 6/10 | Memory leaks require workarounds |

**Key milestone**: Docling inducted into LF AI & Data Foundation on April 29, 2025, ensuring vendor-neutral governance.

**Red Hat commitment**: RHEL AI 1.3+ includes Docling as a supported feature with enterprise-grade support available through subscription.

---

#### 6.1.4.9 Conclusion: Docling is the right choice for Handshake

Docling provides the optimal balance of **AI-powered accuracy**, **permissive licensing**, and **local-first capability** for Handshake's document processing needs. The MIT license stack enables unrestricted commercial use, the docling-serve HTTP API simplifies Tauri integration, and the 97.9% table extraction accuracy exceeds all open-source alternatives. 

**Primary recommendation**: Proceed with docling-serve sidecar integration using the four-week PoC plan. Monitor memory usage carefully in batch scenarios and implement restart policies.

**Secondary tools**: Integrate Apache Tika (via container) for email format support and legacy Office documents. Use pdfplumber for coordinate debugging during development.

**Risk mitigation priorities**: 
1. Use Tesseract over EasyOCR to avoid memory leaks
2. Pin to pypdfium backend for large documents
3. Implement sidecar health monitoring with auto-restart

---

### 6.1.5 Part III â€“ Architectural Evaluation of IBM Docling for the Handshake Workspace
#### 6.1.5.1 Architectural Evaluation of IBM Docling for the Handshake Workspace
##### 6.1.5.1.1 Executive Summary

The "Handshake" initiative represents a strategic pivot in desktop workspace design, prioritizing local-first data processing, user privacy, and high-performance interaction through the Rust and Tauri ecosystem. A fundamental requirement for this workspace is the ability to ingest, parse, and semantically understand a vast array of unstructured documentsâ€”ranging from academic PDFs and financial spreadsheets to scientific manuscripts and legacy archives. The objective of this report is to provide an exhaustive architectural evaluation of IBM Docling, an open-source document processing library, to determine its viability as the core ingestion engine for Handshake.
The analysis reveals that Docling represents a significant advancement in document processing technology, moving beyond traditional Optical Character Recognition (OCR) and text extraction towards a holistic, layout-aware understanding of document structure.1 Powered by specialized AI models such as DocLayNet for layout segmentation and TableFormer for table structure recognition, Docling is capable of reconstructing the hierarchical organization of documents with a fidelity that standard parsers cannot match.3 This capability is critical for the "AI-enhanced" aspect of Handshake, as it enables Retrieval-Augmented Generation (RAG) systems to retrieve information with semantic contextâ€”distinguishing between a footnote, a header, and a table cellâ€”thereby significantly reducing hallucination rates in downstream Large Language Model (LLM) tasks.
From an integration perspective, Doclingâ€™s Python-centric architecture presents a challenge for the Rust-based Handshake environment. However, this report identifies the Sidecar Pattern as a robust solution, allowing the robust encapsulation of the Python runtime while maintaining the performance and safety guarantees of the Rust frontend.5 Furthermore, Doclingâ€™s MIT License 7 offers a decisive commercial advantage over GPL-licensed competitors like Marker, ensuring that Handshake faces no legal barriers to distribution or monetization.
While Docling excels in processing modern formats and providing deep semantic structure, it exhibits notable gaps in legacy format support (e.g., .doc, .xls, .eml) and requires significant computational resources for optimal performance.8 Consequently, this report recommends a hybrid architecture: utilizing Docling as the primary semantic engine, supplemented by a transcoding layer for legacy compatibility and a background job management system to mitigate latency on consumer hardware. This strategy positions Handshake to deliver a state-of-the-art, privacy-preserving document intelligence experience.

##### 6.1.5.1.2 The Imperative of Layout-Aware Document Understanding

To appreciate the necessity of a tool like Docling within the Handshake ecosystem, one must first confront the limitations of traditional document processing. For decades, the industry standard for parsing PDFs and office documents has focused on the extraction of text streamsâ€”simply pulling character codes from the file in the order they appear in the underlying binary. While computationally inexpensive, this approach destroys the semantic fabric of the document.

###### The Failure of Linear Text Extraction

In a linear extraction paradigm, a multi-column scientific paper is often rendered as a jumbled sequence of sentences where the end of column A flows directly into the start of column B, disregarding the visual boundary. Headers are inextricably mixed with body text, and tables are reduced to unintelligible streams of alphanumeric characters. For a modern AI workspace like Handshake, this loss of structure is catastrophic. An LLM attempting to answer a user's query based on such data lacks the necessary context to determine if a number belongs to the "Revenue" column or the "Year" column of a financial table.

###### The Computer Vision Paradigm

Docling fundamentally diverges from this legacy approach by treating document processing as a computer vision problem first, and a text parsing problem second. This philosophy acknowledges that the "meaning" of a document is encoded not just in its words, but in its visual layoutâ€”the spatial relationships between blocks of text, the distinct formatting of headers, and the grid structure of tables.
The core of Doclingâ€™s architecture is a sophisticated pipeline that employs object detection models to segment the page into semantic regions before any text is processed.2 This ensures that the logical reading order is preserved based on visual cues rather than arbitrary binary stream order. By leveraging models trained on the DocLayNet datasetâ€”a massive corpus of human-annotated documents covering diverse domains like finance, law, and scienceâ€”Docling achieves a level of generalization that allows it to handle the "wild" diversity of user documents expected in a desktop workspace.4

##### 6.1.5.1.3 Architectural Analysis of the Docling Pipeline

The efficacy of Docling lies in its modular pipeline architecture, which orchestrates a series of specialized AI models and heuristics to transform raw pixels and binary data into structured knowledge.

###### The Layout Analysis Engine: DocLayNet

The entry point for Doclingâ€™s semantic understanding is the Layout Model. Utilizing architectures such as RT-DETR (Real-Time Detection Transformer) or proprietary IBM implementations, this model scans the rasterized page image to identify bounding boxes for various document elements.11
The classification taxonomy used by DocLayNet is particularly relevant for Handshakeâ€™s RAG capabilities. It distinguishes between:
Narrative Text: The primary content, which should be indexed for vector search.
Headers and Footers: "Furniture" elements that often introduce noise into search results and should be excluded or tagged as metadata.
Figures and Captions: Visual elements that require distinct processing, such as passing to a Vision-Language Model (VLM) for description.12
Tables: Complex structures that trigger a specialized sub-pipeline.
This granularity allows Handshake to implement sophisticated indexing strategies. For instance, a user could filter search results to show only "Figures containing the word 'Architecture'," a query impossible with simple text extraction.

###### Deep Structure Recognition: TableFormer

Perhaps the most significant technical differentiator for Docling is its integration of TableFormer, a specialized transformer model designed to solve the intractable problem of table extraction.4 Tables in PDFs are notoriously difficult because they lack explicit structural tags; they are merely collections of lines and floating text.
TableFormer approaches this challenge by predicting the logical structure of the table (rows, columns, spanning cells) directly from the visual representation. It effectively reconstructs the HTML-like grid of the table, correcting for irregularities like merged cells or invisible borders that defeat rule-based parsers. Benchmarks indicate that this approach yields a table cell accuracy of 97.9%, significantly outperforming competitive solutions like Unstructured.io or LlamaParse in maintaining structural integrity.14 For Handshake users dealing with financial reports or technical specifications, this capability transforms static, locked data into queryable, machine-readable datasets.

###### The Role of Vision-Language Models (VLMs)

Recent architectural updates have introduced the capability to integrate IBM Granite, a dedicated Vision-Language Model, into the Docling pipeline.12 Unlike the standard pipeline which cascades multiple specialized models (Layout -> Table -> OCR), the Granite-Docling model (258M parameters) attempts an end-to-end transformation, predicting the document structure directly from the image inputs.
While the standard pipeline is likely sufficient for general text documents, the VLM capability offers a future-proof path for handling highly complex or graphical documents. It enables "Visual Grounding," where the model can generate descriptive captions for images and charts, effectively making the visual content of a document searchable via natural language.16 For a local-first application, the modularity of Docling allows Handshake to potentially offer this as an "Advanced Processing" toggle, downloading the heavier VLM weights only for users with capable hardware (e.g., Apple Silicon or NVIDIA GPUs).

##### 6.1.5.1.4 The Unified Data Model: DoclingDocument

The output of the Docling pipeline is not a proprietary binary or a simple text file, but a rich, strongly-typed object model known as the DoclingDocument. Defined using Pydantic, this data structure serves as the intermediate representation (IR) for all processing within the ecosystem.17

###### Hierarchical vs. Flat Representation

The DoclingDocument is designed to satisfy two distinct access patterns:
The Hierarchical Tree: The body field contains a nested tree structure representing the document's logical organization (Sections containing Subsections containing Paragraphs). This is essential for rendering the document in the Handshake UI, preserving the "Table of Contents" structure and reading flow.17
The Flat Lists: The document also exposes flattened lists of items (texts, tables, pictures). This design is crucial for the indexing layer of Handshake. It allows the Rust backend to iterate rapidly over every text paragraph to generate embeddings, without needing to traverse the complex recursive tree structure.17

###### JSON Serialization and Rust Interoperability

The bridge between Docling (Python) and Handshake (Rust) is JSON. The DoclingDocument serializes to a JSON schema that utilizes JSON Pointers (e.g., "$ref": "#/texts/12") to link items in the tree to their definitions in the flat lists.17
This referencing mechanism is a sophisticated solution to data duplication, ensuring that a text item appearing in multiple logical groups is stored only once. However, it imposes a requirement on the Rust side: the deserialization logic must be capable of resolving these pointers. While there is no native Rust crate for Docling at present (the docling-core library is Python-based), the schema is stable and well-documented, allowing the engineering team to generate compatible Rust structs using serde and serde_json.
The structure of the exported data is lossless, meaning that all metadataâ€”including bounding boxes (prov), page numbers, and confidence scoresâ€”is preserved.19 This allows the Handshake UI to implement features like "Click-to-Source," where highlighting a search result instantly scrolls the original PDF view to the exact coordinate location of the text.

##### 6.1.5.1.5 Integration Architecture: Bridging Rust and Python

The primary engineering challenge in adopting Docling for Handshake is the ecosystem mismatch: Tauri applications are fundamentally Rust binaries managing a WebView, whereas Docling is a complex Python dependency tree. To reconcile this, we must employ the Sidecar Pattern.

###### The Sidecar Pattern: Architecture and Implementation

In the context of Tauri, a "Sidecar" is an external binary bundled with the application that runs as a subprocess. This pattern isolates the Python environment, preventing the heavy ML libraries from bloating the main application memory space or causing instability in the UI thread.5
The recommended implementation strategy involves packaging the Docling environment into a standalone executable using PyInstaller or Nuitka. This "frozen" binary contains the Python interpreter, the Docling library, and all necessary dependencies (PyTorch, Pillow, etc.).
Data Flow Architecture:
Trigger: The user drops a PDF into Handshake.
Command: The Rust main process invokes the Sidecar binary via the tauri::shell::Command API, passing the file path as an argument.
Processing: The Sidecar initializes the Docling pipeline, loads the necessary models (lazily, to conserve RAM), and processes the file.
Output: The Sidecar writes the resulting JSON to a temporary file or streams it to stdout.
Ingestion: Rust reads the JSON, deserializes it into internal structs, and populates the local SQLite database and Vector Index.

###### Inter-Process Communication (IPC) Strategies

While standard input/output (stdio) is the simplest IPC mechanism, transferring large JSON payloads (which can exceed 10MB for large reports) via stdout can encounter buffer limitations or serialization overhead.
A more robust approach for Handshake is to use File-Based IPC or Local Sockets. In the file-based approach, the Sidecar writes the output to a temporary JSON file and simply returns the file path to Rust. This decouples the serialization speed from the pipe bandwidth and simplifies debugging (as the JSON files can be inspected). For a more advanced implementation, the Sidecar could run as a persistent daemon (using docling-serve), exposing a local HTTP server that Rust communicates with.21 This avoids the overhead of spinning up the Python interpreter for every single document, significantly improving performance for batch imports.

###### Licensing and Distribution: The MIT Advantage

A critical factor in the selection of Docling is its MIT License.7 In the landscape of open-source document AI, this is a distinct competitive advantage. Major competitors like Marker and PyMuPDF (in its newer iterations) often carry GPL or AGPL licenses, which effectively mandate that any application linking to them must also be open-source.22
For a commercial or proprietary desktop application like Handshake, a GPL dependency is a "poison pill." Doclingâ€™s permissive MIT license allows the Handshake team to bundle, modify, and distribute the engine without any obligation to release the source code of the wider application. This legal safety, combined with the technical capability, makes Docling the only viable option for a closed-source or open-core business model.

##### 6.1.5.1.6 Performance Profile and Resource Management

"Local-first" implies that the application must run on the hardware the user has, not the hardware we wish they had. Docling is computationally intensive, and its performance profile dictates specific architectural decisions.

###### Hardware Acceleration: CPU vs. GPU vs. MPS

The performance disparity between CPU and GPU execution is stark. Benchmarks indicate that processing a single PDF page can take approximately 3.1 seconds on a standard x86 CPU, compared to 0.49 seconds on an NVIDIA L4 GPU.8 On Apple Silicon (M3 Max), utilizing the Metal Performance Shaders (MPS), the speed is around 1.27 seconds per page.
This variability means that on a standard corporate laptop without a dedicated GPU, processing a 50-page report could take nearly three minutes. This latency is too high to be a blocking operation. Therefore, Handshake must implement a background job queue. The UI should reflect a "Processing" state, allowing the user to continue working while the Sidecar churns through the document queue in the background.

###### Comparison with Competitors

Despite the heavy resource usage, Docling is comparatively efficient in the realm of Deep Learning parsers. Benchmarks show it outperforming Marker (which takes ~16 seconds per page on CPU) and Unstructured (which is often slower and less accurate on tables).8 While heuristic tools like pdftotext are instantaneous, they fail completely on the structural understanding tasks required by Handshake. Docling represents the optimal trade-off: acceptable speed for high-fidelity semantic data.

###### RAM and VRAM Considerations

Loading the full suite of Docling models (Layout, TableFormer, OCR) can consume 2GB to 4GB of RAM.24 For a desktop app, this is a significant footprint. The integration strategy must therefore include lifecycle management for the Sidecar. The Python process should not run permanently; it should be spawned when ingestion tasks are queued and terminated after a timeout period of inactivity, returning resources to the user's system.

##### 6.1.5.1.7 Format Support and the Legacy Compatibility Gap

A universal workspace must handle universal formats. Docling excels with the modern stack but has a blind spot for legacy files.

###### The Modern Suite

Docling provides native, high-fidelity support for:
PDF: Including complex layouts, scanned images, and scientific papers.
Office Open XML: .docx (Word), .xlsx (Excel), .pptx (PowerPoint) are parsed directly from their XML structure, ensuring 100% text accuracy.19
Web & Text: HTML, Markdown, and AsciiDoc are supported, treating web clips as first-class citizens.

###### The Legacy Gap and Mitigation

Crucially, Docling does not support legacy Microsoft binary formats (.doc, .xls, .ppt) or email formats (.msg, .eml).9 For an enterprise user base, this is a critical deficiency.
To mitigate this, Handshake must implement a Transcoding Layer. The most robust solution is to integrate or bundle a headless version of LibreOffice. Before a file reaches the Docling pipeline, the Rust backend should detect legacy MIME types and trigger a conversion:
soffice --headless --convert-to docx legacy_file.doc
The resulting temporary modern file is then fed to Docling. Similarly, for emails, a lightweight Python library like extract-msg should be added to the Sidecar to parse .msg files into text or Markdown, which Docling can then ingest.26

##### 6.1.5.1.8 Enhancing RAG with Structured Data

The ultimate value of integrating Docling lies in the quality of the data it feeds into the Handshake RAG system.

###### Semantic Chunking

Standard RAG pipelines use "naive chunking"â€”splitting text every 500 characters. This often destroys context, splitting a table header from its rows or a section title from its paragraphs.
Doclingâ€™s hierarchical DoclingDocument allows for Semantic Chunking. Handshake can iterate through the document tree, creating chunks that respect logical boundaries. A chunk can be defined as "One Section" or "One Table." Furthermore, because the tree preserves parentage, every chunk can be enriched with its context path (e.g., "Annual Report 2023 > Q4 Financials > Revenue Table"). This "Metadata Enrichment" significantly improves the retrieval accuracy of the Vector Database, allowing the LLM to understand exactly where a piece of information came from.

###### Visual Grounding and Multi-Modality

Doclingâ€™s support for VLM-based captioning means that images and charts are no longer black holes to the search engine. By generating textual descriptions of visual elements, Docling allows Handshake users to perform semantic searches over charts ("Show me the graph depicting rising inflation")â€”a feature that distinguishes Handshake from standard file explorers.16

##### 6.1.5.1.9 Conclusion and Strategic Recommendation

Following a comprehensive architectural evaluation, IBM Docling is strongly recommended as the ingestion engine for the Handshake workspace. Its ability to provide deep, layout-aware understanding of documents aligns perfectly with the project's goal of delivering an AI-enhanced, local-first experience.
While the integration imposes engineering overheadâ€”specifically regarding the Python Sidecar implementation and the need for a legacy transcoding layerâ€”the benefits in data fidelity and structural comprehension are unmatched by other open-source tools. The MIT license secures the commercial future of the application, and the active development by IBM Research suggests a robust roadmap for future capabilities, including advanced VLM integrations. By adopting Docling, Handshake will not merely "read" documents; it will "understand" them, providing a foundation for truly intelligent user interactions.

###### Table 1: Feature Comparison of Document Processing Engines
|Feature|IBM Docling|Marker|Unstructured.io|LlamaParse|
|---|---|---|---|---|
|Primary Approach|Computer Vision / Layout Analysis|Deep Learning / Text Sequence|Hybrid (Rules + Vision)|Proprietary / Cloud API|
|Table Extraction|Excellent (TableFormer model)|Good|Moderate|Good|
|License|MIT (Permissive)|GPL (Restrictive)|Apache 2.0|Proprietary (Paid Service)|
|Local Execution|Yes (Full)|Yes|Yes|No (Cloud First)|
|Legacy Support|No (.doc, .xls unsupported)|No|Limited|Yes (via Cloud)|
|Speed (CPU)|Moderate (~3s/page)|Slow (~16s/page)|Slow|N/A (Cloud latency)|
|Output Model|Structured Hierarchical JSON|Markdown|JSON Elements|Markdown|
###### Table 2: Benchmark Performance (Time per Page)
8
|Hardware Configuration|Docling|Marker|Unstructured|
|---|---|---|---|
|NVIDIA L4 GPU|0.49s|0.86s|N/A|
|Apple M3 Max (MPS)|1.27s|4.20s|2.70s|
|Standard x86 CPU|3.10s|16.0s|4.20s|

###### Works cited
Docling Project - GitHub, accessed December 2, 2025, 
Documentation - Docling - GitHub Pages, accessed December 2, 2025, 
A new tool to unlock data from enterprise documents for generative AI - IBM Research, accessed December 2, 2025, 
Visual grounding - Docling, accessed December 2, 2025, 
Embedding External Binaries - Tauri, accessed December 2, 2025, 
Writing a pandas Sidecar for Tauri | MClare Blog, accessed December 2, 2025, 
docling/LICENSE at main - GitHub, accessed December 2, 2025, 
Docling Technical Report - arXiv, accessed December 2, 2025, 
.DOC is not supported Â· Issue #2293 Â· docling-project/docling - GitHub, accessed December 2, 2025, 
Docling AI: A Complete Guide to Parsing - Codecademy, accessed December 2, 2025, 
docling-project/docling-models - Hugging Face, accessed December 2, 2025, 
IBM Granite-Docling: Super Charge your RAG 2.0 Pipeline, accessed December 2, 2025, 
docling-project/docling-ibm-models - GitHub, accessed December 2, 2025, 
PDF Data Extraction Benchmark 2025: Comparing Docling, Unstructured, and LlamaParse for Document Processing Pipelines - Procycons, accessed December 2, 2025, 
IBM Granite-Docling:. In recent years, many discussions inâ€¦ | by Nandini Lokesh Reddy | Oct, 2025, accessed December 2, 2025, 
Docling: Make your Documents Gen AI-ready - GeeksforGeeks, accessed December 2, 2025, 
Docling Document - GitHub Pages, accessed December 2, 2025, 
My first hands-on experience with Docling | by Alain Airom (Ayrom) - Medium, accessed December 2, 2025, 
Supported formats - Docling - GitHub Pages, accessed December 2, 2025, 
Embedding External Binaries | Tauri v1, accessed December 2, 2025, 
docling-project/docling-serve: Running Docling as an API service - GitHub, accessed December 2, 2025, 
datalab-to/marker: Convert PDF to markdown + JSON quickly with high accuracy - GitHub, accessed December 2, 2025, 
PymuPDF licensing requirements when its a dependency of another dependency? - Reddit, accessed December 2, 2025, 
Recommended Server Specs Â· Issue #385 Â· docling-project/docling-serve - GitHub, accessed December 2, 2025, 
Unlock Your Data: Supercharge Excel Document Processing with Docling, accessed December 2, 2025, 
Email - Docs by LangChain, accessed December 2, 2025, 

---

### 6.1.6 Docling AI Job Profile

**Why**  
Document ingestion is a key AI capability that needs the same job model, provenance, and validation guarantees as editing. Defining it as a profile ensures consistent behavior and auditability.

**What**  
Defines the Docling-specific AI job profile: profile-specific fields, PlannedOperation types, provenance structure, validation rules, and typical job flow.

**Jargon**  
- **source_file_id**: Reference to the external file being ingested.
- **DocLayNet**: Layout analysis model used for page segmentation.
- **TableFormer**: Table extraction model for structured table recovery.
- **source_bbox**: Bounding box coordinates enabling "click-to-source" navigation.

**Implements:** AI Job Model (Section 2.6.6)  
**Profile ID:** `docling_ingest_v0.1`

This profile governs AI jobs that ingest external documents into the Handshake workspace using the Docling pipeline.

#### 6.1.6.1 Profile-Specific Fields

| Field | Type | Description |
|-------|------|-------------|
| `source_file_id` | FileId | Reference to the source file being ingested |
| `target_doc_id` | DocId | Target workspace document to create/populate |
| `ingest_mode` | IngestMode | `full_structure`, `text_only`, `tables_only` |
| `layout_model` | ModelId | Layout analysis model to use (e.g., DocLayNet) |
| `table_model` | ModelId | Table extraction model to use (e.g., TableFormer) |

#### 6.1.6.2 PlannedOperation Types

| Operation | Description |
|-----------|-------------|
| `extract_structure(source_file_id, layout_model)` | Run layout analysis on source file |
| `extract_tables(source_file_id, table_model)` | Extract tables with structure |
| `import_blocks(source_file_id, target_doc_id, block_mapping)` | Import extracted content as workspace blocks |
| `link_provenance(source_file_id, target_doc_id, segment_mapping)` | Establish provenance links |

#### 6.1.6.3 Provenance

Each imported block carries `ai_origin`:
- `job_id`: The ingestion job
- `source_file_id`: Original file reference
- `source_page`: Page number in original
- `source_bbox`: Bounding box coordinates for "click-to-source"
- `extraction_confidence`: Model confidence score

#### 6.1.6.4 Validation Rules

| Validator | Purpose |
|-----------|---------|
| `file_exists` | Source file is accessible |
| `target_writable` | Target document is not locked |
| `format_supported` | File format is in Docling's supported set |
| `resource_available` | GPU/memory available for extraction |

#### 6.1.6.5 Typical Job Flow

```
1. queued          â†’ File uploaded, basic checks passed
2. running         â†’ Docling sidecar processing file
3. awaiting_validation â†’ Extraction complete, preview available
4. awaiting_user   â†’ User reviews extracted structure
5. completed       â†’ Blocks imported, provenance written
```

---

**Key Takeaways**  
- Docling ingestion jobs are AI jobs under the global model ((AI Job Model, Section 2.6.6)).
- The profile adds source/target file references and extraction modes.
- Provenance links imported blocks back to source file coordinates.
- Users review extracted structure before import commits.

---

## 6.2 Speech Recognition: ASR Subsystem

**Why**  
Handshake needs to transcribe long-form audio (lectures, meetings, screen recordings) into searchable, AI-accessible text. Local-first ASR ensures privacy and offline capability.

**What**  
Specifies ASR goals, model landscape, architecture, audio handling, customization policy, privacy, UX, and evaluation framework.

**Jargon**  
- **ASR**: Automatic Speech Recognition.
- **WER**: Word Error Rateâ€”primary accuracy metric.
- **RTF**: Real-Time Factorâ€”latency measure (RTF < 0.5 means 2x faster than real-time).
- **Whisper**: OpenAI's multilingual ASR model (MIT license).
- **Faster-Whisper**: CTranslate2-based optimized Whisper runtime.
- **whisper.cpp**: C++ port for CPU inference with quantization.
- **VAD**: Voice Activity Detectionâ€”identifies speech vs silence.
- **Diarization**: Identifying who spoke when (speaker separation).

---
### 6.2.1 X.1 Goals, Scope, and Constraints

#### 6.2.1.1 Problem Statement

Handshake needs a local-first, high-quality automatic speech recognition (ASR) pipeline that can transcribe long-form audio and video recordings (lectures, meetings, screen recordings) into text. These transcripts must integrate cleanly into the existing Orchestrator, Model Runtime Layer, and Shadow Workspace so that they can be searched, summarized, and used by other AI tools.

#### 6.2.1.2 Non-Goals

For the initial phases, Handshake ASR is **not**:

- A real-time conferencing / live captioning solution.
- A certified transcription system for regulated domains (healthcare, legal records, court reporting).
- A hosted ASR SaaS product.

#### 6.2.1.3 User and Workload Assumptions

- Primary workloads:
  - 1â€“3 hour university lectures.
  - 30â€“120 minute meetings and screen recordings.
- Users are technical or power users comfortable running a local desktop app on a high-end workstation.
- Transcripts are primarily used for:
  - Search and navigation.
  - Summaries and note-taking.
  - Q&A and retrieval-augmented generation (RAG).

#### 6.2.1.4 Hardware Assumptions (Reference Workstation)

The reference development machine for ASR is:

- CPU: AMD Ryzen 9â€“class 16-core CPU.
- RAM: 64â€“128 GB.
- GPU: NVIDIA RTX 3090 (24 GB VRAM) or equivalent.
- Storage: Fast NVMe SSD.

All performance targets and model tiering rules assume this baseline. On weaker machines, the runtime must degrade gracefully by switching to smaller or quantized models and/or CPU-only execution.

### 6.2.2 X.2 Model Landscape and Selection Rationale

#### 6.2.2.1 Open-Source ASR Landscape (Summary)

Handshake primarily targets the open-source ASR ecosystem. The main model families considered are:

- **Whisper**: General-purpose multilingual ASR with strong performance across many languages and domains, large ecosystem support, and efficient runtimes (Faster-Whisper, whisper.cpp).
- **NeMo / Parakeet / Conformer**: High-quality ASR families from NVIDIA and others, usually optimized for server deployments with rich features (streaming, word boosting, etc.).
- **Multilingual / low-resource families**: MMS / Omni-style models, Shunya Pingala, and similar systems targeting many languages.
- **Language-specialised frameworks**: PaddleSpeech, WeNet, SpeechBrain, ESPnet, and others that provide strong models for Mandarin and other specific languages.

Whisper is chosen as the **primary** ASR foundation because of:

- Strong accuracy for English and many other languages.
- Good support for long-form audio.
- Mature community runtimes that run well on desktops.
- Permissive licensing for local inference.

Other families are treated as complementary or experimental options, especially for Mandarin and low-resource languages.

#### 6.2.2.2 Commercial / Cloud ASR (For Comparison Only)

Commercial ASR providers (e.g. big cloud vendors) offer:

- High accuracy for common languages.
- Enterprise features (speaker diarization, PII redaction, compliance certifications).
- Managed infrastructure and SLAs.

However, they conflict with Handshakeâ€™s core requirements:

- Local-first, offline-capable operation.
- No mandatory dependence on cloud services.
- Fine-grained user control over where data flows.

Cloud ASR may be used as an **optional, explicit fallback** for users who choose to enable it, but it is not part of the default design.

#### 6.2.2.3 Design Criteria for Handshake

Model selection is driven by:

- **Accuracy**: Word Error Rate (WER), especially on long-form lecture and meeting content.
- **Latency / Throughput**: Real-time factor (RTF) on the reference workstation.
- **Resource Usage**: VRAM and CPU utilization; ability to coexist with LLMs and image models.
- **Licensing**: Ability to bundle or download models legally for local use.
- **Ecosystem Health**: Active runtimes, community support, and long-term maintainability.

These criteria feed directly into the default model tiering defined in X.2.4.

#### 6.2.2.4 Handshake Default ASR Model Tiering

This section defines the **default ASR model stack and selection rules** for the Handshake reference workstation.

##### 6.2.2.4.1 X.2.4.1 Hardware assumptions

All defaults in this section assume the reference development machine:

- CPU: AMD Ryzen 9â€“class 16-core desktop CPU
- RAM: â‰¥ 64â€“128 GB system memory
- GPU: NVIDIA RTX 3090 (24 GB VRAM) or equivalent
- Storage: Fast SSD (NVMe recommended)

If the userâ€™s hardware is weaker, the runtime **MUST** degrade gracefully by switching to smaller or quantized models as defined below.

##### 6.2.2.4.2 X.2.4.2 Model roles

Handshake distinguishes the following ASR â€œrolesâ€:

1. **Primary general-purpose model**  
   Used for most workloads (lectures, meetings, videos) when GPU resources are available.

2. **Fast / low-resource mode**  
   Used when GPU VRAM is constrained, when the GPU is busy with other tasks, or on CPU-only machines.

3. **Language-specialized options**  
   Optional models for languages where specialized ASR significantly outperforms general multilingual models.

4. **Experimental / research models**  
   Models that are available for benchmarking and experimentation but not enabled by default.

##### 6.2.2.4.3 X.2.4.3 Default model set

The **default Handshake ASR configuration** SHALL use the following model set:

1. **Primary general-purpose model (GPU)**  
   - Model family: OpenAI Whisper  
   - Variant: `large-v3` (or `large-v3-turbo` equivalent)  
   - Runtime: Faster-Whisper (CTranslate2)  
   - Precision: FP16 on GPU  
   - Role:
     - Default for English and general multilingual transcription
     - Expected to cover: English, Chinese, Dutch, Korean, Japanese, Arabic, Russian â€œwell enoughâ€ for non-regulated use

2. **Fast / low-resource model**  
   - Model family: OpenAI Whisper  
   - Variant: `small` or `medium` (exact choice configurable; `small` is default)  
   - Runtime:
     - GPU: Faster-Whisper with INT8 or mixed-precision
     - CPU fallback: whisper.cpp with 4â€“6-bit quantization  
   - Role:
     - Used when the GPU is unavailable or heavily loaded
     - Used for quick-and-dirty transcription where latency matters more than accuracy
     - Used as a safety fallback if large-v3 fails to load

3. **Mandarin-specialised option (optional)**  
   - Candidate families: PaddleSpeech, WeNet (Mandarin-focused conformer models)  
   - Runtime: Project-specific integration behind the same ASR service interface  
   - Role:
     - **Not enabled by default** in the MVP
     - MAY be enabled as an experimental â€œalt engineâ€ for Mandarin benchmarks
     - Used only when the user explicitly selects a Mandarin-specialised profile or during internal evaluation

4. **Experimental multilingual / low-resource models (optional)**  
   - Candidate families: MMS/Omni-style models, Shunya Pingala, SpeechBrain/ESPnet research checkpoints, etc.  
   - Role:
     - **Never used by default** for end users
     - Exposed only behind a developer / diagnostic flag
     - Used for offline research on accents, low-resource languages, and future model swaps

##### 6.2.2.4.4 X.2.4.4 Runtime selection policy

The ASR runtime **MUST** implement a simple, deterministic selection policy:

1. **GPU-happy path**  
   - If:
     - A compatible GPU is present, and  
     - Available VRAM â‰¥ a configurable threshold (default: 8â€“10 GB free), and  
     - The GPU job queue is not saturated,  
   - THEN:
     - Use **Whisper large-v3 via Faster-Whisper (FP16)** as the primary engine.

2. **GPU-constrained path**  
   - If:
     - GPU exists but free VRAM is below threshold, or  
     - The GPU has an active high-priority job (e.g. image model),  
   - THEN:
     - Prefer **Whisper small via Faster-Whisper (INT8/mixed precision)** on GPU.  
     - If that still cannot load or runs out of memory, fall back to CPU mode.

3. **CPU-only path**  
   - If **no compatible GPU is detected**:
     - Use **Whisper small via whisper.cpp** with 4â€“6-bit quantization.  
     - For very long jobs, chunk audio more aggressively to keep memory bounded.

4. **Language-specialised override (optional)**  
   - If the user or a configuration profile explicitly selects a language-specialised engine (e.g. Mandarin profile):
     - Route segments tagged as that language to the configured specialised model.
     - All other segments still use the primary Whisper path.

5. **Cloud fallback (if enabled)**  
   - Cloud ASR **MUST be disabled by default**.  
   - If the user explicitly opts in to cloud fallback, the runtime MAY:
     - Retry failed segments or low-confidence segments with a configured cloud ASR provider.
     - Clearly annotate transcript segments that originate from cloud ASR.

##### 6.2.2.4.5 X.2.4.5 Configuration and observability

- The active model, runtime, and selection path **MUST** be visible in:
  - Developer logs, and
  - A debug/status panel in the app.
- The user **MUST** be able to:
  - Force â€œfast modeâ€ (small model) for low-latency runs.
  - Opt out of experimental/alternative models entirely.
- Model version and configuration **MUST** be recorded as part of the transcript metadata to support reproducibility and regression testing.
### 6.2.3 X.3 Handshake ASR Architecture

This section defines how ASR is integrated into the Handshake system: dataflow, components, interfaces, and how transcripts enter the workspace and Shadow Workspace.

#### 6.2.3.1 High-Level Dataflow

At a high level, ASR in Handshake follows this pipeline:

1. **Source selection**
   - User selects a source resource:
     - An existing audio/video file in the workspace
     - A newly recorded audio/video capture (screen + system audio, microphone, etc.)
   - The source is represented as a **RawContent** resource with a stable ID in the main workspace.

2. **Ingestion request**
   - The desktop client sends a **Transcription Job Request** to the Orchestrator, referencing:
     - Source resource ID
     - Desired language / language hints
     - Priority and quality profile (e.g. â€œfull qualityâ€, â€œfast modeâ€)
   - The request is enqueued in the Orchestratorâ€™s job queue.

3. **Audio extraction and segmentation**
   - The Orchestrator:
     - Extracts audio streams from the source using `ffmpeg` (or equivalent).
     - Normalises format (mono, 16 kHz, 16-bit PCM WAV).
     - Segments audio into manageable chunks using VAD/silence detection and/or fixed windows with overlap.

4. **ASR inference**
   - Segments are dispatched to the **ASR service** (a specific Model Runtime) over a local API.
   - The ASR service returns per-segment transcripts with timestamps and confidence scores.

5. **Assembly and post-processing**
   - The Orchestrator:
     - Assembles segment-level transcripts into a full transcript for the source.
     - Applies light deterministic cleanup (e.g. punctuation, basic casing) as configured.

6. **Storage and Shadow Workspace integration**
   - The final transcript is persisted as **DerivedContent** associated with the source resource.
   - Shadow Workspace ingests the transcript:
     - Indexes it for search and navigation.
     - Exposes it to LLM tools (summaries, Q&A, etc.).
   - The UI updates to show transcript status and results.

The above flow **MUST** be deterministic and reproducible: given the same source, configuration, and model version, the system **SHOULD** produce the same transcript (modulo nondeterminism in beam search).

#### 6.2.3.2 Integration with the Orchestrator and Model Runtime Layer

The ASR subsystem is implemented as one more **Model Runtime** behind the Orchestrator:

1. **Orchestrator responsibilities**
   - Own the high-level transcription workflow:
     - Accept and validate transcription job requests
     - Manage job queue and priorities
     - Orchestrate extraction, segmentation, inference, and assembly
   - Track job state (queued, running, completed, failed) and progress metadata.
   - Expose a stable API to the desktop client.

2. **Model Runtime Layer responsibilities**
   - Provide a **process boundary** for ASR models:
     - A dedicated ASR service binary / container, separate from the Orchestrator.
   - Implement a narrow, versioned API such as:
     - `POST /v1/asr/transcribe_segments`  
       Request: array of audio segments + config  
       Response: array of transcripts + timing + confidences
   - Handle model loading, GPU/CPU selection, batching, and low-level optimizations.

3. **Communication pattern**
   - The Orchestrator **MUST** treat ASR as a black-box service:
     - No direct model invocation within the Orchestrator process.
     - All calls via explicit HTTP/gRPC or an equivalent IPC protocol.
   - This enables:
     - Swapping ASR engines without changing the Orchestrator
     - Running ASR in a separate process / sandbox if needed

4. **Error handling and resilience**
   - The Orchestrator **MUST**:
     - Handle ASR service unavailability (retry with backoff, surface clear errors)
     - Support partial results (segments completed before failure)
     - Record errors and model metadata in job logs for debugging

#### 6.2.3.3 Audio Ingestion (Files, Recordings, System Audio)

The ASR architecture assumes multiple audio sources, but a single abstraction:

1. **Unified â€œmedia sourceâ€ abstraction**
   - Every ASR job references a **MediaSource** object, which encapsulates:
     - Workspace resource ID
     - Physical file path (local)
     - Media type (audio-only / audio+video)
     - Recording metadata (start time, duration, origin)

2. **Supported ingestion modes (MVP)**
   - MVP **MUST** support:
     - Importing existing audio/video files in the workspace
     - Transcribing newly captured recordings created within Handshake
   - Additional modes (e.g. live system audio capture) are **MAY** for later phases and **MUST NOT** complicate the core pipeline.

3. **Extraction requirements**
   - The Orchestrator **MUST**:
     - Use `ffmpeg` (or equivalent) to extract one or more audio streams from the source.
     - Normalize to:
       - Mono (or well-defined channel handling)
       - 16 kHz sample rate
       - 16-bit PCM WAV
     - Enforce a maximum per-segment duration before ASR (e.g. 15â€“30 seconds).

4. **Metadata propagation**
   - The MediaSource and extraction process **MUST** preserve:
     - Original media duration
     - Timestamps (if relevant for aligning transcript to video)
     - Basic technical metadata (codec, original sample rate)

This metadata is required for accurate timeline mapping in the UI and for potential future alignment features (e.g. word-level highlighting on video).

#### 6.2.3.4 Pre-Processing and Segmentation (ffmpeg, Resampling, VAD)

Before ASR, audio **MUST** be pre-processed and segmented:

1. **Pre-processing**
   - Steps:
     - Decode audio to raw PCM using `ffmpeg`.
     - Resample to 16 kHz if needed.
     - Convert to a consistent sample format (e.g. 16-bit signed integer).
   - Goals:
     - Provide a stable, well-known input format to the ASR service.
     - Avoid duplicating audio decoding logic inside the ASR runtime.

2. **Segmentation**
   - The Orchestrator **MUST** support:
     - Voice Activity Detection (VAD) or silence-based segmentation to split long audio into smaller segments.
     - A fallback fixed-window segmenter (with overlap) for cases where VAD fails.
   - Configuration parameters (per profile):
     - Target segment length (e.g. 5â€“15 seconds)
     - Minimum/maximum segment length
     - Overlap duration between segments
   - Segmentation decisions **MUST** be recorded (start/end times per segment) to allow deterministic re-assembly.

3. **Quality vs speed profiles**
   - Different profiles (e.g. â€œqualityâ€ vs â€œfastâ€) **MAY**:
     - Use different VAD sensitivity
     - Use different maximum segment lengths
   - The chosen profile **MUST** be stored as part of the job configuration.

#### 6.2.3.5 ASR Service Interface (APIs, Job Queue, Progress Reporting)

The interface between desktop client, Orchestrator, and ASR service **MUST** be explicit and versioned.

1. **Client â†” Orchestrator API**

   Minimum required endpoints:

   - `POST /v1/asr/jobs`
     - Input:
       - MediaSource reference (resource ID)
       - Language hints, profile (quality/fast)
       - Optional job metadata (user notes, tags)
     - Output:
       - Job ID
       - Initial status (`queued`)

   - `GET /v1/asr/jobs/{job_id}`
     - Output:
       - Status (`queued` | `running` | `completed` | `failed` | `cancelled`)
       - Progress estimate (0â€“100%, and/or processed duration vs total duration)
       - Basic timing and model metadata once available

   - `DELETE /v1/asr/jobs/{job_id}`
     - Cancels a running job if possible.

2. **Orchestrator â†” ASR Service API**

   Minimum required RPC:

   - `TranscribeSegments` (HTTP/gRPC)
     - Request:
       - Array of segments (raw PCM buffers or paths to temp files)
       - Model/runtime configuration (language, profile, GPU/CPU preference)
     - Response:
       - Array of transcripts (text)
       - Token/word-level timestamps (where available)
       - Confidence scores per segment or per token
       - Optional normalized text (post-punctuation)

3. **Job queue and concurrency**

   - The Orchestrator **MUST**:
     - Maintain a per-user ASR job queue.
     - Limit concurrent jobs according to resource constraints (e.g. maximum concurrent ASR jobs using GPU).
     - Surface queue position and estimated start time when possible.

   - The ASR service **SHOULD**:
     - Implement internal batching when it improves throughput (multiple segments per forward pass).
     - Respect a maximum concurrency configured by the Orchestrator.

4. **Progress reporting**

   - Progress to the client **MUST** be based on:
     - Total media duration vs processed duration, and/or
     - Segment count vs completed segments.
   - The client **SHOULD**:
     - Display coarse-grained progress (e.g. â€œ34 min processed of 90 minâ€).
     - Show final latency and effective real-time factor for diagnostic purposes.

#### 6.2.3.6 Transcript Storage as DerivedContent and Shadow Workspace Integration

Once ASR has completed, the transcript becomes part of the Handshake data model.

1. **DerivedContent representation**

   - Each transcript **MUST** be stored as a **DerivedContent** object linked to:
     - The original MediaSource resource ID.
     - The ASR job ID and model configuration used.
   - Minimum fields:
     - Plain text transcript (UTF-8)
     - Optional structured representation (e.g. JSON with segments, speakers, timestamps)
     - Model metadata (name, version, runtime profile)
     - Job metadata (start/end time, duration, status, errors)

2. **Versioning**

   - Transcripts **MUST** be versioned:
     - Re-transcription with different models or settings creates a new DerivedContent version.
     - Prior versions **MUST** remain accessible for debugging and comparison until explicitly deleted.
   - The UI **SHOULD**:
     - Allow users to see which transcript is â€œactiveâ€ and switch if needed.

3. **Shadow Workspace ingestion**

   - Shadow Workspace **MUST**:
     - Ingest the transcript as soon as the job completes.
     - Create or update embeddings, search indexes, and graph nodes for the transcript.
   - The transcript **MUST** be:
     - Discoverable via global search and filters (e.g. â€œtype:transcriptâ€).
     - Addressable by stable IDs for LLM tools (e.g. â€œsummarize transcript Xâ€).

4. **LLM tools and downstream use**

   - ASR transcripts **MUST** be first-class inputs to:
     - Summarization tools (lecture/meeting summaries)
     - Extraction tools (action items, decisions, entities)
     - Q&A over content (RAG)
   - These tools **MUST NOT** modify the original transcript; they produce additional DerivedContent artifacts (summaries, notes, etc.) with their own IDs and metadata.

5. **Data retention and privacy**

   - Transcripts **MUST** be treated as user-owned workspace data:
     - Stored locally by default
     - Syncing and cloud usage (for LLMs or ASR fallback) only if the user has opted in.
   - Any external calls (e.g. cloud LLM summarization) **MUST** be clearly documented and, where possible, logged at a metadata level (not full content) for user inspection.

This completes the definition of the ASR architecture as integrated into the Handshake Orchestrator, Model Runtime Layer, and Shadow Workspace.
### 6.2.4 X.4 Runtime Modes: Batch vs Streaming

This section defines the execution modes for ASR in Handshake: what is supported in the MVP and what is explicitly deferred. The core assumption is that Handshake is a **local-first desktop app** optimized for ingesting long-form content (lectures, meetings, videos), not a real-time conferencing tool.

#### 6.2.4.1 Batch (Offline) Transcription â€“ MVP Scope

The Handshake ASR MVP **ONLY** supports batch (offline) transcription:

1. **Definition**

   - Batch transcription = transcribing a **finite, already-recorded** audio/video resource.
   - Examples:
     - University lecture recordings (1â€“3 hours)
     - Meeting recordings (30â€“120 minutes)
     - Screen recordings with system audio

2. **User flow**

   - User selects a media resource in the workspace (or records a new one).
   - User triggers a â€œTranscribeâ€ or â€œGenerate transcriptâ€ action.
   - The client:
     - Sends a transcription job request to the Orchestrator (see X.3.5)
     - Shows job state and progress (queued â†’ running â†’ completed/failed)
   - Once completed:
     - A transcript DerivedContent object is attached to the media resource
     - Shadow Workspace indexes the transcript and exposes it to tools (X.3.6)

3. **Batch mode guarantees**

   - The system **MUST**:
     - Handle recordings up to at least 3 hours on the reference workstation.
     - Provide stable progress reporting (see X.3.5).
     - Avoid UI freezes and respect global resource limits (GPU and CPU).

   - The system **SHOULD**:
     - Resume partially completed jobs after crashes where possible
     - Support cancellation without corrupting existing DerivedContent

4. **Scheduling and resource usage**

   - Batch jobs are **background** jobs:
     - They may run at lower priority than interactive UI and other critical tasks.
     - They may be paused/throttled under heavy system load.
   - The Orchestrator **MUST**:
     - Enforce configurable limits on concurrent ASR jobs
     - Coordinate GPU usage with other model runtimes (LLMs, image models, etc.)

#### 6.2.4.2 Lecture-Length Workloads (1â€“3h Recordings)

Handshake explicitly targets **lecture-length workloads** as a primary use case.

1. **Scale assumptions**

   - Typical lecture: 60â€“90 minutes
   - Upper bound for MVP: 180 minutes (3 hours)

2. **Segmentation strategy**

   - For long recordings, the Orchestrator **MUST**:
     - Use VAD/silence-based segmentation or fixed windows to produce segments that:
       - Fit comfortably in the ASR modelâ€™s context window
       - Do not exceed a configured maximum length (e.g. 15â€“30 seconds)
     - Retain accurate segment timestamps for later assembly and navigation.

3. **Performance targets (on reference workstation)**

   - Targets (guidance, not hard guarantees):
     - **Real-time factor (RTF)**: aim for RTF â‰¤ 1.0 on the primary GPU model for typical lectures.
       - Example: 90-minute lecture should finish in â‰ˆ 90 minutes or faster in â€œqualityâ€ mode.
     - **Memory usage**:
       - ASR **MUST NOT** starve LLM runtimes; GPU VRAM thresholds and prioritization rules in X.2.4 apply.

4. **UX considerations**

   - For long jobs, the UI **SHOULD**:
     - Show progress based on processed duration vs total duration (X.3.5).
     - Provide a summary of resource usage (e.g. â€œProcessed 120 min in 80 min; effective RTF 0.67â€).
     - Optionally, allow the user to open partial transcripts (e.g. first hour) once certain milestones are reached (future enhancement; not mandatory for MVP).

#### 6.2.4.3 Future: Streaming / Live Captions (Out of Scope for MVP)

Real-time streaming / live captions are **explicitly out of scope** for the initial ASR MVP, but the architecture **MUST NOT** make them impossible.

1. **Definition**

   - Streaming = transcribing audio as it is being captured, with:
     - End-to-end latency small enough for live captions or near-real-time monitoring.
     - Continuous, unbounded audio streams (no fixed recording duration known in advance).

2. **Non-goals for MVP**

   - MVP **MUST NOT** attempt to:
     - Provide live subtitles for video calls or live streaming platforms.
     - Guarantee sub-second latency for partial hypotheses.
     - Support gRPC/WebSocket streaming APIs from the desktop client.

3. **Architectural hooks for future streaming**

   Even though streaming is not implemented in the MVP, the design **SHOULD** leave room for:

   - A **session-based** ASR API in the Orchestrator, distinct from batch jobs:
     - e.g. `StartStream`, `SendAudioChunk`, `ReceivePartial`, `EndStream`
   - An ASR runtime that can:
     - Consume short audio frames (e.g. 10â€“30 ms)
     - Produce partial hypotheses and revisions over time

   These APIs are **not required** for MVP but must be architecturally compatible with X.3â€™s process boundaries and Model Runtime Layer.

4. **Model and performance implications (for later phases)**

   - Streaming will impose stricter constraints:
     - Lower per-utterance latency
     - Tighter control over GPU sharing with other models
   - It may require:
     - Streaming-optimized model variants (e.g. chunk-wise or online models)
     - More aggressive chunking and incremental decoding strategies

   These requirements are acknowledged but **not implemented** in the first version.

#### 6.2.4.4 Design Constraints for Future Streaming Support

To avoid painting the architecture into a corner, the following constraints apply for all MVP implementations:

1. **Segment-based internal representation**

   - Even in batch mode, the Orchestrator and ASR runtime **MUST** treat audio as an ordered sequence of segments:
     - Each segment has explicit start/end timestamps and an ID.
     - The ASR API operates over arrays of segments (X.3.5).
   - This segment abstraction is the natural bridge to future streaming, where segments become small time slices.

2. **Stateless vs stateful ASR services**

   - The batch MVP may treat ASR requests as stateless (â€œfire and forgetâ€ per segment or batch).
   - However, the service interface **MUST** be designed so that:
     - It can later support session or stream IDs.
     - Model internal state (e.g. online decoding state) can be maintained across calls when needed.

3. **Resource arbitration**

   - Even without streaming, the GPU resource manager in the Orchestrator **MUST**:
     - Be explicit and centralized (no â€œhiddenâ€ GPU users).
     - Support future constraints like â€œlow-latency stream takes precedence over batch jobs.â€

4. **UI separation**

   - Batch transcription UI **MUST** be clearly separated from any future â€œlive captionâ€ UI:
     - Different affordances and expectations (batch = eventual completion; streaming = continuous partials).
   - This avoids coupling design decisions that would be hard to untangle later.

In summary, the MVP supports **batch ASR for long-form recordings only**, with the architecture structured so that **streaming capabilities can be added later** without breaking the Orchestratorâ€“ASR service contract or the data model.
### 6.2.5 X.5 Customization and Fine-Tuning

This section defines how Handshake customizes ASR behavior for different domains, vocabularies, and languages. It builds on the model tiering in X.2.4 and the architecture in X.3.

#### 6.2.5.1 Types of Customization

Handshake distinguishes the following levels of customization, ordered from cheapest to most expensive:

1. **Runtime configuration (no training)**
   - Examples:
     - Language hints and â€œforce languageâ€ options
     - Beam search configuration (beam size, temperature, length penalties)
     - Timestamp policies (segment-level vs word-level)
   - Characteristics:
     - Zero training cost
     - Immediate, reversible
   - Usage:
     - Exposed as profiles (e.g. â€œgeneral lectureâ€, â€œmeetingâ€, â€œno timestampsâ€).

2. **Text-only post-processing (no ASR model changes)**
   - Examples:
     - Punctuation and casing normalization
     - Normalizing numbers, dates, and common abbreviations
     - Basic profanity filtering (optional, user-configurable)
   - Implementation:
     - Deterministic rules (regex, small finite-state machines)
     - Lightweight text models where appropriate
   - Characteristics:
     - No changes to the acoustic/decoder model
     - Cheap to iterate on and easy to test

3. **Lexicon / biasing / LM-rescoring (where supported)**
   - Examples:
     - Domain-specific word/phrase lists for:
       - Product names, company names
       - Course titles, project codenames
     - Text-only language models (n-gram LMs) to rescore candidate transcripts
   - Characteristics:
     - Requires toolkit support (not all runtimes expose this cleanly)
     - Does not require paired audioâ€“text training

4. **Adapter-based or LoRA-based fine-tuning**
   - Small trainable modules attached to a frozen base model:
     - Per-domain adapters (e.g. â€œsoftware engineering lecturesâ€, â€œmedical research talksâ€)
     - LoRA layers on top of core encoder/decoder blocks
   - Characteristics:
     - Lower risk than full fine-tune
     - Easier to enable/disable per domain

5. **Full checkpoint fine-tuning**
   - Training the base modelâ€™s parameters (or large subsets) on in-domain audio+text.
   - Characteristics:
     - Highest potential gain
     - Highest cost and risk (overfitting, regressions)
   - Controlled by the fine-tuning gate in X.5.2.

Handshake **MUST** prioritize options 1â€“3 for the MVP, and treat 4â€“5 as later-phase optimizations governed by X.5.2.

#### 6.2.5.2 Fine-Tuning Gate and Customization Policy

This section defines **when** Handshake is allowed to fine-tune ASR models, and which lighter-weight customization options **MUST** be tried first.

By default, Handshake ships with **unmodified open-source checkpoints** (see X.2.4). Fine-tuning is an **optional, later-phase optimization**, not part of the MVP.

##### 6.2.5.2.1 X.5.2.1 Customization hierarchy

Handshake SHALL follow this hierarchy, from cheapest to most expensive:

1. **Configuration and prompting**  
   Enable or adjust built-in features of the ASR runtime:
   - Language hints
   - Temperature / beam settings
   - Timestamps, word-level timing options  
   Use LLM-based post-processing for:
   - Sectionization and headings
   - Summaries, action items, Q&A  
   No model training involved.

2. **Lexicons, biasing, and rescoring (where supported)**  
   If the chosen runtime supports it, Handshake MAY:
   - Add domain-specific word/phrase lists (brand names, jargon, entities)
   - Use a text-only language model or n-gram LM to rescore ASR hypotheses  
   This option **MUST** be evaluated before full fine-tuning.

3. **Lightweight domain adapters (if available)**  
   If the ecosystem provides adapter-style fine-tuning (LoRA, adapters):
   - Prefer that over full checkpoint fine-tuning.
   - Keep adapter weights small and modular per domain.

4. **Full fine-tuning of base models**  
   This is the **last resort** and **MUST** pass the gate criteria in X.5.2.2.

##### 6.2.5.2.2 X.5.2.2 Fine-tuning entry criteria (â€œthe gateâ€)

Handshake **MUST NOT** initiate fine-tuning of any ASR model unless **all** of the following conditions are true:

1. **Data volume and quality**
   - There is **at least 100 hours** of in-domain audio per target language/domain with:
     - Reasonably clean recordings (no catastrophic noise)
     - Human-checked transcripts with time alignment accurate enough for training
   - For more ambitious gains, the preferred target is **200â€“300 hours** per language/domain.
   - Data **MUST** be collected with explicit user consent and stored in a way that respects privacy requirements.

2. **Demonstrated accuracy gap**
   - A stable evaluation suite exists (see X.8) with:
     - A fixed test set of in-domain audio
     - Baseline metrics for the current untuned model
   - On that test set, the baseline model shows:
     - Word Error Rate (WER) clearly above the acceptable target for the use case, **and**
     - Qualitative errors that materially affect downstream tasks (misrecognised key terms, domain jargon, entity names).
   - Lighter customizations (X.5.2.1 â€“ levels 1â€“3) have already been applied and tested and are **insufficient** to close the gap.

3. **Expected benefit**
   - There is a reasonable expectation, based on prior art or small-scale experiments, that fine-tuning can:
     - Improve WER by at least **25% relative** (e.g. from 20% â†’ 15%) **or**
     - Reduce critical error classes (e.g. entity names) enough to materially improve UX.
   - This expectation **MUST** be documented in a short design note before training begins.

4. **Compute and operational capacity**
   - Dedicated training hardware is available (e.g. a separate GPU machine or cloud training environment).  
     Fine-tuning **MUST NOT** run on end-user devices.
   - There is capacity to:
     - Run multiple training runs for hyperparameter tuning
     - Store, version, and roll back fine-tuned checkpoints
     - Maintain separate â€œstableâ€ and â€œexperimentalâ€ ASR configurations

5. **Governance and rollback**
   - Each fine-tuned model version **MUST**:
     - Have a unique version ID and changelog
     - Be evaluated against the same test suite as the baseline
   - A rollback plan **MUST** exist:
     - If a new model regresses on any tracked metric beyond tolerance, the system **MUST** be able to immediately revert to the previous stable model.

If any of these conditions are not met, fine-tuning is **not allowed**. The team **MUST** continue using untuned models plus lighter customizations.

#### 6.2.5.3 Data Collection and Labeling Strategy

Customization and fine-tuning depend on data. Handshakeâ€™s strategy is:

1. **Default stance: no automatic collection**
   - By default:
     - User audio and transcripts stay local.
     - No audio is uploaded or collected for centralized training.
   - Any deviation from this default (e.g. opt-in data donation) **MUST** be explicit and clearly documented.

2. **Local usage data for personalization (optional, future)**
   - The client **MAY** track:
     - Local corrections the user makes to transcripts
     - User-defined vocabularies (custom terms, names)
   - This information can feed:
     - Local lexicons
     - Local bias lists or on-device post-processing rules
   - This does **not** require server-side model training.

3. **Opt-in data donation for model training (non-MVP)**
   - Only relevant if you decide to build a central training pipeline.
   - Requirements:
     - Explicit user opt-in per workspace or per recording
     - Clear description of:
       - What is collected (audio, transcript, metadata)
       - How it is anonymised or pseudonymised
       - How users can revoke consent and request deletion
   - Donated data **MUST** be:
     - Aggregated
     - Auditable
     - Separable by domain and language

4. **Labeling and quality control**
   - For any training/fine-tuning:
     - A subset of data **MUST** be manually checked or corrected.
     - Weak labels (ASR-generated transcripts) **MAY** be used but:
       - A manually verified validation/test set is mandatory.
   - Labeling **MUST** be guided by eval needs:
     - If the goal is to fix jargon, labels must accurately mark those terms.
     - If the goal is speaker diarization, speaker turns must be reliable.

5. **Data schema**
   - Training data **MUST** use a consistent schema:
     - Audio file path / ID
     - Text transcript
     - Language tag
     - Domain tag (e.g. â€œCS lectureâ€, â€œproduct meetingâ€)
     - Optional metadata (speaker count, noise level, recording device)
   - This schema is shared between:
     - ASR eval (X.8)
     - Any future fine-tuning pipeline

#### 6.2.5.4 Training and Deployment Strategy

If/when Handshake fine-tunes ASR models (under X.5.2), training and deployment follow these rules:

1. **Separation of concerns**
   - Training **MUST NOT** happen in the desktop app.
   - Training and evaluation happen on:
     - Dedicated internal machines, or
     - Explicit training environments (cloud or on-prem).

2. **Model lifecycle**
   - For each model type (e.g. â€œWhisper-large-v3â€), define:
     - Base model (untuned)
     - 0+ domain-specific derivatives (e.g. â€œCS-lecture-tuned-v1â€)
   - Derivatives **MUST**:
     - Declare their base model
     - Be compatible with the same ASR service API

3. **Versioning and promotion**
   - New model versions follow a promotion path:
     1. Experimental:
        - Used only in internal tests and behind developer flags.
     2. Candidate:
        - Passes basic eval but not yet default.
     3. Default:
        - Promoted after:
          - Passing all eval gates in X.8
          - No regressions on critical metrics
   - Demotion:
     - If a default model regresses in real-world use, it **MUST** be demoted and replaced by the previous stable model.

4. **Deployment to clients**
   - Desktop clients receive model updates via:
     - Bundled binaries or
     - Separate model packages (downloaded on demand)
   - The client **MUST**:
     - Verify model package integrity (checksum/signature)
     - Store model version and configuration with each transcript (X.3.6)

5. **Resource envelopes**
   - Fine-tuned models **MUST** respect predefined resource envelopes:
     - Max VRAM usage on reference GPU
     - Max latency for standard workloads
   - If a fine-tuned model exceeds these envelopes, it is **not eligible** to become the default.

6. **Documentation**
   - Each trained model **MUST** have:
     - A model card (data, domains, languages, limitations)
     - An evaluation report (baseline vs tuned)
     - Operational notes (resource usage, compatibility constraints)

This ensures that any customization beyond configuration is controlled, measurable, and reversible.

### 6.2.6 X.6 Post-Processing, Diarization, and LLM Tools

This section defines how raw ASR outputs are cleaned up, enriched with speaker information, and made available to LLM tools and the Shadow Workspace.

#### 6.2.6.1 Deterministic Cleanup (Punctuation, Casing, Numbers)

1. **Goals**
   - Improve readability without changing the semantic content.
   - Avoid making opaque, model-dependent edits that are hard to reason about.

2. **Scope of deterministic cleanup**
   - Basic punctuation:
     - Sentence boundaries (periods, question marks)
     - Commas for obvious pauses where safe
   - Casing:
     - Sentence-initial capitalization
     - Proper nouns where unambiguous (optional)
   - Normalization:
     - Numbers (e.g. â€œtwenty oneâ€ â†’ â€œ21â€) when safe
     - Standard abbreviations (e.g. â€œU S Aâ€ â†’ â€œUSAâ€), if reliably detectable

3. **Implementation**
   - Prefer:
     - Lightweight models or rules provided by the ASR toolkit
     - Simple, testable rule-based passes over transcripts
   - Requirements:
     - Cleanup steps **MUST** be deterministic given the same input
     - Cleanup configuration **MUST** be stored with the transcript metadata

4. **Configuration and visibility**
   - Users **SHOULD** be able to:
     - Toggle some aspects of cleanup (e.g. aggressive vs minimal punctuation)
   - For diagnostics:
     - The â€œrawâ€ ASR output (pre-cleanup) **SHOULD** be accessible to developers and power users.

#### 6.2.6.2 Diarization (Speaker Turns, Optional)

Speaker diarization is optional for the MVP but is highly desirable for meetings and multi-speaker content.

1. **Responsibilities**
   - Identify â€œwho spoke whenâ€:
     - Segment audio into speaker-homogeneous regions
     - Assign speaker IDs (e.g. SPK1, SPK2, â€¦)
   - Align these regions with transcript segments.

2. **Architecture**
   - Diarization is a separate step from ASR:
     - Can run:
       - Before ASR (to segment by speaker) or
       - After ASR (to label segments)
   - It **MAY** use:
     - External toolkits (e.g. embedding-based diarization)

3. **MVP stance**
   - Diarization is **MAY** for the initial release:
     - The architecture must allow adding it later.
     - The transcript schema (X.3.6) **SHOULD** leave room for:
       - Per-segment speaker labels
       - Optional speaker name mappings (user-labeled)

4. **Data model**
   - Transcript representation **SHOULD** support:
     - `speaker_id` per segment or per sentence
     - Separate mapping from `speaker_id` â†’ human-friendly label (e.g. â€œAliceâ€)

5. **UI**
   - When diarization is present:
     - UI **SHOULD** visually distinguish speakers (color, label)
     - Users **SHOULD** be able to rename speakers (SPK1 â†’ â€œAliceâ€)

#### 6.2.6.3 LLM-Based Transforms (Summaries, Action Items, Q&A)

ASR transcripts are a primary input to Handshakeâ€™s LLM tools. These tools **MUST NOT** overwrite the transcript itself; they produce additional DerivedContent.

1. **Transform types**
   - Summarization:
     - High-level summaries (short, long)
     - Section-wise summaries (per lecture topic)
   - Extraction:
     - Action items
     - Decisions
     - Key entities (people, projects, terms)
   - Q&A:
     - User questions about a specific transcript or across multiple transcripts

2. **Execution model**
   - All LLM transforms:
     - Take one or more transcript IDs as input
     - Run via the Orchestrator using existing LLM model runtimes
   - Outputs:
     - New DerivedContent objects (summaries, lists, notes)
     - Linked back to the original transcript(s)

3. **Isolation from ASR**
   - ASR **MUST** remain a separate concern:
     - Changes in LLM behavior (e.g. different summary style) do not affect the underlying transcript.
     - Transcript correctness is evaluated independently of LLM outputs.

4. **Configuration**
   - Users **SHOULD** be able to:
     - Choose which transforms to run (e.g. â€œonly summaryâ€ vs â€œsummary + action itemsâ€)
     - Re-run transforms with updated models without re-running ASR

#### 6.2.6.4 How Transcripts Flow into Shadow Workspace, Search, and RAG

1. **Indexing in Shadow Workspace**
   - On completion, each transcript DerivedContent object is:
     - Parsed into logical units (paragraphs, segments, or time-coded blocks)
     - Embedded (vector representations) for semantic search
     - Inserted into the global index

2. **Search behavior**
   - Transcripts **MUST** be:
     - Searchable by full-text (keywords)
     - Searchable by semantic similarity (embeddings)
   - Filters:
     - Type: transcript
     - Source: video/meeting/lecture
     - Time range, language, tags

3. **RAG (Retrieval-Augmented Generation) integration**
   - When users ask questions, the retrieval layer **MAY**:
     - Pull relevant transcript chunks
     - Feed them to LLMs as context
   - Requirements:
     - Chunks **MUST** preserve pointers back to:
       - Original transcript
       - Timestamps in the media (for â€œjump to videoâ€ UX)

4. **Cross-linking**
   - Derived artifacts (summaries, notes, extracted items) **SHOULD**:
     - Maintain links back to transcript segments
     - Be traversable in the UI (e.g. click an action item â†’ jump to the moment in the transcript/video)

5. **Privacy and scope**
   - By default, all indexing and RAG usage:
     - Happens locally
     - Uses local LLMs where configured
   - Cloud usage (for embedding or LLM) **MUST** be:
     - Explicitly configurable
     - Clearly indicated in UI and documentation

This completes the definition of how raw ASR output becomes clean, structured, and LLM-ready content in the Handshake workspace and Shadow Workspace.
### 6.2.7 X.7 Risk, Compliance, and Limitations

This section enumerates the primary risks of ASR in Handshake and defines how they are mitigated. It also clarifies compliance stance and explicit non-goals.

#### 6.2.7.1 Technical Risks

1. **Latency and throughput**
   - Risk:
     - Long recordings take too long to process, blocking user workflows.
   - Mitigation:
     - Use GPU-accelerated models where available (X.2.4).
     - Enforce segmentation and batching (X.3.4, X.3.5).
     - Treat ASR as background work with clear progress reporting (X.4.1, X.4.2).

2. **Memory and resource contention**
   - Risk:
     - ASR models consume GPU VRAM / CPU and starve LLMs or the UI.
   - Mitigation:
     - Centralized resource arbitration in the Orchestrator.
     - Strict VRAM thresholds and model tiering (X.2.4.4).
     - Limits on concurrent jobs; ability to pause or throttle ASR.

3. **Model drift and regression**
   - Risk:
     - Updating ASR models silently degrades accuracy on certain domains or languages.
   - Mitigation:
     - Model versioning and model cards (X.5.4).
     - Mandatory eval suite runs before promotion (X.8).
     - Regression thresholds and rollback capability.

4. **Multilingual and accent robustness**
   - Risk:
     - Good performance on English but poor on other target languages or accents.
   - Mitigation:
     - Multilingual primary model (Whisper).
     - Language-specialised experimental models (X.2.4.3).
     - Per-language eval subsets (X.8.2).
     - Clear documentation of known limitations in model cards.

5. **Dependency / ecosystem health**
   - Risk:
     - Core dependencies (Faster-Whisper, whisper.cpp, VAD libraries) change or break.
   - Mitigation:
     - Pin versions in deployment.
     - Maintain a minimal abstraction layer around ASR runtimes.
     - Keep a simple CPU-only fallback path that depends on fewer components.

#### 6.2.7.2 Multilingual Coverage and Accent Robustness

1. **Target languages**
   - Handshakeâ€™s primary ASR targets:
     - English, Chinese (Mandarin)
   - Secondary â€œnice-to-haveâ€ targets:
     - Dutch, Korean, Japanese, Arabic, Russian

2. **Baseline expectations**
   - The primary model (Whisper large-v3) is expected to be:
     - Strong for English, adequate for major languages.
     - Imperfect for some accents and low-resource languages.

3. **Mitigation and transparency**
   - For each supported language:
     - Maintain per-language WER in the eval suite.
     - Document whether expected quality is:
       - â€œGood enough for notes and summariesâ€
       - Or â€œnot recommended beyond rough referenceâ€.

4. **Accent and domain gaps**
   - Where severe gaps are identified:
     - Prefer domain-specific guidelines and UX warnings first.
     - Consider:
       - Better configuration/segmentation
       - Lexicons / rescoring
       - Experimental models or, later, fine-tuning (X.5.2)

#### 6.2.7.3 Licensing and Open-Source Obligations

1. **Model licensing**
   - All default ASR models **MUST**:
     - Be under licenses that permit local inference and redistribution in the intended distribution model of Handshake.
   - For each model:
     - License type **MUST** be documented in the model card.
     - Any usage restrictions **MUST** be clearly surfaced in internal docs.

2. **Library licensing**
   - Core ASR libraries (Faster-Whisper, whisper.cpp, etc.) **MUST**:
     - Have compatible licenses with the Handshake codebase.
   - Third-party code **MUST**:
     - Be tracked, pinned, and attributed according to its license.

3. **Attribution**
   - Handshake **SHOULD**:
     - Provide a â€œThird-Party Componentsâ€ section listing:
       - ASR models used
       - Toolkits and key libraries
       - Their respective licenses

#### 6.2.7.4 Privacy and Compliance Stance

1. **Default data flow**
   - By default:
     - All ASR happens locally.
     - Audio and transcripts are stored locally as workspace data.
     - No audio or transcript content is sent to remote servers for ASR.

2. **Compliance scope**
   - Handshake ASR is **NOT** designed or marketed as compliant for:
     - Regulated healthcare transcription (e.g. HIPAA-covered clinical dictation).
     - Legal record creation where certified transcripts are required.
   - If users choose to apply Handshake in those contexts, they do so at their own risk.

3. **Cloud usage (if enabled)**
   - If cloud ASR or cloud LLMs are enabled:
     - This **MUST** require explicit opt-in.
     - The UI **MUST** clearly indicate:
       - Which jobs use cloud services
       - Which providers are involved
   - Telemetry or logs:
     - **MUST NOT** include raw audio or full transcripts without explicit user consent.

4. **User control**
   - Users **MUST** be able to:
     - Delete transcripts and media from their workspace.
     - Disable any cloud-based ASR or LLM integration.
   - Any centralized training or data donation programs (if ever added):
     - **MUST** be opt-in and clearly documented (X.5.3).

#### 6.2.7.5 Cloud Fallback Policy

1. **Default**
   - Cloud fallback for ASR is **disabled by default**.

2. **Optional behavior**
   - When explicitly enabled by the user:
     - The Orchestrator **MAY**:
       - Retry failed segments or low-confidence segments on a configured cloud ASR provider.
     - The transcript:
       - **MUST** annotate which segments came from cloud vs local ASR.

3. **Failure and error handling**
   - If cloud ASR fails or is unavailable:
     - The system **MUST NOT** silently drop segments.
     - The final transcript **MUST** clearly indicate missing or failed sections.

4. **Config surface**
   - Cloud fallback settings **MUST** be:
     - Centralized in a single configuration UI
     - Clearly labeled as sending data off-device

### 6.2.8 X.8 Evaluation and Benchmarks

This section defines how ASR quality and performance are measured and guarded over time.

#### 6.2.8.1 Metrics

Handshakeâ€™s ASR eval suite **MUST** track at least:

1. **Accuracy metrics**
   - Word Error Rate (WER)
   - Character Error Rate (CER) (especially for languages where word boundaries are ambiguous)
   - Optional: entity-level error metrics for key terms (names, technical terms)

2. **Performance metrics**
   - Real-time factor (RTF):
     - `RTF = transcription_time / audio_duration`
   - Latency distribution for segments (p50, p90)
   - GPU VRAM usage and peak CPU usage

3. **Robustness metrics**
   - Per-language WER/CER
   - Per-domain WER/CER (lectures vs meetings vs misc.)
   - Optional: error breakdowns by:
     - Background noise level
     - Accent categories (if labeled)

#### 6.2.8.2 Benchmark Datasets and Synthetic Workloads

1. **Dataset types**
   - Internal, in-domain datasets:
     - Real recordings of lectures, meetings, and user-like content.
   - External public benchmarks (where licensing allows):
     - For cross-checking against known baselines.

2. **Label quality**
   - Test sets **MUST**:
     - Have human-verified transcripts.
     - Be stable across model versions (no silent changes without versioning).

3. **Coverage**
   - Datasets **SHOULD** include:
     - English lectures and meetings (core)
     - Chinese content (at least some lectures/conversations)
     - Smaller but non-zero samples for other target languages

4. **Synthetic workloads**
   - For performance testing:
     - Synthetic â€œlong lectureâ€ jobs MAY be generated by concatenating shorter clips.
   - These workloads:
     - **MUST** stress segmentation, queueing, and resource usage.

#### 6.2.8.3 Target Thresholds for â€œGood Enoughâ€

Target thresholds depend on domain; as a starting point:

1. **Lectures and meetings (EN)**
   - WER:
     - Target: â‰¤ 10â€“12% on internal eval set
     - Warning band: 12â€“15%
     - Fail: > 15% (requires investigation before promotion)
   - RTF (reference workstation, primary model):
     - Target: â‰¤ 1.0
     - Warning band: 1.0â€“1.5
     - Fail: > 1.5 for typical workloads

2. **Non-English target languages**
   - WER/CER targets:
     - Initially looser, e.g.:
       - Target: â€œcomparable to or slightly worse than EN baselineâ€
       - Explicitly documented when significantly worse
   - Expectation:
     - Sufficient for summarization and note-taking, not verbatim transcripts.

3. **Resource usage**
   - VRAM:
     - Primary ASR model **MUST** fit within a defined budget on the reference GPU with headroom for at least one LLM.
   - CPU:
     - ASR **MUST NOT** monopolize all cores; Orchestrator **MUST** cap parallelism.

These thresholds are subject to revision but **MUST** be documented at each revision.

#### 6.2.8.4 Regression Tests and Continuous Evaluation

1. **Pre-release checks**
   - Any change to:
     - ASR models
     - ASR runtime
     - Segmentation or preprocessing
   - **MUST** trigger:
     - Full eval suite run on core datasets
     - Comparison against previous baseline

2. **Regression criteria**
   - A new version **MUST NOT** be promoted to default if:
     - WER increases beyond predefined tolerances
     - RTF or resource usage significantly degrades without compensating benefits

3. **Continuous monitoring (optional)**
   - For internal/dev builds:
     - The system **MAY** collect anonymised metrics on:
       - Job durations
       - Failure rates
       - Effective RTF in the field
   - These metrics:
     - **MUST** not include raw content unless explicitly opted in.

4. **Reporting**
   - Each major ASR update **SHOULD**:
     - Produce a short eval report (baseline vs new)
     - Be attached to the model card and internal release notes

### 6.2.9 X.9 Roadmap and Implementation Plan

This section defines a phased plan for delivering ASR in Handshake.

#### 6.2.9.1 MVP Scope (First Shippable)

The ASR MVP **MUST** deliver:

1. **Core capabilities**
   - Batch transcription for:
     - Locally stored audio/video files
     - Newly recorded content from within Handshake
   - Integration with:
     - Orchestrator and Model Runtime Layer (X.3.2)
     - Shadow Workspace (X.3.6, X.6.4)

2. **Model stack**
   - Primary Whisper large-v3 model on GPU (X.2.4).
   - Fast/low-resource Whisper small model and CPU fallback.

3. **UX**
   - Clear â€œTranscribeâ€ workflow.
   - Job progress indicators and completion status.
   - Transcript view with basic navigation and editing.

4. **Quality and performance**
   - Meet initial RTF and WER targets for English lectures/meetings (X.8.3).
   - Stable behavior on recordings up to 3 hours.

5. **Non-goals for MVP**
   - No streaming/live captions.
   - No fine-tuning pipeline.
   - Diarization optional; if present, basic only.
   - No cloud ASR by default.

#### 6.2.9.2 Phase 2 â€“ Multilingual, Diarization, Better UX

Phase 2 **SHOULD** focus on:

1. **Multilingual improvements**
   - Establish per-language eval subsets.
   - Add and test language-specialised engines (e.g. Mandarin ASR).
   - Improve language detection/hints in the pipeline.

2. **Diarization**
   - Integrate diarization toolchain.
   - Extend transcript schema with speaker IDs.
   - Add UI support for speaker labels and filtering.

3. **LLM tooling over transcripts**
   - Productionize:
     - Summaries
     - Action items
     - Q&A over transcripts
   - Tighten linking between transcripts, media, and derivative notes.

4. **Robustness and ergonomics**
   - Better error handling and recovery (resume partial jobs).
   - More flexible segmentation profiles (quality vs speed).

#### 6.2.9.3 Phase 3 â€“ Fine-Tuning, Streaming, Multi-Engine Consensus

Once MVP and Phase 2 are stable, Phase 3 **MAY** introduce:

1. **Fine-tuning (under X.5.2 gate)**
   - Build a small central training pipeline.
   - Fine-tune ASR for:
     - Specific domains (e.g. CS lectures)
     - Specific languages with enough data
   - Integrate results into the model lifecycle (X.5.4).

2. **Streaming / live captions (if desired)**
   - Implement session-based ASR APIs.
   - Integrate streaming-capable models.
   - Provide a distinct â€œlive captionâ€ UX.

3. **Multi-engine consensus and cloud fallback**
   - For critical content:
     - Explore multi-engine fusion (voting/ROVER-style).
     - Optionally combine local + cloud outputs (with user opt-in).
   - Measure cost/benefit vs single-engine setup.

#### 6.2.9.4 De-Risking Plan and Open Questions

1. **Early technical spikes**
   - Before full implementation:
     - Spike: Whisper large-v3 on reference hardware (latency, VRAM).
     - Spike: segmentation + VAD quality on representative audio.
     - Spike: transcript integration with Shadow Workspace and search.

2. **Open questions (examples)**
   - What is the minimum acceptable experience on CPU-only machines?
   - For which languages is default Whisper quality insufficient?
   - Is streaming a real user need for Handshake, or a distraction?

3. **Feedback loops**
   - Gather qualitative feedback from:
     - Early users
     - Internal use on real lectures/meetings
   - Use this feedback to:
     - Refine thresholds in X.8
     - Prioritize Phase 2 vs Phase 3 features

This roadmap is descriptive, not binding; it is intended to keep ASR development focused and de-risked while leaving room for iteration.
### 6.2.10 X.10 Appendices

#### 6.2.10.1 Original GPT-5.1 ASR Research (Verbatim)

Executive Summary
Leading ASR models: OpenAIâ€™s Whisper (MIT license) stands out: its Large model (1.5B params) achieves ~2â€“5% WER on clean English and supports 99 languages[1]. Whisper has five sizes (39Mâ€“1.5B)[2], allowing tradeoffs between accuracy and speed. Metaâ€™s Wav2Vec 2.0 (Apache-2.0) also performs strongly (~3â€“6% WER)[3] with base (95M) and large (317M) variants, and XLSR models covering 50+ languages. NVIDIAâ€™s NeMo provides Conformer/RNN-T models (hundreds of millions to billions of params) optimized for GPUs: e.g. the â€œCanaryâ€ model transcribes English, Spanish, German, and French with punctuation/translation[4], and inference optimizations report RTFÃ—2000â€“6000 on NVIDIA hardware[5]. Older toolkits like Kaldi/Vosk (HMM-DNN hybrids) and Coqui STT (DeepSpeech 2) offer many languages with lighter resource needs: Vosk models (20+ languages) are very small (50MBâ€“1.8GB) and CPU-real-time[6] (10â€“15% WER)[7], while Coqui STT (50â€“1000MB models) runs on CPU with streaming capability (6â€“10% WER)[8]. Research toolkits (SpeechBrain, ESPnet, PaddleSpeech) yield state-of-art accuracy in many languages and are Apache-licensed, though they require custom setup. Faster-Whisper (CTranslate2) and whisper.cpp (ggml C++) dramatically accelerate Whisper: Faster-Whisper can transcribe 13min in ~1:03 (versus 2:23 in OpenAIâ€™s code)[9], and whisper.cpp supports quantized inference on CPU[10][11].
Emerging models: In 2024â€“25, new entrants appeared. Metaâ€™s Omnilingual ASR (OpenAI/Vad system) offers models from 300M to 7B params covering ~1600 languages[12], with zero-shot generalization to unseen tongues. Shunya Labsâ€™ Pingala V1 (Whisper-architecture, ~1.5B) claims ~3.1% WER[13] and covers 200+ languages (with special strength in Indic languages); it provides ONNX â€œtinyâ€ variants for on-device use. Revâ€™s Reverb V1/Turbo (2024) promises near-human English accuracy and built-in diarization[14], but requires a paid license for production[15]. These are promising but hardware-intensive and (for Reverb) legally restrictive.
Deployment & latency: On GPUs, state-of-art models can run far faster than real-time[16]. For example, Faster-Whisper on an RTX 3070Ti processed 13â€¯min of audio in ~1:03 (fp16)[9], while quantized (int8) took ~0:59 using ~3â€¯GB memory[9]. By contrast, on a modern CPU the same audio needed ~6:58 with vanilla Whisper but ~2:05 with whisper.cpp[11]. In practice, heavy models (Whisper-medium/large, NeMo-big) demand GPUs with 5â€“10+â€¯GB VRAM[17]. Edge deployment relies on smaller or quantized models: Whisper-tiny/base or Vosk on CPU approach realtime, while large models must be downsized. We recommend a hybrid pipeline: run a lightweight model on-device for low-latency draft transcripts, and fallback to a full model on GPU for final accuracy.
Architecture: Align with Handshakeâ€™s unified orchestrator pattern[18]. Treat ASR as one of the â€œModel Runtimeâ€ services behind HTTP/gRPC. When an audio/video file is added, the orchestrator should (1) extract audio (using ffmpeg or an equivalent decoder; whisper.cpp supports many formats via FFmpeg[19]), (2) segment it (silence/VAD), then (3) invoke the ASR model on each segment. Transcripts become DerivedContent in the workspace (e.g. sidecar text files) and feed into the knowledge graph. This matches the specâ€™s model-calling design[18]. The same backend can also extract video frames (ffmpeg or OpenCV) for the canvas. In summary, use one process to coordinate: extract â†’ segment â†’ ASR â†’ integrate, rather than disjoint pipelines.
Gaps & mitigation: No single open ASR handles everything. Whisper and Omni cover many languages, but other languages may need specialized models. Punctuation is built-in for Whisper/NeMo, but e.g. Wav2Vec streams need post-processing. Speaker diarization is not natively handled by these ASR models; Revâ€™s Reverb includes it[14], or one must integrate tools like pyannote.audio. Timestamp granularity is coarse (Whisper ~2s segments). We suggest using WhisperX or similar for word-level alignment. For user experience, provide partial transcripts and timeline markers.
Implementation & integration: We recommend local deployment (no cloud by default). Package ASR engines as subprocesses or services. For example, run a Python service (Flask/FastAPI) hosting HuggingFace/NeMo models and have Rust call it via HTTP[18]. For Rust-native, one can bind whisper.cpp or ONNXRuntime (Rust crates) for CPU-only inference. Docker is optional (Handshakeless prefers host processes). WASM is feasible for whisper.cpp on CPU.
Risks: Key risks include hardware constraints (insufficient GPU/CPU power), licensing (MPL2.0 for Coqui requires sharing mods[20], Rev models need paid license[15]), and maintenance (some OSS ASR projects are niche). We address these by preferring permissive models (MIT/Apache) and offering fallbacks. Detailed risks and mitigations are discussed below.
Validation Results
We benchmarked representative models to confirm performance assumptions. On GPU, transcription is much faster than real-time[16]. In our tests on an RTX 3070Ti, Whisper-large-v2 (1.6B) took ~2:23 (OpenAI PyTorch) for 13â€¯min audio, while Faster-Whisper did it in ~1:03 (fp16)[9]. Using 8-bit quantization, faster-whisper finished in ~0:59, reducing VRAM from ~4.7â€¯GB to ~2.9â€¯GB[9]. Tomâ€™s Hardware similarly noted Whisper(Medium) easily exceeding real-time on GPU[16]. On CPU (Intel i7-12700K), results were slower: vanilla Whisper-base (FP32) needed ~6:58 for 13â€¯min, whereas whisper.cpp (FP32) took ~2:05 (â‰ˆ0.26Ã— real-time)[11]. Faster-Whisper (FP32) took ~2:37, and its int8 mode took ~1:42[11]. In short, optimized C++/quantized builds gave ~3â€“4Ã— speed-ups on CPU. For small models, we observed Whisper-tiny nearly real-time on high-end CPU (RTFâ‰ˆ0.9), Whisper-base ~3Ã— slower, and medium >>10Ã— slower.
We also validated file handling: ffmpeg easily extracts audio and keyframes. For example, ffmpeg -i video.mp4 -vn -acodec pcm_s16le audio.wav ran in under a second on a 1h video. Silence-based segmentation (using webrtcvad) broke audio into ~5â€“10s chunks, which ASR handled well. These checks confirm that system prerequisites (ffmpeg installation, model loading) work as expected and help guide our integration design.
Model Comparisons
Below is a summary of key open-source ASR models (with references):
OpenAI Whisper (MIT) â€“ Encoderâ€“decoder Transformer. 5 sizes (Tiny 39M, Base 74M, Small 244M, Medium 769M, Large 1.5B)[2]. WER ~2â€“5% on clean English[1], robust to accents/noise; supports 99 languages[21]. Includes built-in punctuation, capitalisation and timestamps. Runs on GPU much faster-than-real-time[16], but even Medium needs ~5â€¯GB VRAM[17]; Large needs ~10+â€¯GB. On CPU, only Tiny/Base approach realtime (whisper.cpp on CPU achieved 13â€¯min in ~2:05 vs 6:58 for PyTorch)[11]. Permissive MIT license[22]. Optimized implementations: Faster-Whisper (CTranslate2) yields ~3â€“4Ã— speed-ups with identical accuracy[23][9]; whisper.cpp (ggml) is a lightweight C++ port supporting 4-bit/8-bit quantization[10][11]. With quantization, a Large model can run on typical CPU (Faster-whisper int8 used ~3â€¯GB)[9].
Wav2Vec 2.0 (Meta, Apache-2.0) â€“ Self-supervised CNN+Transformer. Base (95M) / Large (317M) params[24]. Achieves ~3â€“6% WER on Librispeech benchmarks[3]. XLSR variants cover 50+ languages[25]. Good for multilingual fine-tuning. GPU recommended for large model; Base can run on CPU with latency. Apache-2.0.
Shunya Labs Pingala V1 (RAIL-M) â€“ Whisper-based 1.5B-param model[26]. Leader on Open ASR leaderboard (WER ~3.10%)[27]. Supports 200+ languages (including many Indic and code-switched cases)[13]. Universal and Verbatim variants. Available in ONNX (efficient) and quantized formats[26]. License is Responsible-Use (RAIL-M).
Meta Omnilingual ASR (Apache-2.0) â€“ New (2025) multilingual ASR system. Models from 300M to 7B params[12]. Trained on 4.3M+ hours across 1600 languages[12] (500+ previously unsupported). Uses an encoderâ€“decoder with LLM-style decoder for zero-shot. Claimed strong performance in low-resource languages[12]. 7B model likely needs high-end GPU; 300M model is for on-device inference. Fully open-source by Meta[12].
Kaldi (Apache-2.0) â€“ Traditional HMM-GMM/DNN toolkit. Mature with hundreds of recipes (languages, dialects). Accuracy is decent but below end-to-end (e.g. ~5â€“15% WER on English). Requires expert setup (feature extraction, graph models). Typically CPU-based (no heavy GPU needed). Many pretrained models exist. Vosk is a run-time friendly wrapper for Kaldi: offers dozens of small models (50â€“500MB) for 20+ languages (English, Spanish, German, Chinese, Russian, etc.)[6], optimized for CPU with low latency[28]. Vosk WER ~10â€“15%[7], not as high as neural models, but very stable on-device.
Coqui STT (Mozilla DeepSpeech, MPL-2.0) â€“ RNN-based (Conv + LSTM + CTC). Community models for ~10 languages[29]. WER ~6â€“10% on English benchmarks[8]. Supports streaming transcription and mobile (TensorFlow Lite). Model sizes range 50MBâ€“1GB[29]. Runs on CPU (also GPU). License allows free use but requires publishing modifications[20].
NVIDIA NeMo (Apache-2.0) â€“ Toolkit offering many ASR models. Example: Parakeet family (formerly Jasper/QuartzNet) and Conformer-CTC models. New Canary model (1.4B) does EN/ES/DE/FR with full punctuation and bidirectional translation[4]. NeMo models typically have hundreds of millions to billions of params. NVIDIA provides Riva (closed-source) for deployment, optimized via TensorRT. Inference-optimized NeMo models achieve extreme throughput (e.g. RTFÃ—2000â€“6000 on GPU)[5]. For local use, NeMo checkpoints can be exported to ONNX or TorchScript; requires NVIDIA GPU for best performance.
SpeechBrain / ESPnet â€“ End-to-end ASR toolkits (PyTorch). Active research communities. Provide recipes for myriad languages (e.g. LibriSpeech, CommonVoice, AISHELL, CHiME, etc.), using Conformer, Transformer, RNN architectures. No single model to cite; accuracy is comparable to state-of-the-art when properly trained (e.g. Librispeech WER ~2-3%). Suitable for custom training and experiments. Both are Apache/MIT licensed and interoperable with HuggingFace.
PaddleSpeech (Apache-2.0) â€“ Baiduâ€™s ASR/STT framework on PaddlePaddle. Includes Conformer, LAS, RNN-T models. Notably strong on Mandarin (AISHELL-1 WER ~2.0). Also supports bilingual/multilingual tasks (demonstrated in ST benchmarks[30]). Provides features like punctuation restoration, streaming APIs. Primarily uses GPUs; supports CPU inference via ONNX or PaddleLite.
WeNet (Apache-2.0) â€“ Chinese-led E2E ASR toolkit. Claims â€œProduction-readyâ€ status[31]. Accurate on public datasets (state-of-art results[31]). Out-of-box models: Paraformer, Firer9, WeNetSpeech for Chinese, and even includes Whisper-large for English[32]. Lightweight and easy to install. Supports CPU (via PyTorch) and GPU. Active development, used in industry (Alibaba).
Faster-Whisper / whisper.cpp â€“ These are inference engines for Whisper. Faster-Whisper (Python + CTranslate2) achieves ~3â€“4Ã— speed-up over OpenAIâ€™s code[23][9]. E.g. it transcribed 13â€¯min audio in ~1:03 vs 2:23[9]. whisper.cpp is a minimal C/C++ implementation of Whisper (portable and header-only). It uses fixed-point (quantized) GGML matrices to run on CPU. It supports 4-bit (and 8-bit) quantization[10], enabling even Whisper-large to run on laptops. In benchmarks, whisper.cpp (4 cores) did 13â€¯min in ~2:05[11]. These are ideal for on-device scenarios.
Other â€“ wav2letter++ (Facebook), Kaldi K2, wav2vec XLSR (Metaâ€™s multilingual ASR model), and smaller libraries (Silero, Coqui TTS) exist but cover narrower use-cases. We focus on the above due to ecosystem maturity and local-run feasibility.
Architecture Recommendations
We advocate following Handshakeâ€™s single-orchestrator design[18]. Treat ASR as a regular AI task invoked by the orchestrator (Python backend). A recommended pipeline: (1) Extraction: When the user adds an audio/video file, use ffmpeg (or whisper.cppâ€™s built-in decoder) to extract raw audio[19]. Also use ffmpeg or similar to grab key video frames (if needed for the canvas). (2) Segmentation: Split audio into manageable chunks (e.g. 5â€“10s) using silence detection or a voice activity detector. This ensures timely results and bounds memory. (3) Transcription: Invoke the chosen ASR model service on each chunk (via HTTP/gRPC per the spec[18]). Collect transcripts along with timestamps. (4) Integration: Save transcripts as DerivedContent (e.g. Markdown or JSON sidecars) in the workspace. The Shadow Workspace can then parse them into text nodes and index embeddings, RAG vectors, etc. For video, align transcripts with frame timestamps.
This follows a unified orchestrator rather than isolated â€œDoc-firstâ€ modules. All tasks run under one controller: the orchestrator should queue and schedule them (e.g. async tasks or a job queue) to utilize available resources and maintain order. Per the Handshake spec, the orchestrator uses a â€œModel Runtime Layerâ€ to call any AI model[18], so ASR is just another model runtime. There is no separate â€œASR pipelineâ€ outside this framework.
Tool validation: we confirmed that FFmpeg (or PyAV) easily handles common media. For example, compiling whisper.cpp with FFmpeg support allows decoding MP3, AAC, Opus, etc[19]. Keyframe extraction can use ffmpeg -vf select="eq(pict_type\,PICT_TYPE_I)" or libraries like OpenCV. These outputs (audio, frames) become new RawContent files in the workspace, triggering transcription and image analysis as needed. This integration fully aligns with Handshakeâ€™s CRDT + file-tree data model: transcripts become part of the document graph just like user-written text.
Deployment Options
Local inference (GPU vs CPU): Large ASR models benefit greatly from GPUs. For example, Whisper-Large requires ~5â€“10â€¯GB VRAM[17] to load; NeMo Parakeet/Conformer can need even more. In contrast, CPU-only use requires downsizing. Quantization and optimized runtimes help here. Converting a model to 8-bit (via ONNX or GGML) can cut memory by ~60%; e.g. Faster-Whisperâ€™s int8 reduced Whisper-large to ~2.9â€¯GB[9]. Whisper.cpp offers 4-bit quant, letting a 1.5B model run on ~3â€“4â€¯GB RAM[10]. Smaller models (Wav2Vec2-Base, Vosk small, Whisper-Tiny/Base) can run on CPU with some latency.
Model selection: For high accuracy pipelines, use full-size models on GPU (Whisper-medium/large, Wav2Vec2-large, NeMo Conformer). For low-latency mode, use stripped-down models (Whisper-tiny/base, Coqui small, Vosk, or quantized Whisper) on CPU or GPU. Hybrid setups (light preview on CPU, full on GPU) are recommended.
Packaging formats: Docker can encapsulate complex dependencies (e.g. Riva or PyTorch-GPU stacks), but Handshake prefers host processes. We suggest distributing ASR as: (a) Binary executables (e.g. whisper.cpp compiled standalone, or ONNXRuntime with static libs), (b) Python wheel or virtualenv including necessary libs (PyTorch, huggingface, etc.), or (c) WebAssembly bundles for whisper.cpp (targeting WASI/Rust) for extreme portability. For GPU use, either rely on the user having CUDA drivers, or package with container/conda that includes cuBLAS/cuDNN (similar to CUDA docker images).
Optimizations: Use model quantization (ONNX int8) and graph optimizers (TensorRT or OpenVINO) where possible. For instance, NVIDIAâ€™s own guide ported NeMo models to Riva/TensorRT for 10Ã— speed-ups[5]. For CPU, enable vector instructions (OpenBLAS/AVX) as whisper.cpp does. Batch inference (Faster-Whisper with batch_size>1) can also improve throughput on GPU[9].
Integration Paths
Given Handshakeâ€™s Rust/Tauri frontend, ASR engines can be integrated via several patterns:
Subprocess/Web Service: The orchestrator (Python) can spawn ASR engines as external processes and communicate via HTTP or gRPC (as per spec[18]). For example, one could run a local Flask/Starlette server exposing Whisper or Wav2Vec2 endpoints, and have the Rust side call it (or simply have the Python orchestrator call it internally). This is straightforward and aligns with the current Python orchestration model. Linux binaries or Python scripts (whisper.cpp CLI, Vosk API, Coqui CLI) can be invoked with subprocess and return results. HTTP/gRPC decouples failure domains and supports scaling to multiple ASR tasks concurrently.
Language Bindings/FFI: Alternatively, embed ASR libraries directly in Rust. For Whisper, whisper.cpp provides a C API, so Rust can call whisper_transcribe() via bindgen. ONNX Runtime has a Rust crate to load quantized ASR models (e.g. Wav2Vec2 or Whisper ONNX). This avoids Python dependency and improves safety, but requires building/packaging these libs for each platform. WASM is also possible: whisper.cpp can compile to WebAssembly (for CPU-only inferencing) which could be invoked from Rust/Tauri via WASI. Note: GPU inference currently requires native CUDA libs, so pure Rust GPU support is limited.
Hybrid: Use PyO3 or tokio-subprocess within Rust to run Python code. For example, the Rust orchestrator could call a Python function (via pyo3-ffi) that loads a HuggingFace pipeline. This is less common in a Tauri app but technically possible. Given the spec already uses a Python backend, the simplest path is to keep the Python orchestrator and let it manage ASR subprocesses (or library calls) internally, communicating results back to Rust via the existing HTTP API.
In summary, the practical approach is HTTP/gRPC to Python/C++ ASR modules (matching the specâ€™s Model Runtime Layer[18]). This minimizes new interop code. We package each ASR engine with its runtime: e.g. a Docker/Conda environment for NeMo, a static binary for whisper.cpp, etc. The Rust core need only hit an endpoint like POST /transcribe.
Risk Matrix
Risk
Mitigation / Comments
Hardware limitations
Large ASR models need GPUs (5â€“10+â€¯GB VRAM)[17]. Mitigation: Use quantized/smaller models (Whisper-tiny/Base, ONNX int8) on CPU. Check GPU availability at runtime and disable heavy models if absent.
Latency/Throughput
Long inputs (lectures) can cause high latency. Mitigation: Segment audio (silence detection); stream results incrementally. Provide UI progress. Use batch or GPU for throughput[9].
Diarization missing
Most ASR models donâ€™t label speakers. Mitigation: Integrate a diarization tool (e.g. pyannote) or use Revâ€™s Reverb (includes diarization)[14]. Clearly mark speaker changes manually if needed.
Punctuation & formatting
Some models output unpunctuated text. Mitigation: Use models with punctuation (Whisper/NeMo) or run a punctuation model over raw transcript. Otherwise, rely on Handshakeâ€™s grammar features for clarity.
Multilingual coverage
Required languages (Dutch, Korean, etc.) may have poorer ASR support. Mitigation: Default to multilingual models (Whisper, OmniASR[12]). Fall back to best available or cloud API for rare cases, if acceptable.
Accuracy variance
Domain-specific jargon or accents can degrade transcripts. Mitigation: Allow transcript editing by user (AI acts as collaborator). Possibly fine-tune models on in-domain data. Combine multiple ASR models for consensus.
Community/Support
Some projects (Coqui, ESPnet) may see slow updates. Mitigation: Favor robust communities (Whisper, HuggingFace, NVIDIA, Paddle) and monitor releases. Keep flexibility to swap models.
Licensing/Legal
Copyleft licenses (MPL-2.0 for Coqui) require sharing modifications[20]; Revâ€™s models need a commercial license[15]. Mitigation: Prefer MIT/Apache models (Whisper, W2V2, NeMo, Paddle). For MPL, avoid proprietary changes. For Rev, restrict to eval/â€œresearchâ€ or procure license.
Integration complexity
Multiple runtimes increase maintenance. Mitigation: Use unified orchestrator pattern[18]. Containerize or script installations. Automate model downloads. Thoroughly test data flow end-to-end.
Resource contention
Running ASR + other AI tasks concurrently can overload GPU/CPU. Mitigation: Schedule tasks (e.g. only one heavy model at a time). Use quantized models to reduce load. Provide user with settings to limit resource usage.
Data privacy
Local models mitigate this; risk is minimal. Mitigation: Continue with local-only by default; if any cloud use is added, ensure encryption and opt-in.
Implementation Roadmap
Prototype Basic ASR Pipeline: Start with Whisper-small or Wav2Vec2-Base in Python. Integrate audio extraction (ffmpeg) and segmentation. Verify transcripts appear in the Handshake workspace (as text files or notes). Use the orchestrator to call the model API.
Extend to Full Model: Add Whisper-large (or chosen high-accuracy model) as an alternate. Benchmark GPU vs CPU for target hardware. Implement model selection logic (e.g. â€œif GPU present, use large; else use small/quantizedâ€).
Language Expansion: Ensure Chinese support (Whisper already does Chinese transcription). If needed, test a dedicated Mandarin model (PaddleSpeechâ€™s AISHELL model) for accuracy. For other languages (Dutch, Korean, etc.), test Whisperâ€™s performance and consider community models (WeNet or HuggingFace XLSR). Integrate OmniASR once available for truly low-resource languages.
Diarization & Post-Processing: Integrate speaker diarization (e.g. pyannote or WhisperX) as a separate subprocess. Format transcripts with speaker labels. Add punctuation restoration if needed (NeMo has built-in, otherwise use a language model). Validate timestamp precision meets UX needs.
Performance Optimization: Add Faster-Whisper and whisper.cpp options. Test 8-bit quant models for memory reduction. Profile CPU inference and enable multithreading. Consider ONNX conversion for Wav2Vec2/others. For NVIDIA GPUs, explore using Riva or TensorRT (if distribution and licensing permit).
Packaging: Decide on binary formats. For Python models, use PyInstaller or Conda to bundle executables with all dependencies. For C++ ASR (whisper.cpp, Vosk), build static binaries. If supporting WASM, compile whisper.cpp to Wasm and integrate via WASI. Create scripts to download required model files (or package smaller â€œtinyâ€ versions to distribute).
Integration with Rust/Tauri: Expose ASR via the existing backend API. Ensure smooth data exchange (JSON, protobuf). Handle fallback (e.g. if GPU call fails, try CPU). Write tests for the orchestrator to simulate audio inputs.
User Experience: In the UI, allow importing audio/video and show progress. Stream partial transcripts as they arrive. Provide tools to correct or annotate transcripts. Link transcripts with video timeline or document highlights.
Security & Logging: Log all ASR steps to the flight recorder. Sandbox model code (WASI or containers) to enforce capability limits.
Testing & Benchmarks: Continuously test with lecture-length files. Monitor memory usage and RTF under various conditions. Adjust chunk sizes and concurrency accordingly.
Documentation & Maintenance: Document supported languages and models. Keep track of upstream updates (e.g. new Whisper, OmniASR). Plan for periodic re-evaluation as new models appear.
By following this roadmap and the above analysis, we can systematically integrate best-of-breed open-source ASR into Handshake while managing risk and providing a responsive user experience.
[1] [2] [3] [6] [7] [8] [13] [20] [21] [22] [24] [25] [26] [27] [28] [29] Benchmarking Top Open-Source Speech Recognition Models (2025)
https://www.shunyalabs.ai/blog/benchmarking-top-open-source-speech-recognition-models
[4] [5] GitHub - NVIDIA-NeMo/NeMo: A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)
https://github.com/NVIDIA-NeMo/NeMo
[9] [11] [23] GitHub - SYSTRAN/faster-whisper: Faster Whisper transcription with CTranslate2
https://github.com/SYSTRAN/faster-whisper
[10] Â whisper.cpp: High-Performance Speech to Text in C/C++ â€¢ reelikklemind
https://www.reelikklemind.com/posts/whispercpp-high-performance-speech-to-text-in-c-c/
[12] Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages
https://arxiv.org/html/2511.09690v1
[14] [15] Open-Source ASR & Diarization Models | Rev
https://www.rev.com/blog/open-source-asr-diarization-models
[16] [17] OpenAI Whisper Audio Transcription Benchmarked on 18 GPUs: Up to 3,000 WPM | Tom's Hardware
https://www.tomshardware.com/news/whisper-audio-transcription-gpus-benchmarked
[18] Project_Handshake_Unified_Spec_v2_0_MERGED.md
file://file-FLKEEiEWLdmRLZt7snAmGi
[19] GitHub - ggml-org/whisper.cpp: Port of OpenAI's Whisper model in C/C++
https://github.com/ggml-org/whisper.cpp
[30] Released Models â€” paddle speech 2.1 documentation
https://paddlespeech.readthedocs.io/en/latest/released_model.html
[31] [32] GitHub - wenet-e2e/wenet: Production First and Production Ready End-to-End Speech Recognition Toolkit
https://github.com/wenet-e2e/wenet

#### 6.2.10.2 Open-Source ASR Technology (Claude, Verbatim)

### 6.v02.12 Open-Source ASR Technology: A Senior Architect's Comprehensive Guide (2024-2025)

### 6.2.12 Executive Summary

**The open-source ASR landscape has reached an inflection point.** As of late 2025, **73% of production ASR deployments use open-source models**, up from 42% in 2022. The top-performing models on the Hugging Face Open ASR Leaderboard are now all open-source, with NVIDIA's Canary-Qwen-2.5B achieving **5.63% average WER**â€”matching or exceeding commercial alternatives for most use cases.

Three critical findings emerge from this research:

- **Speed-accuracy trade-offs have collapsed**: NVIDIA's Parakeet-TDT-0.6B-v2 achieves 6.05% WER at 3,380x real-time factorâ€”meaning models can now transcribe at production speeds without sacrificing quality
- **Multimodal integration is the new frontier**: Models like IBM Granite Speech 3.3 and Meta's SeamlessM4T combine ASR with LLM reasoning, translation, and document understanding
- **Edge deployment is production-ready**: Whisper.cpp and Sherpa-onnx enable real-time transcription on mobile devices and Raspberry Pi, with Vosk achieving 50MB deployment footprints

**Top recommendations for immediate adoption**: Whisper Large-v3-turbo for general transcription (8x faster than large-v2), Parakeet-TDT-0.6B-v2 for English-only high-throughput, and pyannote-audio 3.1 for speaker diarization.

---

### 6.2.13 Open-Source ASR Model Landscape

The ASR model ecosystem has stratified into three tiers: foundation models (Whisper, Wav2Vec2), production-optimized variants (NeMo, Faster-Whisper), and specialized toolkits (SpeechBrain, ESPnet). Understanding this hierarchy is essential for selecting the right model.

#### 6.2.13.1 Whisper remains the dominant baseline with unprecedented ecosystem depth

OpenAI's Whisper family continues to set the standard for multilingual ASR. Released September 2022 with 680,000 hours of training data, Whisper's encoder-decoder transformer architecture supports **100 languages** with native punctuation and translation capabilities. The October 2024 release of **Whisper Large-v3-turbo** reduced decoder layers from 32 to 4, achieving 8x speedup while maintaining near-equivalent accuracy.

| Model | Parameters | VRAM | LibriSpeech Clean WER | RTFx (GPU) |
|-------|------------|------|----------------------|------------|
| tiny | 39M | ~1 GB | ~8% | ~32x |
| base | 74M | ~1 GB | ~5% | ~16x |
| small | 244M | ~2 GB | ~3.5% | ~6x |
| large-v3 | 1.55B | ~10 GB | 2.8% | ~50x |
| **large-v3-turbo** | **809M** | **~6 GB** | **~3%** | **216x** |

**Faster-Whisper** (CTranslate2-based) delivers 4x speedup over vanilla Whisper with 45% memory reduction through quantization. **Whisper.cpp** enables CPU inference on edge devicesâ€”transcribing 11 seconds of audio in 0.37 seconds on an M2 Pro Mac.

#### 6.2.13.2 NVIDIA NeMo models now lead accuracy benchmarks

NVIDIA's NeMo ecosystem has emerged as the production leader for English ASR. The **FastConformer architecture** delivers 3x compute savings and 4x memory savings versus standard Conformers, while achieving state-of-the-art accuracy.

**Canary-Qwen-2.5B** (July 2025) currently holds #1 on the Hugging Face Open ASR Leaderboard with 5.63% average WER. This Speech-Augmented Language Model combines a FastConformer encoder with Qwen3-1.7B LLM decoder, enabling both transcription and downstream reasoning tasks. Trained on 234,000 hours of English speech, it achieves 1.6% WER on LibriSpeech clean.

**Parakeet-TDT-0.6B-v2** offers the best speed-accuracy trade-off: 6.05% WER at 3,380 RTFxâ€”meaning it processes audio 3,380x faster than real-time. The v3 release (August 2025) extended language support to 25 European languages with automatic detection.

#### 6.2.13.3 Meta's MMS provides unmatched language coverage for low-resource applications

Massively Multilingual Speech (MMS) covers **1,107 languages**â€”10-40x more than any competitor. By using language-specific adapters (~2M parameters each), MMS achieves efficient language switching while maintaining a single base model. While English accuracy lags specialized models (ranking 52nd on the Open ASR Leaderboard for English), MMS halves Whisper's WER across 54 FLEURS languages and enables ASR for languages with as few as 100 speakers.

#### 6.2.13.4 Emerging models to watch: Kyutai and IBM Granite Speech

**Kyutai STT** (late 2024) introduces Delayed Streams Modeling for ultra-low latency streaming. The 2.6B model achieves 6.4% WER with 0.5-2.5 second latency, supporting 400 concurrent real-time streams on a single H100. The Mimi tokenizer enables efficient audio representation at 12.5 Hz.

**IBM Granite Speech 3.3** (8B parameters) represents the Speech-Language Model approachâ€”a two-pass design combining a Conformer encoder with Granite LLM for chain-of-thought reasoning. It preserves LLM text capabilities, enabling RAG applications directly on transcribed content.

#### 6.2.13.5 Legacy status clarification

**Mozilla DeepSpeech**: Formally archived June 2025. Not recommended for new projectsâ€”use Coqui STT or Whisper instead.

**Kaldi**: The original toolkit remains foundational for research but is being superseded by Next-Gen Kaldi (K2/Icefall/Lhotse) for production deployments.

**Vosk**: Actively maintained with 20+ languages in 50MB-2GB models. Optimal for offline/embedded applications requiring minimal footprint.

---

### 6.2.14 Deployment Patterns and Infrastructure

Deployment strategy fundamentally determines cost, latency, and scalability. The right architecture depends on workload volume, latency requirements, and privacy constraints.

#### 6.2.14.1 Edge deployment has achieved production maturity

Real-time transcription on mobile devices is now achievable with optimized models. **Whisper.cpp** achieves real-time performance on iPhone 13+ through Core ML and Apple Neural Engine integration (3x faster than CPU). **Sherpa-onnx** supports 12 programming languages across Android, iOS, HarmonyOS, and Raspberry Piâ€”production deployments include smart glasses and hearing aids.

| Device | Whisper tiny | Whisper base | Whisper small | Vosk |
|--------|--------------|--------------|---------------|------|
| iPhone 13+ | âœ… Real-time | âœ… Real-time | âœ… Usable | âœ… |
| Pixel 7+ | âœ… Real-time | âœ… Real-time | âœ… Usable | âœ… |
| Raspberry Pi 4 | âœ… | âœ… | âš ï¸ Slow | âœ… |
| M1/M2 Mac | âœ… Real-time | âœ… Real-time | âœ… Real-time | âœ… |

**Quantization trade-offs**: INT8 quantization reduces memory by 75% with only ~1% WER increase. Dynamic INT8 with Quanto achieves 57% size reduction while maintaining baseline accuracyâ€”the optimal production choice.

#### 6.2.14.2 Cloud GPU pricing favors GCP L4 and AWS g5 instances

GPU instance selection significantly impacts cost-per-transcription-hour. Based on 2024-2025 pricing:

| Provider | Instance | GPU | VRAM | On-Demand/hr | Best For |
|----------|----------|-----|------|--------------|----------|
| AWS | g4dn.xlarge | T4 | 16GB | $0.53 | Budget inference |
| AWS | g5.xlarge | A10G | 24GB | $1.01 | Production ASR |
| GCP | g2-standard-4 | L4 | 24GB | ~$0.70 | **Best price/performance** |
| Azure | NCasT4_v3 | T4 | 16GB | ~$0.53 | Azure ecosystem |

**Critical finding**: GCP's L4 instances deliver 4x the performance of T4 at only 30% higher costâ€”the clear winner for new deployments.

**Cost analysis for 1,000 hours/month transcription**:
- AWS g5.xlarge Spot (auto-scaling): ~$180-300/month
- GCP g2-standard-4 + L4: ~$250-350/month
- CPU-only (c5.4xlarge with INT8 Whisper): ~$500/month (slower but no GPU)

#### 6.2.14.3 Serverless has GPU limitations that constrain ASR workloads

AWS Lambda lacks GPU support entirely, limiting ASR to CPU inference with 15-minute maximum timeout. Container-backed Lambda enables ~40MB Whisper models but remains impractical for large-scale transcription. **SageMaker async endpoints** or **AWS Batch with Spot instances** provide better alternatives for serverless-like simplicity with GPU acceleration.

GCP Cloud Run remains CPU-only as of late 2025. Azure Container Instances offers limited GPU support but lacks the scaling characteristics needed for production ASR.

#### 6.2.14.4 Kubernetes GPU deployment requires specific components

Production Kubernetes ASR deployments need:
1. **NVIDIA GPU Operator** or Device Plugin for GPU scheduling
2. **DCGM Exporter** for GPU utilization metrics
3. **Prometheus Adapter** for custom metrics HPA
4. **KEDA** (optional) for scale-to-zero capability

Recommended HPA configuration scales on GPU utilization (target 70%) and queue depth (target 5 pending jobs), with a 300-second stabilization window to prevent thrashing.

---

### 6.2.15 Feature Analysis and Gap Assessment

Understanding feature parity between modelsâ€”and the persistent gap versus commercial solutionsâ€”informs realistic deployment expectations.

#### 6.2.15.1 Feature comparison reveals clear specialization patterns

| Feature | Whisper | Wav2Vec2 | NeMo | Vosk | SpeechBrain |
|---------|---------|----------|------|------|-------------|
| Speaker diarization | âš ï¸ Via pyannote | âŒ | âœ… Built-in | âŒ | âœ… Built-in |
| Auto punctuation | âœ… Native | âŒ | âœ… | âŒ | âš ï¸ Separate |
| Word timestamps | âœ… | âœ… | âœ… | âœ… | âœ… |
| Custom vocabulary | âš ï¸ Fine-tuning | âš ï¸ Fine-tuning | âœ… Hot words | âœ… | âš ï¸ |
| Language detection | âœ… | âŒ | âš ï¸ Limited | âš ï¸ | âš ï¸ |
| Real-time streaming | âš ï¸ Via variants | âœ… | âœ… | âœ… Native | âœ… |

**Key differentiators**: Whisper leads in multilingual accuracy and native punctuation but requires GPU for real-time. NeMo provides the most complete production feature set. Vosk excels at lightweight offline deployment.

#### 6.2.15.2 The commercial gap persists in production features, not accuracy

Open-source ASR now matches commercial accuracy for English batch transcription. The gap has shifted to enterprise-ready features:

| Capability | Open-Source Status | Commercial Advantage |
|------------|-------------------|---------------------|
| Custom model training | Complex, requires ML expertise | AWS/Azure: drag-and-drop fine-tuning |
| Domain models (medical/legal) | Limited availability | Nuance Dragon Medical, AWS Transcribe Medical |
| PII redaction | Manual implementation | Built-in with AWS Transcribe Call Analytics |
| Hallucination control | Whisper prone to fabrication | Gladia claims "99.9% hallucination removal" |
| Summarization/action items | Requires external LLM | AssemblyAI: built-in AI summarization |
| SLAs and support | Community forums | 24/7 enterprise support, guaranteed uptime |

**Critical gap**: Medical transcription with EHR integration remains commercial-only (Nuance DAX, Microsoft DAX Copilot). Healthcare deployments requiring HIPAA certification need BAA-signed commercial vendors.

---

### 6.2.16 Comprehensive Use Case Catalog

ASR applications span virtually every industry. Implementation complexity varies from simple API integration to specialized systems requiring domain expertise.

#### 6.2.16.1 Healthcare demands highest accuracy with strictest compliance

Clinical documentation represents the highest-stakes ASR application. **Nuance Dragon Medical One** remains the standard, with 70% of US healthcare providers using speech recognition in EHR systems. Key findings:

- **Accuracy requirement**: 95%+ for medical terminology
- **Productivity impact**: 30-40% faster chart completion, up to 15 hours/month saved per clinician
- **Compliance**: HIPAA BAA required; December 2024 HHS Security Rule updates mandate encryption, vulnerability scanning every 6 months
- **Recommended stack**: AWS Transcribe Medical + AWS HealthScribe, or Microsoft DAX Copilot for ambient clinical documentation

**Open-source alternative**: Fine-tuned Whisper with medical vocabulary, deployed in HIPAA-compliant infrastructure with BAA-signed cloud provider. Achievable but requires significant implementation investment.

#### 6.2.16.2 Legal transcription still requires human verification

Court reporting demands 99%+ accuracyâ€”a threshold no ASR system consistently achieves. Current AI transcription achieves approximately 62% accuracy on legal proceedings versus 99%+ for certified court reporters (225+ WPM at >95% accuracy for NCRA certification).

**Recommendation**: Human transcription services (Ditto, Verbalscripts, GMR) for court-admissible documents. AI-assisted preprocessing for discovery and document review where 98-99% accuracy suffices.

#### 6.2.16.3 Meeting transcription represents the highest-volume opportunity

The AI meeting assistant market is expanding rapidly, with platforms like Otter.ai reporting up to 95% accuracy. Platform-native AI features have matured significantly:

| Platform | Stability | AI Performance | Latency |
|----------|-----------|----------------|---------|
| Zoom AI Companion | 96% | Leading | Sub-second |
| Microsoft Teams Copilot | 89% | Good summary quality | ~1 second |
| Cisco Webex | 84% | Improving | ~1 second |

**Recommended stack**: Whisper Large-v3-turbo + pyannote-audio 3.1 for speaker diarization + LLM for summarization and action item extraction. Achieves 90-95% accuracy with full customization control.

#### 6.2.16.4 Accessibility compliance requires human review

WCAG compliance for closed captioning requires 99%+ accuracyâ€”auto-generated captions at 60-70% accuracy **do not meet accessibility standards**. Level AA compliance (1.2.4) mandates live captions for real-time content.

**Validation status**: Pattern of using AI-generated captions for accessibility = **DEPRECATED**. Human review or professional CART services remain mandatory for legal compliance.

---

### 6.2.17 Technical Implementation Guide

Avoiding common pitfalls requires understanding audio preprocessing, model loading patterns, and error handling strategies that have been validated in production deployments.

#### 6.2.17.1 Audio preprocessing fundamentals remain consistent

**Validated best practices**:
- **Sample rate**: 16kHz standard (matches most model training data)
- **Format**: WAV/PCM, linear signed 16-bit
- **Normalization**: Zero mean, unit variance (Wav2Vec2 approach)
- **Duration**: Segments under 30 seconds; 10-15 seconds optimal for RNN decoders
- **VAD**: Voice Activity Detection for chunking long audio and reducing silence

**Anti-pattern to avoid**: Sample rate mismatch causes "chipmunk voices" or recognition failures. Always resample to match model's expected rate before inference.

#### 6.2.17.2 Confidence thresholds enable quality gating

Implementing confidence-based routing significantly improves production quality:
- **High confidence (>0.9)**: Accept transcript directly
- **Medium (0.7-0.9)**: Flag for review or request clarification
- **Low (<0.7)**: Reject and request re-recording

#### 6.2.17.3 Error handling with exponential backoff is essential

```python
```
#  Validated pattern for ASR service reliability
@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=1, max=8),
    retry=retry_if_exception_type((RateLimitError, ServerError))
)
def transcribe_with_retry(audio_file):
    return asr_service.transcribe(audio_file)
```

Add jitter to prevent thundering herd, log all retries with request IDs, and implement circuit breakers for cascading failure prevention.

#### 6.2.17.4 Security requirements are non-negotiable

**Mandatory practices**:
- AES-256 encryption at rest, TLS 1.2+ in transit
- Role-based access controls with least privilege
- Never log raw audio data or full transcripts in plain text
- Implement automatic data deletion schedules (GDPR: 30 days max retention recommended)
- Background checks for personnel with data access

**Critical warning**: Consumer tools (Siri, Google Voice, Alexa) are NOT HIPAA compliantâ€”they don't sign BAAs. Never use for PHI.

---

### 6.2.18 Known Limitations and Technical Difficulties

Understanding ASR limitations enables realistic expectation-setting and appropriate mitigation strategies.

#### 6.2.18.1 Overlapping speech remains the hardest unsolved problem

The "cocktail party problem" causes WER to increase from <5% to 80%+ when multiple speakers talk simultaneously. ConVoiFilter research achieved breakthrough reduction from 80% to 26.4% WER using speech separation, but real-world performance still degrades significantly with overlap.

**Mitigation**: Implement pyannote-audio 3.1 for speaker turn detection, use directional microphones where possible, and design conversation flows that minimize simultaneous speech.

#### 6.2.18.2 Accent and dialect bias creates demographic accuracy disparities

Stanford research documented ~2x error rates for African American speakers compared to standard American English. UK and Indian accents show 10-15% WER gaps. This isn't a bug but a training data imbalanceâ€”models see more standard American English than any other variety.

**Mitigation**: Test across demographic groups before deployment, fine-tune on accent-specific data for critical applications, and provide alternative input methods for users experiencing consistently poor recognition.

#### 6.2.18.3 Hallucination is a Whisper-specific risk requiring vigilance

Whisper can generate text not present in audioâ€”particularly during silence, noise, or very quiet speech. Unlike transcription errors, hallucinations are fabricated content that may be plausible-sounding but entirely false.

**Mitigation**: Implement post-processing checks for repeated n-grams (hallucination marker), validate output length against audio duration, and consider commercial alternatives (Gladia claims "99.9% hallucination removal") for high-stakes applications.

#### 6.2.18.4 Fine-tuning requires substantial resources and expertise

Whisper Large fine-tuning requires approximately **100 GPU-hours on A100 (40GB)** for 5-10 training runs. Minimum 10-50 hours of domain-specific audio recommended for meaningful improvement.

**Efficient alternative**: LoRA (Low-Rank Adaptation) achieves 24x fewer GPU hours by reducing trainable parameters by 10,000x. Use LoRA for domain adaptation when full fine-tuning is prohibitive.

---

### 6.2.19 Legal and Ethical Compliance Framework

Deploying ASR systems requires navigating complex regulatory requirements that vary by jurisdiction, industry, and use case.

#### 6.2.19.1 Recording consent laws vary dramatically by jurisdiction

**Two-party (all-party) consent states (11 US states)**: California, Delaware, Florida, Illinois, Maryland, Massachusetts, Montana, Nevada, New Hampshire, Pennsylvania, Washington. All parties must consent before recordingâ€”violations can result in criminal penalties up to 5 years imprisonment.

**One-party consent (39 states + DC)**: Recording permitted if one participant (including the recorder) consents.

**International**: UK, Germany, Canada, and Australia generally require all-party consent. EU consent requirements are context-dependent under GDPR.

**Critical rule**: For cross-border calls, apply the stricter standard.

#### 6.2.19.2 GDPR classifies voice data as biometric under certain conditions

Voice recordings are personal data under GDPR. **Voiceprints used for identification** qualify as biometric data (Article 9 special category), requiring explicit consent. EDPB Guidelines 02/2021 mandate voice-based interfaces for mandatory privacy information and strictly limit human review to "strictly necessary pseudonymized data."

#### 6.2.19.3 BIPA creates significant litigation exposure for voice applications

Illinois BIPA explicitly includes voiceprints as "biometric identifiers." Requirements include:
- Written consent before collection
- Published retention/destruction policy
- Prohibition on selling or profiting from biometric data

**Penalties (August 2024 amendment)**: $1,000 per negligent violation, $5,000 per intentional violation. The amendment limits damages to one violation per person (previously unlimited per-scan damages created massive class action exposure).

#### 6.2.19.4 Model licensing analysis

| Model | License | Commercial Use | Key Consideration |
|-------|---------|----------------|-------------------|
| OpenAI Whisper | MIT | âœ… Yes | Include license notice |
| NeMo models | Apache 2.0 / CC-BY-4.0 | âœ… Yes | CC-BY-4.0 increasingly common |
| Meta MMS | CC-BY-NC 4.0 | âš ï¸ Non-commercial only | Academic/research use |
| SpeechBrain | Apache 2.0 | âœ… Yes | No copyleft restrictions |

---

### 6.2.20 Integration Patterns and Framework Guidance

Framework selection and integration architecture significantly impact development velocity and production reliability.

#### 6.2.20.1 FastAPI with lifespan context manager is the validated 2024-2025 pattern

**Confirmed best practice**: The pattern of sync endpoints for small files, async job queues for large files, and WebSocket for streaming remains correct and recommended.

**Model loading**: Use lifespan context manager (not deprecated @app.on_event("startup")):

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

ml_models = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    ml_models["whisper"] = load_whisper_model("large-v3")
    yield
    ml_models.clear()

app = FastAPI(lifespan=lifespan)
```

**Concurrency for sync ASR libraries**: Use `run_in_executor` with ThreadPoolExecutor (4 workers typically optimal for GPU-bound inference).

#### 6.2.20.2 Node.js should call Python via gRPC, not native inference

While `whisper-onnx-speech-to-text` npm package exists, production deployments should use **gRPC to a Python service** running faster-whisper. This approach provides:
- Better accuracy (full whisper capabilities vs. limited ONNX export)
- GPU acceleration
- Mature error handling and retry patterns

Python subprocess calls are deprecatedâ€”too slow and resource-intensive for production.

### whisper-rs (Rust) is production-ready

The Rust bindings (v0.15.1) provide near-native whisper.cpp performance with CUDA support via feature flag. Production deployments should use `tokio::spawn_blocking` for sync whisper calls within async web frameworks (Actix, Axum).

#### 6.2.20.3 Go bindings have acceptable overhead

Official whisper.cpp Go bindings at `github.com/ggerganov/whisper.cpp/bindings/go` require CGO and achieve ~70% of native performance. For Go services, gRPC to a dedicated ASR service is often preferable to embedded inference.

---

### 6.2.21 Tool Combinations and Multimodal Integration

Modern ASR deployments increasingly combine transcription with downstream processing for search, analysis, and document understanding.

#### 6.2.21.1 RAG over audio transcripts follows established LangChain patterns

**Validated architecture**:
1. **Transcription**: AssemblyAI or Whisper with word timestamps
2. **Chunking**: RecursiveCharacterTextSplitter (1000 chars, 200 overlap) or speaker-aware segmentation
3. **Embedding**: sentence-transformers/all-mpnet-base-v2
4. **Vector store**: Chroma for prototyping, Pinecone/Weaviate for production

**Metadata preservation**: Include start_time, end_time, speaker_id, and source_file_id for filtering and attribution.

#### 6.2.21.2 Speaker diarization accuracy depends heavily on conditions

pyannote-audio 3.1 (14.2M HuggingFace downloads) achieves 10-15% Diarization Error Rate on standard benchmarks but degrades significantly in challenging conditions:

| Condition | Expected Accuracy |
|-----------|-------------------|
| 2 speakers, clean audio | 90-95% |
| 3-5 speakers | 80-90% |
| Overlapping speech | 70-80% |
| Noisy environments | 60-75% |

**Integration pattern**: Run pyannote first for speaker segments, then align Whisper word timestamps to speaker turns.

#### 6.2.21.3 IBM Docling enables unified multimodal document processing

Docling supports PDF, DOCX, PPTX, HTML, **WAV, MP3**, and images with built-in ASR. However, it does **not** directly extract audio from videoâ€”preprocessing with ffmpeg is required.

**Optimal architecture for lecture processing**:
```
Video â†’ ffmpeg (audio + keyframes) â†’ Docling ASR + OCR â†’ 
Timestamp alignment â†’ DoclingDocument unified schema
```

**Real-world accuracy for slide-transcript alignment**: 80-90%, requiring scene detection threshold tuning for optimal keyframe extraction.

#### 6.2.21.4 Meta SeamlessM4T represents the future of multilingual speech

SeamlessM4T provides single-model support for 101 input languages, 96 text output languages, and 36 speech output languages. Key advantages:
- 20% BLEU improvement over prior SOTA for direct speech-to-text translation
- ~2 second latency with SeamlessStreaming
- SeamlessExpressive preserves prosody, speech rate, and emotional tone

**Validation status**: SeamlessM4T = **ADOPT** for multilingual applications. Published in Nature 2024 with extensive validation.

---

### 6.2.22 Cloud Provider Deployment Matrix

#### 6.2.22.1 AWS remains strongest for integrated ML workflows

**Recommended configuration**: g5.xlarge ($1.01/hr on-demand, ~$0.25/hr spot) provides 24GB A10G VRAMâ€”sufficient for Whisper Large-v3 with room for batching.

**SageMaker deployment validated patterns**:
- Real-time endpoints for low-latency (<1s)
- Async endpoints for large files (S3-based I/O)
- Batch Transform for bulk processing at reduced cost
- JumpStart provides pre-built Whisper deployment in console

**Lambda limitations confirmed**: 15-minute timeout, no GPU, 10GB memory maximum. Suitable only for very short audio with small models.

#### 6.2.22.2 GCP L4 instances offer best price/performance

The L4 GPU delivers 4x T4 performance at only 30% higher costâ€”the clear winner for cost-sensitive deployments. Cloud Run remains CPU-only, limiting its ASR applicability.

**GKE Autopilot** supports GPU node pools with automatic provisioningâ€”recommended for Kubernetes deployments with variable load.

#### 6.2.22.3 Azure GPU options are transitioning

NC and NC_Promo series retired September 2023. **NCasT4_v3** is the current budget option; **NCads_H100_v5** (2024) provides enterprise performance but requires sales contact for pricing.

Azure Container Instances offers limited GPU support suitable for development/testing but lacks production scaling characteristics.

---

### 6.2.23 Future Outlook and Adoption Recommendations

#### 6.2.23.1 Efficiency improvements enable immediate cost reduction

**Adopt now**:
- **Distil-Whisper**: 51% fewer parameters, 5.8x faster, within 1-2% WER of original
- **Speculative decoding**: 2x inference speedup with mathematically identical output
- **Whisper Large-v3-turbo**: 8x faster than large-v2, production-ready

#### 6.2.23.2 LLM integration represents the next accuracy frontier

The **HyPoradise benchmark** demonstrates that LLMs can correct ASR errorsâ€”including tokens missing from N-best listsâ€”achieving results that surpass traditional re-ranking. RobustHP extends this to noisy conditions with 53.9% WER reduction.

**Pilot in 2025-2026**: LLM error correction for high-stakes applications where 1-2% WER improvement justifies additional compute.

#### 6.2.23.3 State-space models may displace transformers by 2027

**Samba-ASR** and related Mamba-architecture models demonstrate better efficiency than transformers with competitive accuracy. Monitor this space for potential architectural shift.

#### 6.2.23.4 Predictions for 2027

| Metric | 2024 Baseline | 2027 Prediction | Confidence |
|--------|---------------|-----------------|------------|
| English WER (clean) | 2-3% | <1.5% | HIGH |
| On-device RTF | 0.2-0.5 | <0.1 | HIGH |
| Streaming latency | ~2s | <500ms | MEDIUM-HIGH |

**Commercial viability trajectory**: Open-source achieves parity with commercial for English/major languages in 2025. By 2027, fully on-device multimodal assistants will be competitive with cloud services for most applications.

---

### 6.2.24 Validation Summary

| Pattern | Status | Notes |
|---------|--------|-------|
| FastAPI lifespan for model loading | âœ… VALIDATED | Replaces deprecated startup events |
| Sync/async/WebSocket pattern | âœ… VALIDATED | Remains best practice |
| Whisper for general transcription | âœ… VALIDATED | Large-v3-turbo recommended |
| Auto-captions for accessibility | âŒ DEPRECATED | Human review required for WCAG |
| Lambda for ASR | âš ï¸ LIMITED | CPU-only, 15-min max, small files only |
| DeepSpeech for new projects | âŒ DEPRECATED | Archived June 2025 |
| GCP L4 for production | âœ… RECOMMENDED | Best price/performance ratio |
| SeamlessM4T for multilingual | âœ… VALIDATED | Nature 2024 publication |
| pyannote-audio 3.1 for diarization | âœ… VALIDATED | 14.2M downloads, MIT license |

---

### 6.2.25 Risk Assessment

#### 6.2.25.1 Technical Risks
- **Hallucination in Whisper**: Medium risk, mitigate with post-processing validation
- **Overlapping speech degradation**: High risk for meeting transcription, mitigate with speaker turn design
- **Model version compatibility**: Low risk with containerization and dependency pinning

#### 6.2.25.2 Legal Risks
- **BIPA class actions**: High risk in Illinois, mitigate with explicit consent and policy publication
- **HIPAA violations**: High risk without BAA-signed infrastructure, average breach cost $10.1M
- **GDPR voice biometric classification**: Medium risk, requires explicit consent for identification use

#### 6.2.25.3 Vendor Risks
- **OpenAI model changes**: Low risk, MIT license ensures continued access to current weights
- **NVIDIA pricing/licensing**: Medium risk, CC-BY-4.0 license provides protection
- **Cloud provider lock-in**: Medium risk, mitigate with containerization and multi-cloud architecture

---

*This report synthesizes research current through December 2025. The ASR landscape evolves rapidlyâ€”validate specific version numbers, pricing, and feature availability before production deployment.*

#### 6.2.25.4 A Hobbyist's Guide to Building a Custom ASR (Verbatim)

A Hobbyist's Guide to Building a Custom ASR Service with OpenAI's Whisper

Introduction: The "High ROI" Starting Point

Building a custom Automatic Speech Recognition (ASR) service from scratch was once a monumental task, reserved for large research teams with deep pockets. However, the release of powerful foundational models like OpenAI's Whisper has fundamentally changed the game, making this technology more accessible than ever before.

To understand this shift, think in terms of Return on Investment (ROI). The goal is to get the best possible performance for the least amount of time and money. Imagine two paths to creating a market-ready ASR product:

* The Blue Curve (Starting from Scratch): This is the old way. It represents a long, slow, and expensive process of hiring experts, collecting massive amounts of data, building infrastructure, and training a model from the ground up. It's a steep climb just to reach the "good to go line" where your product is viable.
* The Red Curve (Starting with Whisper): This is the modern, high-ROI approach. Whisper provides a "fairly good starting point" that gets you remarkably close to the finish line right away. The model has already absorbed a massive investment from OpenAI, and you get to benefit from it. Your job is no longer to build the entire system, but to intelligently customize this powerful foundation.

This guide will walk you through the essential components and considerations for fine-tuning Whisper, transforming it from a general-purpose tool into a specialized service tailored to your specific needs.
--------------------------------------------------------------------------------
1. Understanding the Foundation: Whisper's Core Capabilities

Before you can customize Whisper, you need a clear picture of what it is, what it excels at, and where its out-of-the-box performance might not be enough for your project.

1.1. What Makes Whisper a Game-Changer?

Whisper's power comes from a unique combination of factors that make it an ideal starting point for developers and hobbyists alike.

* Massive Supervised Dataset: It was trained on an unprecedented scaleâ€”initially hundreds of thousands, and later millions, of hours of audio, giving it a broad understanding of human speech.
* Permissive MIT License: OpenAI released Whisper with a very generous license that allows for 100% free commercial use, removing a significant barrier for entrepreneurs and builders.
* Broad Language Support: The model supports 99 different languages, making it a versatile foundation for global applications.
* Evolving Open-Source Ecosystem: Because of its open nature, a vibrant community has built a rich ecosystem of tools around Whisper for fine-tuning, model compression, and other downstream tasks.

1.2. Where Does the Foundation Crack? Whisper's Limitations

Despite its strengths, the base Whisper model is a generalist. It can struggle when faced with specialized tasks. The table below outlines common scenarios where you'll likely need to customize the model.

Limitation	Impact on Your Project
Low-Resource Languages	For languages with less training data (e.g., Vietnamese, Indonesian), the model's accuracy is good but not great, making it less reliable for production use.
Specific Dialects	It may struggle with regional accents and dialects, such as Singaporean English, leading to lower accuracy for users in those specific communities.
Domain-Specific Terms	The model often fails to recognize specialized jargon in fields like medicine, law, or finance, making the output unusable for professional transcription.
New Named Entities	It cannot reliably recognize new brand names, public figures, or events (like "Wembanyama") that emerged after its training was completed.
Long Audio & Streaming	The base model is designed for offline processing. Building real-time applications like voice assistants or live captioning requires a completely different, session-based architecture that the base model doesn't support, and simply chunking audio introduces its own significant errors.

Transition: Now that we understand what Whisper can and cannot do out-of-the-box, let's explore the process of customizing it to fill those gaps.
--------------------------------------------------------------------------------
2. The Customization Blueprint: How to Fine-Tune Whisper

Fine-tuning is the process of taking the pre-trained Whisper model and further training it on your own specialized data. This teaches the model the nuances of your specific domain. A successful fine-tuning project requires three key ingredients: data, compute, and algorithms.

2.1. Ingredient 1: Data - The Key to Differentiation

Data is the single most important factor that will make your custom model effective. While algorithms and compute are relatively standardized, your unique dataset is what will distinguish your service from others. There are three primary approaches to acquiring data. Ultimately, the quality of any datasetâ€”whether from a vendor or your own effortsâ€”isn't about the claimed label accuracy, but its proven efficacy. The real test is how much it improves your model's performance on a benchmark after fine-tuning.

Data Approach	Description	Pros & Cons
Traditional Vendors	Hiring a company to provide pre-recorded audio files or to record new ones based on scripts.	Pros:<br>- Data is professionally recorded and labeled.<br><br>Cons:<br>- Very expensive and time-consuming.<br>- Often results in "read speech," which sounds different from natural, improvised conversation.
Your Own Production Data	Using audio that your product or service has already collected (e.g., customer service calls).	Pros:<br>- Highly domain-specific and directly relevant to your use case.<br><br>Cons:<br>- Often small-scale.<br>- Requires costly, time-consuming human labeling to create transcripts.
Large-Scale, Weakly-Labeled Data	A modern approach that involves gathering a massive amount of domain-specific audio and using automated methods to create "good enough" (weakly-labeled) transcripts.	Pros:<br>- Can be more cost-effective than human labeling for achieving higher ASR accuracy.<br>- The sheer volume of domain-specific data often outweighs the need for perfect labels.<br><br>Cons:<br>- Requires sophisticated data cleaning and verification pipelines to be effective; raw, un-curated data can introduce significant noise into the model.

2.2. Ingredient 2: Computational Power - Your Training Environment

Fine-tuning large models like Whisper requires access to powerful Graphics Processing Units (GPUs). Here are the three main ways to get the necessary computational power:

1. Automatic Fine-Tuning Services (e.g., OpenAI, Microsoft) This is the simplest option. You use a platform that handles the entire fine-tuning process for you. The trade-off is that it's often the least flexible, giving you less control over the training parameters.
2. GPU Cloud Providers (e.g., AWS, GCP) This is a balanced approach where you rent GPU time from a major cloud provider. It offers much more control and flexibility than automated services without the headache of managing physical hardware.
3. DIY GPU Cluster This involves building and maintaining your own data center with a cluster of GPUs. It is the most costly and complex option, typically reserved for organizations with extreme data privacy requirements who cannot let their data leave their premises.

2.3. Ingredient 3: The Algorithms - The "Easy" Part

Fortunately, your job isn't to reinvent the wheel here. The algorithms for fine-tuning Transformer models like Whisper are largely a solved problem, and there's no "secret sauce" you need to invent. Platforms like Hugging Face and GitHub host numerous high-quality, open-source recipes and toolkits that you can use.

Your only job is to verify that the toolkit you choose has the receipts. That means clear benchmarksâ€”word error rate, latency, etc.â€”showing a measurable improvement in the model's performance before and after fine-tuning. This data proves that their recipe works. If a toolkit doesn't publish these numbers, be skeptical and move on.

Transition: Fine-tuning the core model is a huge step, but to create a truly professional service, we need to consider what happens after the initial transcription.
--------------------------------------------------------------------------------
3. Beyond the Basics: Advanced Customization Techniques

A raw transcript from an ASR model is often just a "verbatim" stream of words. Advanced techniques are needed to polish this output and improve its accuracy for specific use cases.

3.1. Post-Processing: From Verbatim to Readable

The goal of post-processing is to transform the raw, spoken-language output into clean, readable text. This involves fixing a range of common issues:

* Filtering profanity
* Adding proper punctuation and capitalization
* Correcting the casing of specific brand names (e.g., ensuring adidas is always lowercase)
* Fixing misspellings of custom terms (e.g., correcting Olay wave to Olewave)
* Handling filler words (e.g., deciding whether to keep or remove "um," "uh," etc.)

OpenAI's recommended solution is to use a Large Language Model (LLM) like GPT-4 to perform these corrections.

* Pros: This "few-shot learning" approach is powerful and requires no model training. Any developer can write a prompt to guide the LLM's corrections.
* Cons: LLMs can be a black box. This method adds significant cost and latency to each ASR request. Getting consistent, reliable corrections requires a frustrating amount of "prompt engineering," and a slightly different input can sometimes produce a wildly different output, which is a nightmare for a production service.

3.2. Language Modeling: Improving Accuracy with Text-Only Data

This is a powerful technique for improving accuracy on domain-specific terms, especially in a common scenario: when you do not have domain-specific audio but you do have text-based knowledge.

Imagine you are building a medical transcription app. You might have access to an entire medical textbook (text data) but no corresponding audio recordings of doctors using those terms. You can use this text to fine-tune a separate language model. This model can then "rescore" Whisper's initial output, correcting transcription errors and boosting the probability of it recognizing the correct medical terms. It's a clever way to inject domain knowledge into your ASR system using only text.

Transition: With these components and techniques, the path to building a powerful, customized ASR service becomes clear.
--------------------------------------------------------------------------------
4. Conclusion: Your Path to a Custom ASR Service

Building on a powerful foundation model like Whisper is a "high ROI" strategy that has made custom speech recognition accessible to a new generation of students, hobbyists, and entrepreneurs. The path is no longer about building from zero, but about intelligent and targeted customization.

The essential steps are clear:

1. Start with Whisper as your powerful, general-purpose base model.
2. Identify its limitations for your specific project, whether it's recognizing medical terms or understanding a regional dialect.
3. Gather domain-specific data, which is the most critical ingredient for differentiating your service.
4. Fine-tune the model using accessible cloud computing and proven open-source algorithms.
5. Refine the final output with post-processing rules and specialized language models to make it polished and professional.

The tools and foundational models available today have dramatically lowered the barrier to entry. With the right data and a clear plan, you have everything you need to start building your own specialized speech recognition services.

---

### 6.2.26 ASR AI Job Profile

**Why**  
Speech recognition jobs need the same provenance, validation, and lifecycle guarantees as document editing. Defining ASR as a profile ensures transcripts are traceable and integrate cleanly with the workspace data model.

**What**  
Defines the ASR-specific AI job profile: profile-specific fields (media references, time ranges, model selection), PlannedOperation types, provenance structure, validation rules, and typical job flow.

**Jargon**  
- **media_id**: Reference to the audio/video resource being transcribed.
- **time_range**: Start/end timestamps for segment-based transcription.
- **asr_origin**: Provenance record attached to transcript segments.
- **diarization**: Speaker identification and separation in multi-speaker audio.

**Implements:** AI Job Model (Section 2.6.6)  
**Profile ID:** `asr_transcribe_v0.1`

This profile governs AI jobs that transcribe audio/video content in the Handshake workspace.

#### 6.2.26.1 Profile-Specific Fields

| Field | Type | Description |
|-------|------|-------------|
| `media_id` | MediaId | Reference to the audio/video resource |
| `time_ranges` | [TimeRange] | Segments to transcribe (or full if empty) |
| `language_hint` | LanguageCode | Expected language (optional) |
| `asr_model_id` | ModelId | ASR model to use (e.g., `whisper-large-v3`) |
| `diarization_enabled` | Boolean | Whether to attempt speaker diarization |
| `target_doc_id` | DocId | Optional: document to attach transcript to |

#### 6.2.26.2 PlannedOperation Types

| Operation | Description |
|-----------|-------------|
| `transcribe_segment(media_id, time_range, asr_model_id)` | Transcribe a single segment |
| `align_transcript(media_id, transcript_segments)` | Align segments with timestamps |
| `identify_speakers(media_id, segments)` | Run speaker diarization (if enabled) |
| `attach_transcript(media_id, target_doc_id, transcript)` | Link transcript to document |

#### 6.2.26.3 Provenance

Transcript segments carry `asr_origin`:
- `job_id`: The transcription job
- `media_id`: Source audio/video reference
- `time_range`: Start/end timestamps
- `asr_model_id`: Model used
- `confidence`: Word-level or segment-level confidence

#### 6.2.26.4 Validation Rules

| Validator | Purpose |
|-----------|---------|
| `media_accessible` | Audio/video file is readable |
| `format_supported` | Audio format is supported (via ffmpeg) |
| `duration_within_limits` | Recording length within configured maximum |
| `gpu_available` | GPU resources available (or CPU fallback configured) |

#### 6.2.26.5 Typical Job Flow

```
1. queued          â†’ Media uploaded, format validated
2. running         â†’ ASR runtime processing segments
3. awaiting_validation â†’ Transcription complete
4. completed       â†’ Transcript attached as DerivedContent
```

**Note:** ASR jobs typically do not require `awaiting_user` since transcripts are DerivedContent and non-destructive. Users can edit transcripts post-completion.

---

**Key Takeaways**  
- ASR transcription jobs are AI jobs under the global model ((AI Job Model, Section 2.6.6)).
- The profile adds media references, time ranges, and ASR-specific options.
- Transcripts are attached as DerivedContent with full provenance.
- The workflow integrates with the Model Runtime Layer's ASR service.

---

# 7. User Experience & Development
## 7.1 User Interface Components

**Why**  
The UI components define how users interact with Handshake. Choosing the right libraries and patterns ensures a familiar yet powerful experience combining the best of Notion, Milanote, and Excel.

**What**  
Covers the three main UI components: Rich Text Editor (Notion-like block-based editing with Tiptap/BlockNote), Freeform Canvas (Milanote-like infinite whiteboard with Excalidraw), Spreadsheet Engine (Excel-like data manipulation with Wolf-Table + HyperFormula), and Additional Views (Kanban, Calendar, Timeline).

**Jargon**  
- **Block-Based Editor**: Content made of stackable, movable blocks (paragraphs, images, lists) rather than continuous text.
- **Tiptap**: Popular open-source editor framework built on ProseMirror.
- **BlockNote**: Notion-style block editor built on Tiptap with pre-built components.
- **Slash Commands**: Type "/" to access insertion menu (like /heading, /image).
- **Excalidraw**: Popular open-source whiteboard with hand-drawn aesthetic.
- **Infinite Canvas**: Workspace extending forever in all directions with pan/zoom.
- **HyperFormula**: Open-source formula engine with 400+ Excel-compatible functions.
- **Wolf-Table (x-spreadsheet)**: Lightweight JavaScript spreadsheet grid.

---
This section covers the frontend UI components that make up the Handshake user experience, combining the best features of Notion, Milanote, and Excel.

---

### 7.1.1 Rich Text Editor (Notion-like)

**Prerequisites:** Section 2.1 (High-Level Architecture)  
**Related to:** Section 7.1 (User Interface Components)  
**Implements:** Core document editing  
**Read time:** ~6 minutes

**The document editor is the heart of Handshakeâ€”a "block-based" editor where every paragraph, image, and element is a separate, movable piece.**

---

#### 7.1.1.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Block-Based Editor** | Instead of one continuous document, content is made of stackable "blocks" (paragraphs, images, lists, etc.) | Enables drag/drop, AI operations on specific sections |
| **Tiptap** | A popular open-source editor framework built on ProseMirror | Leading candidate for our editor |
| **BlockNote** | A Notion-style block editor built on Tiptap | Pre-built Notion-like components |
| **Slash Commands** | Type "/" to get a menu of things to insert (like /heading, /image) | Familiar UX from Notion |
| **Real-Time Collaboration** | Multiple people editing the same document simultaneously | Requires CRDT integration |

---

#### 7.1.1.2 The Block Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRADITIONAL DOCUMENT                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  One continuous blob of formatted text                      â”‚
â”‚  that flows from top to bottom. Hard to                     â”‚
â”‚  rearrange, hard for AI to understand                       â”‚
â”‚  structure.                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         vs.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BLOCK-BASED DOCUMENT                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Heading                                       â”‚ â˜°  â”‚
â”‚  â”‚ "Project Overview"                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Paragraph                                     â”‚ â˜°  â”‚
â”‚  â”‚ "This project aims to..."                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: AI-Generated Summary                         â”‚ â˜°  â”‚
â”‚  â”‚ "Key points: 1) ... 2) ... 3) ..."                 â”‚ ğŸ¤– â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Image                                        â”‚ â˜°  â”‚
â”‚  â”‚ [diagram.png]                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â˜° = Drag handle (reorder blocks)                          â”‚
â”‚  ğŸ¤– = AI-generated content indicator                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.1.3 Technology Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Rich text editor framework         â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Tiptap/ProseMirror - Most extensible, proven            â”‚
â”‚   â€¢ BlockNote - Notion-style, built on Tiptap               â”‚
â”‚   â€¢ Lexical (Meta) - Newer, less collaboration support      â”‚
â”‚   â€¢ Slate.js - Flexible but complex                         â”‚
â”‚                                                              â”‚
â”‚ Recommendation: TIPTAP with BLOCKNOTE components             â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ BlockNote provides Notion-style blocks out of the box   â”‚
â”‚   â€¢ Tiptap is highly extensible for custom AI blocks        â”‚
â”‚   â€¢ Yjs integration available for real-time collaboration   â”‚
â”‚   â€¢ Large community and good documentation                   â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Some learning curve                                      â”‚
â”‚   â€¢ May need custom extensions for AI features              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.1.4 Block Types to Implement

| Block Type | Priority | Description |
|------------|----------|-------------|
| **Paragraph** | [CORE] | Basic text |
| **Heading** | [CORE] | H1, H2, H3 |
| **List** | [CORE] | Bullet, numbered, checklist |
| **Image** | [CORE] | With AI generation capability |
| **Code** | [CORE] | Syntax highlighting |
| **Quote** | [CORE] | Blockquotes |
| **Divider** | [CORE] | Horizontal rule |
| **Table** | [OPTIONAL] | Basic tables |
| **Callout** | [OPTIONAL] | Colored highlight boxes |
| **Toggle** | [OPTIONAL] | Collapsible sections |
| **Embed** | [ADVANCED] | YouTube, tweets, etc. |
| **Database View** | [ADVANCED] | Inline Notion-style databases |
| **AI Block** | [CORE] | AI-generated content with indicators |

---

#### 7.1.1.5 AI Integration Points

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI-ENHANCED EDITING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SLASH COMMAND MENU (type "/")                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ / Basic                         â”‚                        â”‚
â”‚  â”‚   Paragraph, Heading, List...   â”‚                        â”‚
â”‚  â”‚                                 â”‚                        â”‚
â”‚  â”‚ / AI Actions âœ¨                 â”‚                        â”‚
â”‚  â”‚   ğŸ“ Generate text              â”‚                        â”‚
â”‚  â”‚   ğŸ“‹ Summarize above            â”‚                        â”‚
â”‚  â”‚   ğŸ”„ Rewrite selection          â”‚                        â”‚
â”‚  â”‚   ğŸŒ Translate                  â”‚                        â”‚
â”‚  â”‚   ğŸ¨ Generate image             â”‚                        â”‚
â”‚  â”‚   ğŸ’» Generate code              â”‚                        â”‚
â”‚  â”‚   ğŸ“Š Create table from text     â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  CONTEXT MENU (select text, right-click)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Improve writing                 â”‚                        â”‚
â”‚  â”‚ Make shorter                    â”‚                        â”‚
â”‚  â”‚ Make longer                     â”‚                        â”‚
â”‚  â”‚ Fix grammar                     â”‚                        â”‚
â”‚  â”‚ Explain this                    â”‚                        â”‚
â”‚  â”‚ Ask AI...                       â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.1.6 Key Takeaways

- âœ“ **Block-based editing** enables flexible layouts and AI operations
- âœ“ **Tiptap + BlockNote** is the recommended stack
- âœ“ **Slash commands** provide quick access to AI features
- âœ“ Blocks can be drag-and-dropped, nested, and reordered
- âœ“ Real-time collaboration via Yjs integration

**See Also:** [Section 3.2 - CRDT Libraries Comparison](#32-crdt-libraries-comparison)

---

### 7.1.2 Freeform Canvas (Milanote-like)

**Prerequisites:** Section 2.1 (High-Level Architecture)  
**Related to:** Section 7.1 (User Interface Components)  
**Implements:** Visual brainstorming space  
**Read time:** ~5 minutes

**The canvas is an infinite whiteboard where you can drag notes, images, and shapes anywhereâ€”like a digital corkboard for visual thinkers.**

---

#### 7.1.2.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Infinite Canvas** | A workspace that extends forever in all directions | No page boundaries, unlimited space |
| **Excalidraw** | Popular open-source whiteboard with hand-drawn look | Leading candidate for our canvas |
| **React-Konva** | Library for drawing graphics in React | Alternative for custom canvas needs |
| **Pan & Zoom** | Moving around and magnifying the canvas | Essential for large boards |

---

#### 7.1.2.2 The Canvas vs. Document Distinction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT EDITOR                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Text flows top-to-bottom                          â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Linear structure                                  â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Like a Word document or web page                  â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  BEST FOR: Writing, documentation, structured content       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CANVAS BOARD                              â”‚
â”‚                                                              â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚      â”‚ Note  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Image â”‚               â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚            \                                                â”‚
â”‚             \     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚              â”€â”€â”€â”€â”€â”‚ Idea Box  â”‚                             â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”‚                                    â”‚
â”‚    â”‚Sketch â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ Reference â”‚            â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  BEST FOR: Brainstorming, mood boards, spatial thinking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.2.3 Technology Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Canvas/whiteboard library          â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Excalidraw - Mature, MIT-licensed, hand-drawn feel      â”‚
â”‚   â€¢ tldraw - Modern, React-focused, good collaboration      â”‚
â”‚   â€¢ React-Konva - Low-level, full control                   â”‚
â”‚   â€¢ Fabric.js - Canvas library, more work to integrate      â”‚
â”‚                                                              â”‚
â”‚ Recommendation: EXCALIDRAW                                   â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ Production-proven (used by many products)               â”‚
â”‚   â€¢ Built-in collaboration support                          â”‚
â”‚   â€¢ Familiar "whiteboard" UX                                â”‚
â”‚   â€¢ Can embed in React easily                               â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ "Hand-drawn" aesthetic may not fit all use cases        â”‚
â”‚   â€¢ May need customization for Milanote-style features      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.2.4 Canvas Element Types

| Element | Priority | Description |
|---------|----------|-------------|
| **Sticky Note** | [CORE] | Text cards that can be moved |
| **Image** | [CORE] | Photos, generated images |
| **Shape** | [CORE] | Rectangles, circles, arrows |
| **Line/Arrow** | [CORE] | Connect elements |
| **Text** | [CORE] | Freestanding labels |
| **Drawing** | [OPTIONAL] | Freehand sketching |
| **Frame/Group** | [OPTIONAL] | Organize related items |
| **Embedded Note** | [ADVANCED] | Link to full document |
| **AI Image Generation** | [CORE] | Generate images directly on canvas |

---

#### 7.1.2.5 AI Integration for Canvas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI-ENHANCED CANVAS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  RIGHT-CLICK ON CANVAS:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ¨ Generate image here...       â”‚                        â”‚
â”‚  â”‚ ğŸ“ Add AI note about...         â”‚                        â”‚
â”‚  â”‚ ğŸ’¡ Brainstorm ideas about...    â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  SELECT MULTIPLE ITEMS:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ“‹ Summarize selected items     â”‚                        â”‚
â”‚  â”‚ ğŸ”— Find connections             â”‚                        â”‚
â”‚  â”‚ ğŸ“Š Organize into categories     â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  DRAG IMAGE ONTO CANVAS:                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ” Describe this image          â”‚                        â”‚
â”‚  â”‚ ğŸ¨ Generate variations          â”‚                        â”‚
â”‚  â”‚ âœ‚ï¸ Remove background             â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.3.6 Key Takeaways

- âœ“ **Two components:** Data Grid (UI) + Formula Engine (HyperFormula)
- âœ“ **HyperFormula** provides Excel-compatible formulas
- âœ“ Data stored as CSV (portable) with JSON sidecar for formatting
- âœ“ AI can help write formulas and analyze data
- âœ“ Start simple, add advanced features later

---

### 7.1.4 Additional Views: Kanban, Calendar, Timeline

**Prerequisites:** Section 7.1 (User Interface Components), Section 7.1 (User Interface Components)  
**Related to:** Section 2.2 (Data & Content Model)  
**Implements:** Notion-style database views  
**Read time:** ~4 minutes

**The same data can be viewed different ways: as a table, as Kanban cards, as calendar events, or as a timeline.**

---

#### 7.1.4.1 The "Views" Concept

â•â•â• CORE CONCEPT â•â•â•

> **One dataset, many presentations.** A list of tasks can be:
> - A **table** (spreadsheet-style rows)
> - A **Kanban board** (cards in columns like "To Do", "In Progress", "Done")
> - A **calendar** (if tasks have dates)
> - A **timeline/Gantt** (showing duration and dependencies)
>
> The underlying data is identical; only the visualization changes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SAME DATA, DIFFERENT VIEWS                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DATABASE: Tasks                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ID â”‚ Title        â”‚ Status      â”‚ Due Date â”‚ Owner   â”‚   â”‚
â”‚  â”‚ 1  â”‚ Design logo  â”‚ In Progress â”‚ Dec 1    â”‚ Alice   â”‚   â”‚
â”‚  â”‚ 2  â”‚ Write copy   â”‚ To Do       â”‚ Dec 3    â”‚ Bob     â”‚   â”‚
â”‚  â”‚ 3  â”‚ Launch site  â”‚ To Do       â”‚ Dec 10   â”‚ Alice   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚           â”‚                    â”‚                    â”‚        â”‚
â”‚           â–¼                    â–¼                    â–¼        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   TABLE      â”‚  â”‚   KANBAN     â”‚  â”‚   CALENDAR   â”‚       â”‚
â”‚  â”‚   VIEW       â”‚  â”‚   VIEW       â”‚  â”‚   VIEW       â”‚       â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚       â”‚
â”‚  â”‚ Spreadsheet  â”‚  â”‚ To Do â”‚ In   â”‚  â”‚    Dec       â”‚       â”‚
â”‚  â”‚ style rows   â”‚  â”‚       â”‚Progr â”‚  â”‚ 1 [Design]   â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ [Copy]â”‚[Logo]â”‚  â”‚ 3 [Copy]     â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ [Site]â”‚      â”‚  â”‚ 10 [Launch]  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.1.4.2 Implementation Priority

| View Type | Priority | Library Options |
|-----------|----------|-----------------|
| **Table** | [CORE] | AG Grid, React Table |
| **Kanban** | [CORE] | react-beautiful-dnd, dnd-kit |
| **Calendar** | [OPTIONAL] | FullCalendar, react-big-calendar |
| **Timeline/Gantt** | [ADVANCED] | frappe-gantt, custom |
| **Gallery** | [OPTIONAL] | Custom grid layout |

---

## 7.2 Multi-Agent Orchestration

**Why**  
Complex tasks require coordinating multiple specialized AI models. This section explains how to orchestrate agents effectively using the lead/worker pattern for cost-effective, high-quality results.

**What**  
Compares orchestration frameworks (AutoGen, LangGraph, CrewAI), explains the lead/worker pattern for cost optimization, covers shared context/memory between agents, and defines task routing and fallback logic.

**Jargon**  
- **Agent**: An AI model with a specific job and ability to take actions.
- **Orchestrator**: The "boss" code that decides which agent handles what.
- **AutoGen**: Microsoft's conversational multi-agent framework.
- **LangGraph**: LangChain's graph-based workflow framework.
- **CrewAI**: Simple role-based sequential pipeline framework.
- **Lead/Worker Pattern**: Smart model plans (once), simpler models execute (many times).
- **Shared Context Store**: Central memory where agents share information.

---
This section covers how multiple AI models coordinate to accomplish complex tasks.

---

### 7.2.1 Framework Comparison: AutoGen vs LangGraph vs CrewAI

**Prerequisites:** Section 7.2 (Multi-Agent Orchestration)  
**Related to:** Section 4 (LLM Infrastructure)  
**Implements:** Orchestration framework choice  
**Read time:** ~6 minutes

**Orchestration frameworks help coordinate multiple AI agents. Each framework has a different approach and strengths.**

---

#### 7.2.1.1 Framework Philosophies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THREE APPROACHES TO ORCHESTRATION               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  AUTOGEN (Microsoft)                                        â”‚
â”‚  Philosophy: Agents CONVERSE with each other                â”‚
â”‚                                                              â”‚
â”‚       Agent A â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Agent B                â”‚
â”‚          â”‚                              â”‚                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Agent C â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  Like: A meeting where experts discuss until done           â”‚
â”‚  Best for: Complex reasoning, human-in-loop                 â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  LANGGRAPH (LangChain)                                      â”‚
â”‚  Philosophy: Tasks flow through a GRAPH of steps            â”‚
â”‚                                                              â”‚
â”‚       [Start]â”€â”€â–¶[Plan]â”€â”€â–¶[Execute]â”€â”€â–¶[Review]â”€â”€â–¶[End]      â”‚
â”‚                    â”‚                    â”‚                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                         (if review fails)                   â”‚
â”‚                                                              â”‚
â”‚  Like: A flowchart where you define exactly what happens    â”‚
â”‚  Best for: Predictable workflows, complex conditionals      â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  CREWAI                                                     â”‚
â”‚  Philosophy: Agents have ROLES and work in SEQUENCE         â”‚
â”‚                                                              â”‚
â”‚       [Researcher]â”€â”€â–¶[Writer]â”€â”€â–¶[Editor]â”€â”€â–¶[Publisher]     â”‚
â”‚                                                              â”‚
â”‚  Like: An assembly line with specialists                    â”‚
â”‚  Best for: Simple, linear workflows                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.2.1.2 Detailed Comparison

| Aspect | AutoGen | LangGraph | CrewAI |
|--------|---------|-----------|--------|
| **Learning Curve** | Medium | High | Low |
| **Flexibility** | High | Very High | Medium |
| **Debugging** | Conversation logs | Visual graph | Role inspection |
| **Human-in-Loop** | Excellent | Good | Limited |
| **Complex Branching** | Good | Excellent | Limited |
| **Setup Effort** | Medium | Higher | Low |
| **Documentation** | Good | Good | Growing |
| **Local-First** | Yes | Yes | Yes |

---

#### 7.2.1.3 Decision Point

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Multi-agent orchestration frameworkâ”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ AutoGen - Conversational agents, Microsoft-backed        â”‚
â”‚   â€¢ LangGraph - Graph-based workflows, very flexible         â”‚
â”‚   â€¢ CrewAI - Simple role-based pipelines                    â”‚
â”‚                                                              â”‚
â”‚ Recommendation: START WITH AUTOGEN, consider LangGraph      â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ AutoGen balances power and approachability              â”‚
â”‚   â€¢ Good human-in-loop support (important for AI trust)     â”‚
â”‚   â€¢ Microsoft backing suggests long-term maintenance        â”‚
â”‚   â€¢ Can migrate to LangGraph if more control needed         â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Less explicit flow control than LangGraph               â”‚
â”‚   â€¢ Conversation logging can be verbose                     â”‚
â”‚   â€¢ May need custom work for complex branching              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.2.1.4 Key Takeaways

- âœ“ **AutoGen** recommended for initial development
- âœ“ **LangGraph** as alternative if explicit flow control needed
- âœ“ **CrewAI** too limited for complex Handshake workflows
- âœ“ All frameworks run locally with any LLM

---

### 7.2.2 The Lead/Worker Pattern

**Prerequisites:** Section 7.2 (Multi-Agent Orchestration)  
**Related to:** Section 4.2 (LLM Inference Runtimes)  
**Implements:** Cost-effective multi-model approach  
**Read time:** ~4 minutes

**Use a powerful model to PLAN, then cheaper models to EXECUTE. This balances quality and cost.**

---

#### 7.2.2.1 The Pattern Explained

â•â•â• CORE CONCEPT â•â•â•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEAD/WORKER PATTERN                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  COMPLEX TASK: "Create a product launch plan with           â”‚
â”‚                 marketing copy and social media posts"      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              LEAD (GPT-4 Cloud)                      â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  "Here's the plan:                                  â”‚    â”‚
â”‚  â”‚   1. Executive summary (100 words)                  â”‚    â”‚
â”‚  â”‚   2. Target audience analysis                       â”‚    â”‚
â”‚  â”‚   3. Key messaging (3 bullet points)                â”‚    â”‚
â”‚  â”‚   4. Timeline with milestones                       â”‚    â”‚
â”‚  â”‚   5. Social posts: Twitter (3), LinkedIn (2)        â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚   Each section should follow format X..."           â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Cost: $0.15 (one complex reasoning call)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           WORKERS (Local Llama 3 13B)               â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Task 1: Write executive summary â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 2: Write audience analysis â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 3: Write key messaging â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 4: Create timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 5: Write social posts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Cost: $0.00 (local, unlimited)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  TOTAL COST: ~$0.15 instead of ~$1.50+ if all cloud        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.2.3.2 Key Takeaways

- âœ“ Agents share context through a central store
- âœ“ File-based storage aligns with overall architecture
- âœ“ Vector store enables semantic search over past interactions
- âœ“ Essential for coherent multi-step tasks

---

### 7.2.4 Task Routing and Fallback Logic

**Prerequisites:** Section 3.1  
**Related to:** Section 4 (LLM Infrastructure)  
**Implements:** Intelligent model selection  
**Read time:** ~4 minutes

**The orchestrator must decide which model handles each task, and what to do if it fails.**

---

#### 7.2.4.1 Routing Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TASK ROUTING LOGIC                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  NEW TASK ARRIVES                                           â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it code-related? â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ Code Llama            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it image gen?    â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ SDXL/ComfyUI          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it complex       â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ Lead/Worker           â”‚
â”‚  â”‚ multi-step?         â”‚            (GPT-4 â†’ Local)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Default             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Local LLM (Llama 3)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  IF ANY MODEL FAILS:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Check error type                                 â”‚    â”‚
â”‚  â”‚ 2. If quality issue â†’ retry with larger model      â”‚    â”‚
â”‚  â”‚ 3. If timeout â†’ retry with smaller model           â”‚    â”‚
â”‚  â”‚ 4. If persistent failure â†’ escalate to cloud       â”‚    â”‚
â”‚  â”‚ 5. Log everything for debugging                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Key Takeaways**  
- AutoGen recommended for conversational multi-agent orchestration with good human-in-loop support.
- Lead/Worker pattern optimizes costs: cloud models plan, local models execute.
- Shared context store enables agents to collaborate without redundant processing.
- Task routing uses complexity analysis and confidence thresholds for intelligent fallback.

---

## 7.3 Collaboration and Sync

**Why**  
Multi-device and multi-user collaboration requires robust synchronization. This section covers how CRDT-based sync enables real-time collaboration while maintaining offline-first functionality.

**What**  
Explains sync architecture using Yjs, covers server infrastructure options, handles conflict resolution, and defines sharing/permissions model.

**Jargon**  
- **CRDT**: Conflict-free Replicated Data Typeâ€”data structures that automatically merge without conflicts.
- **Yjs**: JavaScript CRDT library chosen for real-time collaboration.
- **y-websocket**: Yjs sync provider using WebSocket connections.
- **y-indexeddb**: Yjs persistence provider using browser IndexedDB.
- **Awareness**: Yjs feature showing who's online and cursor positions.

---
This section covers how Handshake enables multiple users and devices to work together.

---

### 7.3.1 Understanding CRDTs

**Prerequisites:** Section 3.1 (Local-First Data Fundamentals)  
**Related to:** Section 3.1 (Offline-First Architecture)  
**Implements:** Conflict-free collaboration  
**Read time:** ~5 minutes

**CRDTs are special data structures that allow multiple people to edit simultaneously without conflictsâ€”even while offline.**

---

#### 7.3.1.1 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **CRDT** | Conflict-free Replicated Data Type - data that merges automatically | Enables real-time collaboration |
| **Yjs** | Most popular JavaScript CRDT library | Our likely choice for sync |
| **Automerge** | Alternative CRDT library | Fallback option |
| **Merge** | Combining two versions of a document | Happens automatically with CRDTs |
| **Operational Transform (OT)** | Older technique (Google Docs uses this) | CRDTs are newer and better for offline |

---

#### 7.3.1.2 How CRDTs Work (Simplified)

â•â•â• CORE CONCEPT â•â•â•

> Traditional documents: "Last write wins" (someone's work gets lost)
> 
> CRDT documents: "All writes merge" (everyone's work is preserved)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TRADITIONAL SYNC (CONFLICTS!)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Original: "Hello World"                                    â”‚
â”‚                                                              â”‚
â”‚  Alice (offline):  "Hello World!" (added !)                 â”‚
â”‚  Bob (offline):    "Hello Earth" (changed World)            â”‚
â”‚                                                              â”‚
â”‚  When both sync:                                            â”‚
â”‚  âŒ CONFLICT! Which version wins?                           â”‚
â”‚  â€¢ Keep Alice's? Bob loses his change.                      â”‚
â”‚  â€¢ Keep Bob's? Alice loses her change.                      â”‚
â”‚  â€¢ Show conflict dialog? Annoying.                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CRDT SYNC (NO CONFLICTS!)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Original: "Hello World"                                    â”‚
â”‚                                                              â”‚
â”‚  Alice (offline): Insert "!" at position 11                 â”‚
â”‚  Bob (offline):   Replace "World" with "Earth"              â”‚
â”‚                                                              â”‚
â”‚  When both sync:                                            â”‚
â”‚  âœ… CRDT merges both operations:                            â”‚
â”‚  Result: "Hello Earth!"                                     â”‚
â”‚                                                              â”‚
â”‚  Both changes preserved! No conflict dialog!                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.3.1.3 Yjs: Our CRDT Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: CRDT implementation                â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Yjs - Most popular, used by many editors                â”‚
â”‚   â€¢ Automerge - Good, Rust implementation available         â”‚
â”‚   â€¢ Custom - Too much work                                   â”‚
â”‚                                                              â”‚
â”‚ Recommendation: YJS                                          â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ Tiptap (our editor) has Yjs integration built-in       â”‚
â”‚   â€¢ Large ecosystem and community                           â”‚
â”‚   â€¢ Works offline natively                                   â”‚
â”‚   â€¢ Can sync via any transport (WebSocket, WebRTC, file)   â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ JavaScript-focused (need yrs for Rust interop)         â”‚
â”‚   â€¢ Learning curve for CRDT concepts                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.3.2.2 Key Takeaways

- âœ“ App is fully functional offline
- âœ“ Sync is optional, not required
- âœ“ CRDTs handle conflict-free merging
- âœ“ User chooses if/where to sync

---

### 7.3.3 Google Workspace Integration

**Prerequisites:** Section 3.1 (Offline-First)  
**Related to:** Section 2.1 (High-Level Architecture)  
**Implements:** Gmail, Drive, Calendar sync  
**Read time:** ~4 minutes

**Optionally sync with Google services: backup to Drive, import emails, show calendar events.**

---

#### 7.3.3.1 Integration Points

| Service | Integration | Priority |
|---------|-------------|----------|
| **Google Drive** | Backup workspace, sync files | [OPTIONAL] |
| **Gmail** | Import emails as documents | [OPTIONAL] |
| **Calendar** | Show events in calendar view | [OPTIONAL] |
| **Google Docs** | Export/import documents | [ADVANCED] |

---

#### 7.3.3.2 OAuth2 Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOOGLE AUTH FLOW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. User clicks "Connect Google Account"                    â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  2. Opens system browser to Google login                    â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  3. User grants permissions (minimal scopes)                â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  4. Google redirects back to app with auth code             â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  5. App exchanges code for tokens                           â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  6. Tokens stored encrypted locally                         â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  7. App can now call Google APIs                            â”‚
â”‚                                                              â”‚
â”‚  SECURITY: Tokens never leave user's machine                â”‚
â”‚  PRIVACY: Minimal scopes requested                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Key Takeaways**  
- Yjs provides the CRDT foundation for real-time collaboration.
- Sync server can be self-hosted or use managed services.
- Offline-first means changes always save locally first.
- Permissions model uses simple owner/editor/viewer roles.

---

## 7.4 Reference Application Analysis

**Why**  
Learning from similar applications avoids repeating their mistakes. This section summarizes insights from analyzing AppFlowy, AFFiNE, Obsidian, and Logseq.

**What**  
Analyzes four reference applications (their stacks, data models, sync approaches), identifies patterns to follow and patterns to avoid.

**Jargon**  
- **AppFlowy**: Flutter + Rust open-source Notion alternative.
- **AFFiNE**: Electron + React workspace with custom Rust CRDT (OctoBase).
- **Obsidian**: Electron + TypeScript note-taking app with thriving plugin ecosystem.
- **Logseq**: Electron + ClojureScript outliner with bidirectional linking.

---

### 7.4.1 Reference Applications

#### 7.4.1.1 AppFlowy
**Stack:** Flutter (Dart) + Rust backend  
**Data:** CRDT-based (yrs), RocksDB storage  
**Sync:** Offline-first CRDT via Supabase

**Key Insights:**
- âœ“ Flutter provides native performance and feel
- âœ“ Rust CRDT implementation is solid
- âš ï¸ Flutter limits JavaScript plugin ecosystem
- âš ï¸ Minimal plugin API currently

#### 7.4.1.2 AFFiNE
**Stack:** Electron + React/TypeScript  
**Data:** OctoBase (custom Rust CRDT)  
**Sync:** P2P CRDT, local-first

**Key Insights:**
- âœ“ "Everything is a block" model works well
- âœ“ Blocksuite component library is promising
- âš ï¸ Switched from Tauri to Electron (webview issues)
- âš ï¸ Performance issues with large documents
- âš ï¸ No mature plugin API yet

#### 7.4.1.3 Obsidian
**Stack:** Electron + TypeScript  
**Data:** Plain Markdown files  
**Sync:** Local vault with optional Obsidian Sync

**Key Insights:**
- âœ“ Thriving plugin ecosystem (hundreds of plugins)
- âœ“ Markdown files = portable, future-proof
- âœ“ Excellent community engagement
- âš ï¸ Some performance issues with huge vaults

#### 7.4.1.4 Logseq
**Stack:** Electron + ClojureScript  
**Data:** Markdown/EDN files, SQLite  
**Sync:** Git/WebDAV/LiveSync options

**Key Insights:**
- âœ“ Mature JS plugin API
- âœ“ Bidirectional linking works well
- âš ï¸ Performance issues with large graphs/pages
- âš ï¸ Team added pagination to mitigate

---

### 7.4.2 Lessons Learned

#### 7.4.2.1 Patterns to Follow

| Pattern | Why It Works | Handshake Application |
|---------|--------------|----------------------|
| **File-based storage** | Portable, user-owned data | âœ“ Already planned |
| **Block-based editing** | Flexible, AI-friendly | âœ“ Using Tiptap/BlockNote |
| **CRDT sync** | Offline-first, conflict-free | âœ“ Using Yjs |
| **Plugin API early** | Builds ecosystem | Plan internal APIs from start |

#### 7.4.2.2 Patterns to Avoid

| Anti-Pattern | What Went Wrong | Handshake Mitigation |
|--------------|-----------------|---------------------|
| **Full doc re-render** | AFFiNE lag on keystroke | Virtualization, incremental updates |
| **Monolithic DB** | Joplin RAM bloat | File-based with SQLite index only |
| **No export path** | Athens shutdown orphaned users | Standard formats, export from day 1 |
| **Tauri webview issues** | AFFiNE switched to Electron | Minimal Tauri responsibilities, test early |

---

**Key Takeaways**  
- Learn from others' mistakes before building.
- Performance at scale is a real concernâ€”virtualize and paginate.
- Export/migration paths are essential for user trust.
- Plugin ecosystems take years to buildâ€”start API design early.
- Test Tauri thoroughly early; keep its responsibilities minimal.

---

## 7.5 Development Workflow

**Why**  
Consistent development practices ensure code quality and team productivity. This section defines the tooling, processes, and standards for the project.

**What**  
Covers repository structure (monorepo with Turborepo), code quality tools (ESLint, Prettier, Ruff), CI/CD pipeline (GitHub Actions), testing strategy, and project health practices.

**Jargon**  
- **Monorepo**: Single repository containing multiple packages/projects.
- **Turborepo**: Monorepo build tool with intelligent caching.
- **ESLint**: JavaScript/TypeScript linting tool.
- **Prettier**: Code formatter for consistent style.
- **Ruff**: Fast Python linter and formatter.
- **CI/CD**: Continuous Integration/Continuous Deployment pipeline.
- **Pre-commit Hooks**: Scripts that run before commits to catch issues early.

---
This section covers how to actually build Handshake efficiently.

---

### 7.5.1 Using AI Coding Assistants Effectively

**Prerequisites:** Section 4 (AI Models)  
**Related to:** Section 7.5 (Development Workflow)  
**Implements:** Development efficiency  
**Read time:** ~5 minutes

**The research documents provide a clear model for using AI assistants during development.**

---

#### 7.5.1.1 The Three-Layer Model

â•â•â• CORE CONCEPT â•â•â•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AI ASSISTANTS IN DEVELOPMENT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         GPT-4 / CLAUDE (Architects)                  â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ Feature specs and requirements                   â”‚    â”‚
â”‚  â”‚  â€¢ Architecture decisions                           â”‚    â”‚
â”‚  â”‚  â€¢ Trade-off analysis                               â”‚    â”‚
â”‚  â”‚  â€¢ Code review                                      â”‚    â”‚
â”‚  â”‚  â€¢ Debugging complex issues                         â”‚    â”‚
â”‚  â”‚  â€¢ Test strategy                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         CODEX / CODE MODELS (Implementers)          â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ Writing code from specs                          â”‚    â”‚
â”‚  â”‚  â€¢ Mechanical refactoring                           â”‚    â”‚
â”‚  â”‚  â€¢ Generating tests                                 â”‚    â”‚
â”‚  â”‚  â€¢ Writing boilerplate                              â”‚    â”‚
â”‚  â”‚  â€¢ Documentation comments                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         N8N / AUTOMATION (Operations)               â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ CI/CD workflows                                  â”‚    â”‚
â”‚  â”‚  â€¢ Health monitoring                                â”‚    â”‚
â”‚  â”‚  â€¢ Notifications                                    â”‚    â”‚
â”‚  â”‚  â€¢ External integrations                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.5.1.2 AI Development Workflow

| Phase | Use Generalist (GPT-4/Claude) | Use Code Model (Codex) |
|-------|------------------------------|------------------------|
| **Planning** | âœ“ Define specs, goals, non-goals | |
| **Architecture** | âœ“ Design systems, APIs | Scaffold structure |
| **Implementation** | Review PRs | âœ“ Write code from specs |
| **Testing** | Design test strategy | âœ“ Write test code |
| **Debugging** | âœ“ Analyze logs, hypothesize | Apply fixes |
| **Documentation** | âœ“ Write overviews | Docstrings, comments |

---

#### 7.5.2.4 Key Takeaways

- âœ“ **One health command** for all checks
- âœ“ Linters and formatters for consistency
- âœ“ Pre-commit hooks to catch issues early
- âœ“ Type annotations for AI and human safety

---

### 7.5.3 CI/CD and Testing Strategy

**Prerequisites:** Section 7.5 (Development Workflow)  
**Related to:** Section 7.6 (Development Roadmap)  
**Implements:** Automated quality assurance  
**Read time:** ~4 minutes

**Continuous Integration ensures every code change is tested automatically.**

---

#### 7.5.3.1 CI Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CI PIPELINE (on every push)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. LINT                                                    â”‚
â”‚     â””â”€ Ruff, ESLint                                        â”‚
â”‚                                                              â”‚
â”‚  2. TYPE CHECK                                              â”‚
â”‚     â””â”€ mypy, TypeScript                                    â”‚
â”‚                                                              â”‚
â”‚  3. UNIT TESTS                                              â”‚
â”‚     â””â”€ pytest, vitest (fast tests only)                    â”‚
â”‚                                                              â”‚
â”‚  4. BUILD                                                   â”‚
â”‚     â””â”€ Frontend bundle, backend validation                 â”‚
â”‚                                                              â”‚
â”‚  IF ALL PASS â†’ âœ… Ready to merge                            â”‚
â”‚  IF ANY FAIL â†’ âŒ Block merge, fix issues                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 7.5.3.2 Testing Pyramid

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   E2E     â”‚  Few, slow, high confidence
            â”‚   Tests   â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Integration   â”‚  Some, medium speed
         â”‚    Tests      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Unit Tests    â”‚  Many, fast, low coupling
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Key Takeaways**  
- Monorepo structure with Turborepo enables efficient builds and caching.
- Consistent linting and formatting enforced through pre-commit hooks.
- CI/CD pipeline automates testing and deployment.
- Project health practices prevent technical debt accumulation.

---

## 7.6 Development Roadmap

**Why**  
A clear roadmap with phases and dependencies ensures focused effort and prevents scope creep. This section provides the practical build order for Project Handshake.

**What**  
Defines four development phases (Foundation, Core Editing, AI Integration, Visual Tools + Polish), specifies MVP scope, and shows the dependency graph for build order.

**Jargon**  
- **MVP (Minimum Viable Product)**: The smallest set of features that delivers value and validates the concept.
- **IPC (Inter-Process Communication)**: How the frontend and backend processes communicate.
- **Phase 0**: Foundation work (monorepo, scaffolding, CI pipeline, basic IPC).

---

#### ## 7.6 Development Roadmap

**Why**  
Handshake is intentionally ambitious: local models, workflows, governance, ingestion, ASR, collaboration. A roadmap is required to sequence this complexity into phases that are buildable, testable, and debuggable. The goal is to ensure that governance (Diary, AI Job Model, capabilities), observability (Flight Recorder, metrics, traces), migrations, and debug tools are present from the start, not bolted on after features ship.

**What**  
This section defines a phased implementation plan for Handshake, from a pre-MVP foundation to a multi-user, extensible workspace. Each phase specifies what MUST be shipped, what is explicitly out of scope, and:

- A **vertical slice** (end-to-end user flow) that proves the phase is real, not just infra.  
- Key **risks** the phase should reduce.  
- **Acceptance criteria** that define â€œDoneâ€, including debug and diagnostic surfaces.

All phases are aligned with the architecture and mechanisms defined in Sections 2â€“6 (Architecture, Data Model, LLM Infrastructure, Observability, Mechanical Integrations).

**Jargon**  
- **Pre-MVP (Phase 0)** â€“ Foundation work that produces a running but non-compelling app; used to validate architecture, tooling, and debug surfaces.  
- **Product MVP (Phase 1)** â€“ First version that a single user can use for serious work, with governance and full diagnostic surfaces.  
- **Phase** â€“ A coherent bundle of changes that is shippable, testable, and has a clear vertical slice.  
- **Core loop** â€“ The smallest end-to-end user flow that exercises architecture and observability: â€œedit doc â†’ ask AI â†’ see changes + history + logs.â€  
- **Shadow Workspace** â€“ Background index and graph over workspace content used for search and RAG.  
- **Flight Recorder** â€“ Append-only event log for AI jobs, workflows, and user-visible actions, used for debugging and audit.  
- **AI Job** â€“ A single AI operation with ID, profile, capabilities, inputs, outputs, lifecycle, and provenance, as defined by the global AI Job Model.  
- **Debug surface** â€“ Any UI, log view, trace viewer, or health check that makes it possible for a human to understand and diagnose system behaviour without reading the entire codebase.  
- **Vertical slice** â€“ A thin, end-to-end scenario that exercises UI, backend, data, and observability in one flow.  

---

### 7.6.1 Scope and Principles

This roadmap applies to the **entire Handshake product**, not just subsystems.

It MUST:

- Align with the architectural layers in Section 2 (Architecture), Section 3 (Data Model), Section 4 (LLM Infrastructure), Section 5 (Observability & Benchmarks), and Section 6 (Mechanical Integrations).  
- Ensure that **AI Job Model**, **Workflow & Automation Engine**, **Flight Recorder**, and the **capability system** are exercised early and consistently.  
- Deliver a **single-user, local-first, offline-capable** product before adding multi-user sync, plugins, or cloud dependencies.  
- Treat **Docling** and **ASR** as extensions on top of the same AI Job and workflow mechanisms, not separate systems.  
- Require that every user-facing feature ships with a **diagnostic path**:
  - Logs or events in Flight Recorder.  
  - At least one debug surface (UI, CLI, or trace) that shows how it behaves.  
- Use each phase to **burn down risk**, not only to add features.  
- Provide **clear acceptance criteria** per phase: conditions and tests that must pass before moving on.  
- Preserve a **migration path**: schema and config changes must respect existing data where possible.

Out of scope for this section:

- Detailed team planning (sprints, owners, ticket breakdown).  
- Budget and resourcing assumptions.  
- Full QA test plans (see Sections 5.4â€“5.5 instead).

---

### 7.6.2 Phase 0 â€” Foundations (Pre-MVP)

**Goal**  
Stand up a stable â€œHello, workspaceâ€ application that matches the high-level architecture and establishes baseline logging, health checks, and a reproducible dev environment. No serious AI or governance yet, but debug tooling MUST already exist.

**MUST deliver**

1. **Desktop shell and process model**  
   - Tauri-based desktop application with a React front-end.  
   - Backend orchestrator process started and managed by the desktop shell.  
   - Canonical IPC/API channel between frontend and backend (HTTP/WebSocket/IPC), documented and testable.

2. **Workspace and data layer (single user)**  
   - SQLite workspace database with minimal schema for:
     - Workspaces / projects.  
     - Documents and blocks (honouring the Raw/Derived/Display split, even if only Raw/Display are used initially).  
     - Canvases, nodes, and edges.  
   - Basic, tested CRUD operations for documents and canvases.  
   - Initial schema migration mechanism (even simple, versioned migrations) so the DB can evolve safely.

3. **Editors and navigation**  
   - Rich text editor integrated in the main content area with:
     - Headings, paragraphs, lists, code blocks, quotes, inline marks.  
   - Canvas view integrated with:
     - Sticky notes / text boxes, simple shapes, arrows, pan/zoom.  
   - Workspace sidebar listing documents and canvases; open/save loop must be reliable.

4. **Project health, logging, and basic debug tools**  
   - Monorepo and tooling per Section 7.5 (linting, formatting, tests, CI) wired to this stack.  
   - Structured logging in frontend and backend (log level, context, correlation IDs where applicable).  
   - A **health check** endpoint or command that verifies at least:
     - App shell â†’ backend connectivity.  
     - Database connectivity.  
   - A simple developer-facing log view (tail of logs or structured log output) suitable for non-expert developers.  
   - One-command dev startup (script or target that starts frontend, backend, and DB with sample data).

**Vertical slice**  
- Start the app.  
- Create a workspace.  
- Create a document and a canvas, make simple edits, close the app, reopen, and verify content is intact.  
- Run the health check and inspect logs to confirm basic operations are recorded.

**Key risks addressed in Phase 0**

- Stack (Tauri + frontend + backend + DB) is unstable or too hard to run.  
- No consistent logging/health model, making later debugging painful.  
- Schema and migrations are ad-hoc from day one.

**Acceptance criteria**

- App can be started and used locally by running a single documented command.  
- Health check succeeds in a clean environment.  
- Logs clearly show at least workspace creation, document creation, and document save events.  
- A sample workspace can be created, exported (or re-created), and used as a fixture for later phases.

**Explicitly OUT of scope**

- AI Job Model, workflows, Flight Recorder, Shadow Workspace.  
- Multi-user sync or CRDT.  
- Docling, ASR, connectors, plugin system.

---

### 7.6.3 Phase 1 â€” Core Product MVP (Single-User, Local AI)

**Goal**  
Deliver the **first real Handshake**: a single-user, local-first workspace where documents and canvases are editable, AI assistance is available, and every AI action is traceable through the AI Job Model, Workflow Engine, Flight Recorder, and capability system. Debug tools for AI behaviour and workflows are mandatory.

**MUST deliver**

1. **Model runtime integration (LLM core)**  
   - Integrate one local LLM runtime (e.g. Ollama) as specified in Section 4.  
   - Configure at least one general-purpose model.  
   - Backend API for:
     - Chat-style requests with system + user prompts.  
     - Passing document context (selected text, document snapshot or summary).  

2. **AI Job Model (minimum viable implementation)**  
   - Implement the **global AI Job Model** (Section 2.6.6) in the backend:
     - `job_id`, `job_kind`, `protocol_id`, `status`, timestamps, error, inputs, outputs, metrics.  
     - Profile fields (`profile_id`, `capability_profile_id`, `access_mode`, `safety_mode`).  
   - Implement a **Docs AI Job profile subset** compatible with the Docs & Sheets profile:
     - `doc_id`, selection/range selector, layer scope, provenance fields linking edits back to source content and user.  

3. **Workflow & Automation Engine (minimum viable)**  
   - Implement the Workflow Engine core (Section 2.6) with:
     - Single-node workflows representing one AI job.  
     - Durable state in SQLite (workflow run + job records).  
     - Status transitions: `queued â†’ running â†’ completed / failed`.  
   - All AI work MUST go through the Workflow Engine; no direct â€œcall the modelâ€ shortcuts are allowed in production paths.

4. **Capability and consent enforcement (minimal slice)**  
   - Define a minimal capability set for documents:
     - `doc.read`, `doc.write`, `doc.summarize` (at minimum).  
   - Every AI job MUST declare required capabilities.  
   - The Workflow Engine MUST enforce that:
     - Jobs without `doc.write` cannot mutate documents.  
     - Write operations are applied only after passing validation (even if the MVP uses a deterministic, auto-accept validator).  
   - Consent-related fields MUST be persisted, even if the MVP uses a simple â€œglobal consentâ€ toggle.

5. **Flight Recorder (always-on, with UI)**  
   - Implement a Flight Recorder subsystem (Section 2.1.5 and Bootloader clauses) with:
     - Append-only log of AI job lifecycle events.  
     - Append-only log of model calls (model name, tokens, latency, outcome).  
     - Minimal tags to correlate events (job ID, workflow ID, document ID, user ID where applicable).  
   - Provide a **Job History** panel in the UI:
     - List jobs with status, timestamps, model used, and linked document.  
     - Ability to inspect job input and output payloads.  

6. **Baseline metrics and traces (debugging AI behaviour)**  
   - Export basic metrics for:
     - Request counts and error counts.  
     - Latency distribution per action (no target values required here).  
     - Token usage per job/model.  
   - Attach simple trace identifiers to AI jobs and workflow runs so that:
     - A single user action can be followed across model calls and internal steps.  
   - Provide at least one way to view or export these diagnostics (e.g. a debug UI panel or log-based trace view).

7. **AI UX in the editor (basic actions)**  
   - Command Palette actions:
     - â€œAsk about this documentâ€ (chat with context).  
     - â€œSummarize document.â€  
   - Inline actions:
     - â€œRewrite selectionâ€ for document text.  
   - All actions MUST:
     - Create AI jobs with the correct profile and capabilities.  
     - Execute via the Workflow Engine.  
     - Persist results back into documents through structured patches.  
     - Emit events into Flight Recorder; the corresponding jobs must appear in Job History and in metrics/traces.

8. **Governance hooks (Diary alignment)**  
   - Store enough metadata on jobs and workflows to later map them to Diary RIDs and clauses (activation, modes, gates).  
   - Enforce the invariant: **no silent AI edits**. Every AI mutation of user content MUST be traceable to a specific job and workflow run.  
   - Add basic Bootloader/Diary compliance checks to CI to prevent regressions in logging or observability.

9. **Dev experience and ADRs**  
   - One-command dev startup MUST include local model runtime (or a mock) and sample jobs.  
   - Create initial Architecture Decision Records (ADRs) for key choices:
     - Runtime selection.  
     - DB layout for jobs and Flight Recorder.  
     - Capability model shape.  

**Vertical slice (core loop)**  
- Start the app and open a sample document.  
- Select text and trigger â€œRewrite selectionâ€.  
- See the updated text in the document.  
- Open Job History and locate the corresponding job with correct status and metadata.  
- Inspect logs and traces that show the model call, workflow execution, and any errors.

**Key risks addressed in Phase 1**

- AI Job Model and Workflow Engine are too complex or too weak for real usage.  
- Observability (Flight Recorder, metrics, traces) is not wired end-to-end.  
- Capability and consent models are unclear or easily bypassed.  

**Acceptance criteria**

- For every AI action in the UI, a corresponding AI job and workflow run exists and can be inspected.  
- Flight Recorder shows a coherent timeline for at least the core loop.  
- Metrics and logs are sufficient to explain failures in the core loop without reading the entire codebase.  
- Bootloader/Diary checks for logging and non-silent edits pass in CI.

**Explicitly OUT of scope**

- Multiple LLM runtimes and sophisticated model routing.  
- Sheets engine beyond a minimal stub (tables can be represented, but no full formula engine yet).  
- Docling ingestion, ASR pipeline, connectors, plugin system.  
- Multi-user sync and CRDT.

---

### 7.6.4 Phase 2 â€” Ingestion & Shadow Workspace (Docling + RAG MVP)

**Goal**  
Make Handshake useful over **existing** files and unlock basic retrieval-augmented generation, reusing the existing AI Job, workflow, and observability stack. Maintain and extend debug surfaces for ingestion and retrieval.

**MUST deliver**

1. **Docling integration (mechanical ingestion)**  
   - Integrate Docling as described in Section 6.1.  
   - Implement the **Docling AI Job profile**:
     - Jobs for format detection, conversion, structure extraction, and error recovery.  
   - Support importing at least `.docx` and `.pdf` into internal document blocks.  
   - Log ingestion jobs and their states in Flight Recorder (including failures and retryable conditions).

2. **Shadow Workspace (index + graph)**  
   - Implement the Shadow Workspace as per Section 3:
     - Incremental parsing and chunking of documents.  
     - Embedding generation via a local model.  
     - Storage of embeddings and metadata in a local store.  
   - Provide a unified â€œSearch workspaceâ€ command in the UI using Shadow Workspace.  
   - Emit metrics for indexing operations and query counts; record search queries and result identifiers in Flight Recorder or a dedicated search log.

3. **RAG-aware AI jobs**  
   - New job kinds for:
     - â€œAnswer question using workspace documents.â€  
   - These jobs MUST:
     - Query Shadow Workspace for relevant chunks.  
     - Include retrieved context in prompts.  
     - Log retrieval steps and context (e.g. document IDs, snippet hashes) in Flight Recorder.  
   - Provide a debug view for at least one RAG action that shows:
     - Which documents/snippets were used.  
     - How they influenced the final answer (e.g. by showing context alongside the answer).

**Vertical slice**  
- Import a `.docx` or `.pdf` file.  
- Wait for ingestion to complete and open the resulting document.  
- Use â€œSearch workspaceâ€ to find content from the imported file.  
- Ask a question that should be answered from that content; see a RAG-backed answer and inspect the corresponding jobs and logs (ingestion + retrieval + answer job).

**Key risks addressed in Phase 2**

- Ingestion pipeline is too brittle or slow to be practical.  
- Shadow Workspace design is wrong or too hard to debug.  
- RAG behaviour is opaque (user cannot see why an answer was produced).

**Acceptance criteria**

- Ingestion jobs are visible and inspectable in Job History and Flight Recorder.  
- Shadow Workspace can be inspected via logs or a debug view (e.g. number of indexed documents, last index time).  
- For at least one RAG scenario, you can show which documents and snippets were used to produce an answer.

**Explicitly OUT of scope**

- Advanced knowledge graph visualization.  
- Complex retriever configuration UIs.  
- Non-text modalities beyond what Docling supports in MVP.

---

### 7.6.5 Phase 3 â€” ASR & Long-Form Capture

**Goal**  
Add lecture/meeting capture via ASR, using the same AI Job, workflow, and observability primitives. Debugging ASR behaviour must be possible from logs and UI.

**MUST deliver**

1. **ASR engine integration**  
   - Integrate an ASR engine (e.g. Whisper / whisper-rs) as described in Section 6.2.  
   - Support batch transcription for locally stored audio/video files.  
   - Log ASR runs with duration, model, and basic quality-related metrics (where available) into Flight Recorder or a dedicated ASR log.

2. **ASR AI Job profile**  
   - Implement the ASR profile:
     - `media_id`, time ranges, diarization flags, language configuration, provenance.  
   - ASR jobs MUST flow through the Workflow Engine and log into Flight Recorder like any other AI job.  
   - Expose ASR-specific status and errors (e.g. decoding failure, unsupported format) in Job History.

3. **Transcription UX**  
   - â€œTranscribe fileâ€ flow:
     - Drop audio/video into Handshake.  
     - See job progress and final transcript document with segments and timestamps.  
   - Transcripts MUST be regular workspace documents, subject to the same governance and editing rules as other documents.  
   - Provide at least one debug surface that shows:
     - Input file details.  
     - Chosen model and parameters.  
     - Any segmentation or diarization decisions, where applicable.

**Vertical slice**  
- Drop an audio file into Handshake.  
- Run transcription and see progress.  
- Open the resulting transcript document.  
- Inspect Job History and Flight Recorder entries for the ASR job and confirm model choice, status transitions, and any errors.

**Key risks addressed in Phase 3**

- ASR pipeline is unreliable or too opaque.  
- Long-form capture produces transcripts that are hard to relate back to source media.  
- ASR jobs are not easily distinguishable or debuggable compared to other jobs.

**Acceptance criteria**

- At least one realistic audio file can be ingested, transcribed, and inspected end-to-end.  
- ASR jobs appear clearly in Job History and can be filtered and inspected separately.  
- Logs and debug views provide enough information to reason about ASR failures or poor transcripts.

**Explicitly OUT of scope**

- Real-time streaming captions.  
- Fine-tuning workflows for ASR models.  
- Complex diarization and speaker management UIs.

---

### 7.6.6 Phase 4 â€” Collaboration & Extension Ecosystem

**Goal**  
Move from a single-user tool to a collaborative, extensible platform, while preserving and extending observability and debug tools.

**MUST deliver**

1. **Collaboration & sync**  
   - Integrate a CRDT library (e.g. Yjs) with the existing document and canvas model, as described in Section 7.3.  
   - Define and implement a sync topology (file-based or server).  
   - Ensure Workflow Engine, AI Jobs, and Flight Recorder behave correctly under concurrent edits:
     - Conflicts are visible and traceable.  
     - Job history clearly shows which user and which device triggered which actions.

2. **Multi-user semantics**  
   - Introduce an authentication/session model.  
   - Define how AI jobs behave when multiple users interact with the same artefacts (ownership, consent, capability scope, per-user audit trails).  
   - Extend debug tooling to:
     - Filter Flight Recorder and Job History by user, workspace, and device.  
     - Inspect collaborative sessions and their timelines.

3. **Plugin / extension system (initial)**  
   - Design an internal plugin API built on top of the AI Job Model and capability system.  
   - Expose safe extension points:
     - New workflow nodes.  
     - New AI Job profiles or capability profiles.  
   - Require that plugins:
     - Use the same logging, metrics, and Flight Recorder frameworks.  
     - Register their actions so they appear in Job History and traces.  
   - Prepare for external plugins by aligning with security and sandboxing constraints defined in Section 5.2.  

4. **Security and privacy hygiene**  
   - Document how logs, Flight Recorder data, and debug traces handle sensitive content.  
   - Provide at least basic controls for:
     - Clearing or rotating logs.  
     - Exporting/importing data safely.  

**Vertical slice**  
- Two users (or two devices) edit the same document using the chosen sync topology.  
- One user triggers an AI action that modifies the shared document.  
- Both users see the changes.  
- Job History and Flight Recorder show which user triggered the job, how it ran, and how it interacted with sync/CRDT.

**Key risks addressed in Phase 4**

- Collaboration behaviour is inconsistent or not auditable.  
- Plugins bypass governance, capabilities, or observability.  
- Logs and debug tools become unusable in multi-user scenarios.

**Acceptance criteria**

- Collaborative edits are correctly synced and traceable in logs.  
- AI jobs in collaborative sessions are correctly attributed to users/devices.  
- Plugins can register actions and appear in Job History without bypassing capabilities or Flight Recorder.  
- Minimal security/privacy documentation exists for logs and debug data.

---

**Key Takeaways**  
- The roadmap is **architecture-aligned and debug-first**: every phase explicitly requires health checks, structured logging, Flight Recorder integration, and at least one human-usable debug surface.  
- **Vertical slices** ensure each phase ends with a real, end-to-end scenario you can manually test, not just abstract infra.  
- Phases are used to **burn down risk**: stack stability in Phase 0, AI Jobs + workflows + observability in Phase 1, ingestion/RAG in Phase 2, ASR in Phase 3, and collaboration/plugins in Phase 4.  
- Cross-cutting concernsâ€”migrations, security/privacy of logs, dev experience, ADRsâ€”are included so they are not forgotten while focusing on features.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
---


# 8. Reference

## 8.1 Risk Assessment

**Why**  
Understanding risks upfront enables proactive mitigation. This section identifies key risks and their mitigation strategies.

**What**  
Risk matrix covering likelihood and impact, complexity ratings for each component, and mitigation strategies.

**Jargon**  
- **Scope Creep**: Uncontrolled expansion of project requirements.
- **Graceful Degradation**: System continues working (with reduced capability) when components fail.

---

### 8.1.1 Risk Matrix

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **Tauri webview issues** | Medium | High | Minimal Tauri role; test early on all platforms |
| **Local model performance** | Medium | Medium | Cloud fallback; smaller model options |
| **Complexity overwhelm** | High | High | Strict MVP scope; phases; hire help |
| **CRDT learning curve** | Medium | Medium | Use Yjs (proven); start with single-user |
| **Plugin security** | Low | High | Delay plugins; learn from existing models |
| **Scope creep** | High | High | Written MVP definition; say no to extras |

---

### 8.1.2 Complexity Ratings

| Component | Complexity | Notes |
|-----------|------------|-------|
| Tauri setup | âš ï¸ Medium | Some Rust knowledge needed |
| Block editor | âš ï¸ Medium | Tiptap helps a lot |
| AI orchestration | âš ï¸âš ï¸ High | Multi-model coordination is complex |
| Canvas | âš ï¸ Medium | Excalidraw does heavy lifting |
| Spreadsheets | âš ï¸ Medium | HyperFormula helps |
| CRDT sync | âš ï¸âš ï¸ High | Conceptually challenging |
| ComfyUI integration | âš ï¸ Medium | API-based, manageable |
| Plugin system | âš ï¸âš ï¸ High | Defer to post-MVP |

---

**Key Takeaways**  
- Highest risks: complexity overwhelm and scope creepâ€”mitigate with strict MVP scope.
- Tauri webview issues are medium risk but high impactâ€”test early on all platforms.
- AI orchestration and CRDT sync are the most complex components.
- Delay plugin system to post-MVP to reduce initial complexity.

---

## 8.2 Technology Stack Summary

**Why**  
A consolidated reference of all technologies enables quick lookup and ensures consistency across the project.

**What**  
Complete list of technologies organized by layer: Core Stack, Frontend Libraries, Backend Libraries, AI Models, DevOps Tools.

**Jargon**  
See individual technology entries in the Consolidated Glossary (Section 8.4).

---

### 8.2.1 Core Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Desktop Shell** | Tauri | Cross-platform wrapper |
| **Frontend** | React + TypeScript | User interface |
| **Backend** | Python (FastAPI) | API server, orchestration |
| **AI Runtime** | Ollama, ComfyUI | Model execution |
| **Storage** | File system + SQLite | Data persistence |
| **Sync** | Yjs (CRDT) | Collaboration |

---

### 8.2.2 Frontend Libraries

| Library | Purpose |
|---------|---------|
| Tiptap / BlockNote | Block-based editor |
| Excalidraw | Canvas/whiteboard |
| HyperFormula | Spreadsheet formulas |
| Wolf-Table | Spreadsheet UI |
| React Table / AG Grid | Data grid views |
| React Beautiful DnD | Drag and drop |

---

### 8.2.3 Backend Libraries

| Library | Purpose |
|---------|---------|
| FastAPI | HTTP API server |
| AutoGen or LangGraph | Agent orchestration |
| Ollama API | Local LLM access |
| ComfyUI API | Image generation |
| Pydantic | Data validation |
| SQLAlchemy | SQLite access |

---

### 8.2.4 AI Models

| Model | Purpose | Size |
|-------|---------|------|
| Llama 3 13B | General text | ~14GB |
| Code Llama 13B | Code generation | ~14GB |
| Mistral 7B | Fast responses | ~8GB |
| SDXL 1.0 | Image generation | ~10GB |

---

### 8.2.5 DevOps Tools

| Tool | Purpose |
|------|---------|
| GitHub Actions | CI/CD |
| Ruff, Black, isort | Python linting/formatting |
| ESLint, Prettier | TypeScript linting/formatting |
| pytest | Python testing |
| vitest | TypeScript testing |
| n8n (optional) | Workflow automation |

---

**Key Takeaways**  
- Core stack: Tauri + React + Python + Ollama + SQLite + Yjs.
- Frontend: Tiptap for editing, Excalidraw for canvas, HyperFormula for spreadsheets.
- Backend: FastAPI + AutoGen/LangGraph for orchestration.
- AI: Llama 3, Code Llama, Mistral, SDXL for different tasks.

---
## v2.0 Complete Technology Stack (Frozen Diagram)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPLETE TECHNOLOGY STACK                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DESKTOP FRAMEWORK                                           â”‚
â”‚  â”œâ”€â”€ Primary: Tauri (Rust) + React/Vue                      â”‚
â”‚  â””â”€â”€ Alternative: Electron (if more JS ecosystem needed)    â”‚
â”‚                                                              â”‚
â”‚  LLM INFRASTRUCTURE                                          â”‚
â”‚  â”œâ”€â”€ Runtime: Ollama (dev) + vLLM (production)              â”‚
â”‚  â”œâ”€â”€ Models: Mistral-7B, CodeLlama-7B, Llama2-13B           â”‚
â”‚  â””â”€â”€ Images: ComfyUI + SDXL                                 â”‚
â”‚                                                              â”‚
â”‚  DATA LAYER                                                  â”‚
â”‚  â”œâ”€â”€ CRDT: Yjs (or Loro for Rust)                           â”‚
â”‚  â”œâ”€â”€ Database: SQLite                                        â”‚
â”‚  â””â”€â”€ Sync: Yjs WebSocket provider (later)                   â”‚
â”‚                                                              â”‚
â”‚  PLUGIN SYSTEM                                               â”‚
â”‚  â”œâ”€â”€ Sandbox: WASM (Wasmtime)                               â”‚
â”‚  â”œâ”€â”€ Language: AssemblyScript/Rust â†’ WASM                   â”‚
â”‚  â””â”€â”€ Permissions: Manifest-based capability model           â”‚
â”‚                                                              â”‚
â”‚  OBSERVABILITY                                               â”‚
â”‚  â”œâ”€â”€ Telemetry: OpenTelemetry                               â”‚
â”‚  â”œâ”€â”€ Metrics: Prometheus                                    â”‚
â”‚  â”œâ”€â”€ Visualization: Grafana                                 â”‚
â”‚  â””â”€â”€ Traces: Jaeger or Grafana Tempo                        â”‚
â”‚                                                              â”‚
â”‚  LANGUAGES                                                   â”‚
â”‚  â”œâ”€â”€ Backend: Python (orchestrator) + Rust (Tauri)          â”‚
â”‚  â”œâ”€â”€ Frontend: TypeScript + React/Vue                       â”‚
â”‚  â””â”€â”€ Plugins: AssemblyScript â†’ WASM                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 8.3 Gap Analysis & Open Questions

**Why**  
Acknowledging what the research doesn't cover prevents false confidence and highlights areas needing further investigation.

**What**  
Documents research gaps (UI/UX, authentication, business model, fine-tuning, Windows-specific), open technical questions, and unresolved issues requiring further work.

**Jargon**  
- **RAG (Retrieval Augmented Generation)**: Technique for handling long documents by retrieving relevant chunks.
- **Fine-tuning**: Training a pre-trained model on specific data to improve performance on particular tasks.

---

### 8.3.1 What the Research DOESN'T Cover

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RESEARCH GAPS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  USER INTERFACE                                              â”‚
â”‚  â€¢ No detailed UI/UX designs                                â”‚
â”‚  â€¢ No accessibility considerations                          â”‚
â”‚  â€¢ No mobile/responsive strategy                            â”‚
â”‚  Action: Need separate UI design research                   â”‚
â”‚                                                              â”‚
â”‚  AUTHENTICATION & MULTI-USER                                 â”‚
â”‚  â€¢ No user account system design                            â”‚
â”‚  â€¢ No team/sharing model                                    â”‚
â”‚  â€¢ No encryption for sensitive data                         â”‚
â”‚  Action: Research if/when adding cloud sync                 â”‚
â”‚                                                              â”‚
â”‚  BUSINESS MODEL                                              â”‚
â”‚  â€¢ No pricing strategy                                      â”‚
â”‚  â€¢ No marketplace economics for plugins                     â”‚
â”‚  Action: Business planning separate from technical          â”‚
â”‚                                                              â”‚
â”‚  SPECIFIC MODEL FINE-TUNING                                  â”‚
â”‚  â€¢ Research covers pre-trained models only                  â”‚
â”‚  â€¢ No guidance on fine-tuning for specific use cases        â”‚
â”‚  Action: May need if default models insufficient            â”‚
â”‚                                                              â”‚
â”‚  WINDOWS-SPECIFIC ISSUES                                     â”‚
â”‚  â€¢ Limited coverage of Windows sandboxing options           â”‚
â”‚  â€¢ No Windows installer/distribution guidance               â”‚
â”‚  Action: Platform-specific research needed                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8.3.2 Open Technical Questions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPEN QUESTIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Tauri vs Electron final decision?                       â”‚
â”‚     â€¢ Tauri: Smaller, faster, Rust backend                  â”‚
â”‚     â€¢ Electron: More mature, larger ecosystem               â”‚
â”‚     â†’ Recommendation: Start with Tauri, reconsider if       â”‚
â”‚       ecosystem limitations become blocking                 â”‚
â”‚                                                              â”‚
â”‚  2. How to handle very long documents?                      â”‚
â”‚     â€¢ Context windows are limited (4K-8K tokens)            â”‚
â”‚     â€¢ Options: Chunking, summarization, RAG                 â”‚
â”‚     â†’ Need: RAG (Retrieval Augmented Generation) research   â”‚
â”‚                                                              â”‚
â”‚  3. Offline-first sync strategy?                            â”‚
â”‚     â€¢ File sync (OneDrive/Dropbox) simple but limited       â”‚
â”‚     â€¢ Custom sync server more powerful but complex          â”‚
â”‚     â†’ Recommendation: Start with file sync, add server      â”‚
â”‚       when multi-user collaboration is priority             â”‚
â”‚                                                              â”‚
â”‚  4. Plugin language choice?                                 â”‚
â”‚     â€¢ WASM requires compilation (barrier to entry)          â”‚
â”‚     â€¢ JavaScript simpler but harder to sandbox              â”‚
â”‚     â†’ Recommendation: Support bothâ€”sandboxed JS for         â”‚
â”‚       simple plugins, WASM for advanced/untrusted           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8.3.3 Unresolved Issues

| Question | Why It Matters | Suggested Action |
|----------|---------------|------------------|
| **Exact Tauri version?** | v1 vs v2 have API differences | Check latest stable, test early |
| **Python bundling strategy?** | How to package Python with Tauri | Research PyInstaller + Tauri sidecar |
| **Model download UX?** | How do users get 10GB+ models? | Design in-app download + progress UI |
| **License audit** | Some libraries have complex licenses | Full audit before production |
| **Performance benchmarks** | Real numbers on target hardware | Build prototype, measure |

---

### 8.3.4 Not Covered (Future Research)

The documents don't cover:
- Mobile versions (iOS/Android)
- Web version (browser-only)
- Enterprise features (SSO, audit logs)
- Monetization strategy
- Analytics/telemetry approach
- Accessibility (a11y) requirements

---

### 8.3.5 Immediate Next Steps

1. **Set up monorepo** with Tauri + React + Python structure
2. **Validate Tauri** on Windows, Mac, Linux
3. **Prototype IPC** between React and Python
4. **Test Ollama** integration
5. **Build health check** command

---

**Key Takeaways**  
- Research gaps exist in UI/UX, authentication, business model, fine-tuning, and Windows-specific issues.
- Key open questions: long document handling (needs RAG research), plugin language choice.
- Immediate next steps: monorepo setup, Tauri validation, IPC prototyping, Ollama testing.

---

## 8.4 Consolidated Glossary

**Why**  
A unified glossary ensures consistent terminology across the project and serves as quick reference.

**What**  
Alphabetical list of all technical terms defined throughout this specification.

---

| Term | Definition |
|------|------------|
| **access_mode** | Job field specifying consent level: `analysis_only` (read), `preview_only` (propose), `apply_scoped` (apply); see (AI Job Model, Section 2.6.6) |
| **Agent** | An AI model configured for a specific role with the ability to take actions |
| **AI Job** | Durable, capability-scoped unit of AI work executed by the workflow engine; see (AI Job Model, Section 2.6.6) |
| **AI Job Profile** | Artefact-specific extension of the core AI job model (e.g., Docs Profile, ASR Profile); see (AI Job Model, Section 2.6.6).6 |
| **API** | Application Programming Interfaceâ€”how programs communicate with each other |
| **Automerge** | A CRDT library that stores full history, good for version tracking but higher memory usage |
| **AutoGen** | Microsoft's multi-agent conversation framework |
| **Batching** | Processing multiple requests together for efficiency |
| **Block-Based Editor** | Editor where content is made of stackable blocks instead of continuous text |
| **Capability Model** | Security pattern where code receives explicit permission tokens for specific resources |
| **Chromium** | Open-source browser engine that Chrome is built on |
| **ComfyUI** | Node-based visual tool for Stable Diffusion image generation |
| **Context Window** | How many tokens an LLM can "see" at onceâ€”its working memory |
| **Continuous Batching** | Advanced batching that dynamically adds/removes requests mid-generation |
| **CRDT** | Conflict-free Replicated Data Typeâ€”enables automatic merge of concurrent edits |
| **CUDA** | NVIDIA's technology for running computations on GPUs |
| **Default Deny** | Security stance where nothing is permitted unless explicitly granted |
| **Desktop Shell** | Program that wraps web code to run as a native desktop application |
| **Electron** | Popular desktop shell that bundles Chromium and Node.js |
| **EntityRef** | ID-based reference into workspace artefacts used by AI jobs to specify scope; see (AI Job Model, Section 2.6.6).2.3 |
| **GGUF** | File format for quantized AI models, used by llama.cpp and Ollama |
| **Golden Test Suite** | Set of representative prompts with expected properties to verify |
| **GPU** | Graphics Processing Unitâ€”hardware that runs AI models very fast |
| **Hot Model** | A model kept loaded in VRAM for instant response |
| **HyperFormula** | Open-source spreadsheet formula engine with 400+ functions |
| **Inference** | Using a trained AI model to generate outputs (vs training which creates the model) |
| **IPC** | Inter-Process Communicationâ€”how different parts of an app talk to each other |
| **KV Cache** | Key-Value cacheâ€”memory used to store conversation context during inference |
| **LangGraph** | LangChain's graph-based agent orchestration framework |
| **Langfuse** | Open-source LLM observability platform |
| **layer_scope** | Job field specifying which content layers (raw, derived, display) may be read/written; see (AI Job Model, Section 2.6.6) |
| **Lead/Worker Pattern** | Smart model plans, simpler models execute |
| **LLM** | Large Language Modelâ€”AI trained on text to understand and generate language |
| **LLM-as-Judge** | Using another LLM to rate output quality on criteria |
| **Local-First** | Architecture where data lives primarily on user's device, not in the cloud |
| **Loro** | A new CRDT library with full history and movable trees, written in Rust |
| **Manifest** | Configuration file declaring a plugin's metadata, permissions, and capabilities |
| **Monorepo** | Single repository containing multiple related projects |
| **OAuth2** | Standard protocol for secure third-party authorization |
| **Observability** | Ability to understand internal system state from external outputs |
| **Ollama** | Easy-to-use local LLM runner |
| **On-Demand Model** | A model loaded only when specifically needed |
| **OpenTelemetry (OTel)** | Industry standard for collecting metrics, traces, and logs |
| **Orchestrator** | Code that coordinates multiple AI models to work together |
| **PagedAttention** | vLLM's memory optimization technique for efficient KV cache management |
| **Parameters** | The "knobs" inside an AI model (more = smarter but heavier) |
| **Plugin Manifest** | JSON file declaring plugin metadata, permissions, and contributions |
| **PlannedOperation** | Typed struct describing an intended workspace mutation in an AI job (e.g., `insert_block`, `apply_formula`); see (AI Job Model, Section 2.6.6).2.4 |
| **Prometheus** | Time-series database commonly used for metrics |
| **Property-Based Test** | Test checking structural properties rather than exact content |
| **Pyodide** | Full Python interpreter compiled to WASM |
| **Q4/Q5/Q8** | Quantization levelsâ€”lower numbers mean smaller size but slightly lower quality |
| **Quantization** | Shrinking AI models to use less memory by reducing number precision |
| **RAG** | Retrieval Augmented Generationâ€”technique for handling long documents |
| **REST API** | Common style for web APIs using HTTP methods |
| **Runtime** | Software that loads and executes AI models |
| **Sandbox** | Isolated environment where untrusted code can run safely |
| **safety_mode** | Job field specifying behavior preset: `strict`, `normal`, or `experimental`; see (AI Job Model, Section 2.6.6) |
| **SDXL** | Stable Diffusion XLâ€”high-quality image generation model |
| **Sidecar File** | Small metadata file that accompanies a main file |
| **Slash Commands** | Type "/" to access insertion menu in editors |
| **Span** | A single unit of work within a trace |
| **SQLite** | Lightweight database contained in a single file |
| **Streaming** | Sending response tokens one at a time as they're generated |
| **Tauri** | Lightweight desktop shell using Rust and system webview |
| **TGI** | Text Generation Inferenceâ€”HuggingFace's production LLM server |
| **Tiptap** | Extensible rich text editor framework built on ProseMirror |
| **Token** | A chunk of text (roughly Â¾ of a word) that LLMs process |
| **Trace** | End-to-end record of a request's path through the system |
| **vLLM** | High-performance LLM inference engine optimized for throughput |
| **VRAM** | Video RAMâ€”memory on graphics card where AI models run |
| **WASM (WebAssembly)** | Binary format that runs in a secure sandbox |
| **WebSocket** | Protocol for real-time, two-way communication |
| **Yjs** | Popular JavaScript CRDT library with excellent editor integrations |

---

**Key Takeaways**  
- This glossary provides definitions for all technical terms used throughout the specification.
- Terms are organized alphabetically for quick lookup.
- Cross-reference with relevant sections for deeper understanding.

---

## 8.5 Sources Referenced

**Why**  
Documenting sources enables verification, further research, and acknowledgment of the research foundation.

**What**  
Lists all source documents that were synthesized into this unified specification.

---

### 8.5.1 Source Documents

This document consolidates research from the following sources:

#### 8.5.1.1 Part II (LLM Infrastructure) Sources
1. **LLM Inference Runtimes** (8 pages) â€” Runtime comparison, model candidates, image generation, scheduling patterns
2. **Inference Runtimes** (7 pages) â€” Runtime comparison, model selection by role, GPU bottlenecks, recommendations
3. **Benchmark Harness Design** (5 pages) â€” Modular Python benchmark architecture, adapters, scenarios, reporting

#### 8.5.1.2 Part III (Data Architecture) Sources
4. **Local-First Data and Sync Architecture** (9 pages) â€” CRDT libraries, database patterns, sync topologies, conflict resolution UX

#### 8.5.1.3 Part IV (Plugin System) Sources
5. **Extension Platforms: Architectural Overview** (10 pages) â€” Plugin system analysis (VS Code, Obsidian, Figma, browsers), proposed architecture
6. **Sandboxing Options for Untrusted Code** (12 pages) â€” WASM, Pyodide, OS sandboxing, permission models, security architecture

#### 8.5.1.4 Part V (Observability & Testing) Sources
7. **AI Observability and Evaluation** (10 pages) â€” Logging, metrics, privacy, evaluation methods, multi-agent tracing, phased rollout

#### 8.5.1.5 Part VII (Consolidated Architecture) Sources
8. **Handshake_Project.pdf** (9 pages) â€” Core specification: multi-model orchestration, UI frameworks, Google API integration, ComfyUI, architecture overview
9. **Model_Strategy_and_Tooling_Guide.pdf** (4 pages) â€” AI assistant usage strategy, Codex vs GPT-4/Claude roles, n8n evaluation
10. **Reference_App_Deep_Dive_Local-First_Open_Workspace_Tools.pdf** (7 pages) â€” Technical analysis of AppFlowy, AFFiNE, Anytype, Logseq, Obsidian, Joplin
11. **Tauri_Electron_Decision.pdf** (4 pages) â€” Framework comparison, consensus from multiple AI advisors recommending Tauri
12. **Project_Health_Hygiene_Guide.pdf** (7 pages) â€” Codebase standards, testing, CI/CD, logging, AI-friendly practices
13. **Development_Roadmap_Draft.pdf** (7 pages) â€” Phase planning, implementation order, testing strategy, deployment
14. **Notion_vs_Milanote_vs_Excel_Feature_Comparison.pdf** (4 pages) â€” Target app analysis, orchestration framework comparison, local model recommendations
#### 8.5.1.6 Part VIII (Embedded Protocol Sources)

15. **Handshake Docs & Sheets AI Integration Protocol (v0.5-draft)** (Markdown spec) â€” Defines AI jobs over documents and sheets, stable IDs and entity references, provenance fields, job configuration, observability, safety rules, and a threat model; now fully integrated as Section 2.5.10 of this specification.

16. **AI Job Model v0.1 (Design Note)** â€” Defines the global AI job model, core schema, lifecycle, workflow engine integration, and profile extension pattern; integrated as Section 2.6.6 with profiles in (Docs & Sheets Profile section), (Docling Profile section), (ASR Profile section).
---

### 8.5.2 Document Statistics

- **Total source items:** 16 (14 external documents + 2 design notes/protocol specs)
- **Total page-equivalent:** ~110+ pages
- **Total merged sections:** 33 sections
- **Estimated read time:** ~130 minutes for complete document

---

### 8.5.3 Quick Reference Guide

**For quick lookup:**
- [Section 1.8 - Introduction](#18-introduction) â€” Project overview, 5 min
- [Section 8.2 - Technology Stack Summary](#82-technology-stack-summary) â€” Quick reference
- [Section 7.6 - Development Roadmap](#76-development-roadmap) â€” What to build when
- [Section 8.4 - Consolidated Glossary](#84-consolidated-glossary) â€” Term definitions

**For DECISION POINTs:**
Search for "DECISION POINT" to find all major technical choices with recommendations.

**For Implementation:**
Follow the roadmap in Section 7.6 and refer to specific technical sections as needed.

---

**Key Takeaways**  
- This specification synthesizes 14 source documents totaling ~100+ pages.
- Sources cover infrastructure, data architecture, plugins, observability, and consolidated architecture.
- Use quick reference guide for navigation; search "DECISION POINT" for key choices.

---

## 8.6 Appendices

**Why**  
Appendices provide supplementary reference material including foundation concepts for newcomers, detailed architecture decisions, comparison tables, benchmark data, and works cited that support the main specification.

**What**  
Contains Foundation Concepts (beginner explainers), Architecture Decisions (detailed rationale), Plugin System design, Docling/ASR comparison tables, and works cited.

---

### 8.6.1 Foundation Concepts

*This appendix provides beginner-friendly explanations of core concepts for readers new to desktop application development or AI systems.*

#### 8.6.1.1 Foundation Concepts Overview

Before diving into specific technical decisions, let's establish foundational understanding of the core concepts that appear throughout this document.

---

#### 8.6.1.2 What is a Desktop Application Shell?

**Prerequisites:** None - foundational  
**Related to:** Section 8.6 (Appendices)  
**Implements:** Understanding architecture choices  
**Read time:** ~4 minutes

**A "shell" is the container that turns web code into a desktop application. It's the bridge between your web-based user interface and the operating system.**

---

#### 8.6.1.3 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Desktop Shell** | A program that wraps website-style code so it runs as a regular desktop app (with window controls, file access, etc.) | We need to choose between Tauri and Electron as our shell |
| **Electron** | The most popular shell; used by VS Code, Slack, Discord. Bundles a complete Chrome browser inside your app | Higher memory usage but battle-tested and familiar |
| **Tauri** | A newer, lighter shell using Rust. Uses the operating system's built-in browser instead of bundling one | Much lower memory usageâ€”critical when AI models need that RAM |
| **WebView** | A "browser window without the browser"â€”just the part that displays web pages | Tauri uses the system's webview; Electron bundles its own |
| **IPC (Inter-Process Communication)** | How different parts of a program talk to each other | How the UI will communicate with the Python AI backend |

---

#### 8.6.1.4 The Mental Model

Think of building a desktop app like building a food truck:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DESKTOP SHELL                   â”‚
â”‚         (The food truck itself)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           YOUR WEB APP               â”‚    â”‚
â”‚  â”‚      (The kitchen equipment)         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚    React + TypeScript       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   (The menu & recipes)      â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â”‚                         â”‚
â”‚                    â–¼                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚     Operating System        â”‚          â”‚
â”‚    â”‚   (Where the truck parks)   â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Electron** = A food truck that brings its own generator, water supply, and waste systemâ€”self-contained but heavy.

**Tauri** = A food truck that plugs into the venue's electricity and plumbingâ€”lighter but depends on what's available.

---

#### 8.6.1.5 Why This Matters for Handshake

â•â•â• CORE CONCEPT â•â•â•

> Every megabyte of RAM the shell uses is a megabyte NOT available for AI models.
> 
> - Electron idle: ~150-300 MB RAM
> - Tauri idle: ~10-50 MB RAM
> 
> That 200+ MB difference could mean running a larger AI model or faster response times.

---

#### 8.6.1.13 Key Takeaways

- âœ“ Local-first = your data lives on your device primarily
- âœ“ Critical for privacy when AI models access your documents
- âœ“ Enables offline work and eliminates API costs
- âœ“ CRDTs enable collaboration without central servers
- âœ“ You can still sync to cloudâ€”it's just optional

---

#### 8.6.1.14 What are AI Models and How Do They Run Locally?

**Prerequisites:** None - foundational  
**Related to:** Section 4 (LLM Infrastructure)  
**Implements:** Understanding AI integration approach  
**Read time:** ~6 minutes

**An AI model is a very large mathematical formula that takes in text (or images) and produces intelligent-seeming responses. "Running locally" means this formula executes on YOUR computer, not a company's servers.**

---

#### 8.6.1.15 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **LLM (Large Language Model)** | An AI trained on massive text to understand and generate language. ChatGPT is an LLM. | The "brain" that will write, summarize, and reason |
| **Parameters** | The "knobs" inside the AI model. More parameters = smarter but heavier. "7B" = 7 billion parameters | Determines which models fit on your hardware |
| **VRAM** | Video RAMâ€”memory on your graphics card | Where AI models live during use; RTX 3090 has 24GB |
| **Inference** | The AI actually doing its job (generating a response) | What happens when you ask the AI something |
| **Quantization** | Shrinking a model to fit in less memory (with some quality loss) | How we fit big models on consumer hardware |
| **GGUF** | A file format for quantized models | The format we'll download models in |

---

#### 8.6.1.16 How Big Are These Models?

```
Model Size vs. Quality vs. Hardware Requirements

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 70B (GPT-4 class)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ 140GB+ â”‚
â”‚  - Smartest, needs multiple GPUs or cloud    â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 34B (Very Good)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚ ~70GB  â”‚
â”‚  - Excellent quality, pushes 3090 limits     â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 13B (Good)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 â”‚ ~26GB  â”‚
â”‚  - Great balance, fits 3090 with room       â”‚ â† Sweetâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7B (Decent)        â–ˆâ–ˆâ–ˆâ–ˆ                     â”‚ ~14GB  â”‚
â”‚  - Fast, leaves room for other models        â”‚  Spot  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3B (Basic)         â–ˆâ–ˆ                       â”‚ ~6GB   â”‚
â”‚  - Quick tasks, limited capability           â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.1.17 The Model Zoo

Handshake needs DIFFERENT models for DIFFERENT tasks:

| Task | Model Type | Example | Size |
|------|-----------|---------|------|
| Writing & Reasoning | General LLM | Llama 3, Mistral | 7-13B |
| Code Generation | Code-specialized | Code Llama, StarCoder | 7-15B |
| Image Generation | Diffusion Model | SDXL | ~3B |
| Task Planning | Reasoning LLM | GPT-OSS-20B | 20B |

â•â•â• CORE CONCEPT â•â•â•

> **You won't run all models simultaneously.** The orchestrator loads/unloads models based on what's needed. The 3090 has 24GB; a 13B model uses ~14GB quantized, leaving 10GB for SDXL image generation.

---

#### 8.6.1.18 Local vs. Cloud AI

```
                    LOCAL                     CLOUD (API)
                      â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ âœ“ Private - data stays home   â”‚  â”‚ âœ— Data sent to  â”‚
    â”‚ âœ“ Free after download         â”‚  â”‚   company       â”‚
    â”‚ âœ“ Works offline               â”‚  â”‚ âœ— Per-request   â”‚
    â”‚ âœ— Limited by your hardware    â”‚  â”‚   cost          â”‚
    â”‚ âœ— Slower than cloud GPUs      â”‚  â”‚ âœ— Needs internetâ”‚
    â”‚                               â”‚  â”‚ âœ“ Latest models â”‚
    â”‚ GOOD FOR: Frequent,           â”‚  â”‚ âœ“ Most powerful â”‚
    â”‚ routine tasks                 â”‚  â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ GOOD FOR: Hard  â”‚
                                       â”‚ tasks, fallback â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.1.19 How It Actually Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR COMPUTER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PYTHON BACKEND                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚         MODEL RUNTIME                    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  (vLLM, Ollama, or llama.cpp)           â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                                          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”‚Llama 3â”‚  â”‚ Code  â”‚  â”‚ SDXL  â”‚       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”‚ 13B   â”‚  â”‚ Llama â”‚  â”‚       â”‚       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚        â”‚          â”‚          â”‚          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                   â”‚                     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚           GPU (RTX 3090)                â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                     â”‚                           â”‚    â”‚
â”‚  â”‚           Orchestrator (AutoGen/LangGraph)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚  HTTP/WebSocket   â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                        â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              TAURI SHELL + REACT UI              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.1.26 Key Takeaways

- âœ“ Different AI models excel at different tasks
- âœ“ An "orchestrator" coordinates which model handles what
- âœ“ The lead/worker pattern: smart model plans, simple models execute
- âœ“ This approach balances quality, cost, and speed
- âœ“ All coordination happens in the Python backend

**See Also:** [Section 7.2 - Multi-Agent Orchestration](#72-multi-agent-orchestration)

---
---

### 8.6.2 Architecture Decisions

*This appendix provides detailed rationale behind key architecture choices including desktop shell selection, system architecture, and data architecture.*

#### 8.6.2.1 Architecture Decisions Overview

This section covers the major architectural choices for Project Handshake, based on research and multi-source analysis.

---

#### 8.6.2.2 Desktop Shell: Tauri vs Electron

**Prerequisites:** Section 8.6 (Appendices)  
**Related to:** Section 2.1 (High-Level Architecture)  
**Implements:** Core technology choice  
**Read time:** ~7 minutes

**This section explains why Tauri was chosen over Electron as the desktop shell, based on consensus from multiple AI advisors and research documents.**

---

#### 8.6.2.3 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Chromium** | The open-source browser that Chrome is built on | Electron bundles this; it's why Electron apps are large |
| **Rust** | A programming language focused on speed and safety | Tauri's backend is written in Rust |
| **System WebView** | The browser component already on your computer | Tauri uses this instead of bundling Chromium |
| **Binary Size** | How big the app installer is | Tauri: ~10-30MB; Electron: ~100-200MB |
| **Memory Footprint** | RAM used when app is running | Critical when AI models need that RAM |

---

#### 8.6.2.4 The Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Desktop application shell          â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Electron (used by VS Code, Slack, Discord)              â”‚
â”‚   â€¢ Tauri (newer, used by some AI apps)                     â”‚
â”‚   â€¢ Flutter (AppFlowy uses this - different paradigm)       â”‚
â”‚                                                              â”‚
â”‚ Recommendation: TAURI                                        â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ 90% less memory usage (crucial for AI models)           â”‚
â”‚   â€¢ Smaller install size                                     â”‚
â”‚   â€¢ Better security model for plugins                        â”‚
â”‚   â€¢ Python backend means shell is "just a wrapper"          â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Smaller ecosystem than Electron                          â”‚
â”‚   â€¢ Rust knowledge needed for advanced shell features       â”‚
â”‚   â€¢ Some webview quirks across operating systems            â”‚
â”‚   â€¢ AFFiNE actually switched FROM Tauri TO Electron         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.2.5 Head-to-Head Comparison

| Factor | Electron | Tauri | Winner for Handshake |
|--------|----------|-------|---------------------|
| **Memory at idle** | 150-300 MB | 10-50 MB | âš¡ **Tauri** |
| **Install size** | 100-200 MB | 10-30 MB | **Tauri** |
| **Startup time** | 1-2 seconds | Sub-second | **Tauri** |
| **Ecosystem maturity** | Excellent | Growing | Electron |
| **Documentation** | Extensive | Good | Electron |
| **Security model** | Permissive | Deny-by-default | âš¡ **Tauri** |
| **Cross-platform consistency** | Very consistent | Some quirks | Electron |
| **Node.js integration** | Built-in | Not applicable | Electron |
| **Rust backend** | Not applicable | Built-in | Context-dependent |

---

#### 8.6.2.6 Why Memory Matters So Much

â•â•â• CORE CONCEPT â•â•â•

```
Available GPU Memory (RTX 3090): 24 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WITH ELECTRON (300MB shell overhead):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚  LLM Model (14GB)          â”‚  SDXL(~8GB)  â”‚
â”‚                            â”‚  Cramped!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
System RAM also constrained for model loading

WITH TAURI (30MB shell overhead):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚  LLM Model (14GB)          â”‚  SDXL (10GB) â”‚
â”‚                            â”‚  Comfortable â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
270MB more RAM available for models/context
```

---

#### 8.6.2.7 The Research Consensus

Three independent analyses (GPT-4, Claude, and Gemini) were asked to evaluate this decision. **All three recommended Tauri** for the following reasons:

ğŸ“Œ **Key Points from Multi-AI Analysis:**

1. **Resource Efficiency Under AI Load**
   > "Every megabyte of RAM you save in the shell is headroom for bigger models, more context windows, and smoother SDXL runs."

2. **Architecture Alignment**
   > "Your backend is Python, not Node. The hard logic is not written in Rust; it is in Python and TypeScript."

3. **Long-Term Product Vision**
   > "This is not a tiny helper tool; it is your primary local-first, multi-model AI workspace."

4. **Security for Plugins**
   > "Tauri has a stricter, deny-by-default permission model, which makes it safer to load third-party code."

---

### âš ï¸ Risk: AFFiNE's Tauri-to-Electron Switch

One research document notes that AFFiNE, a similar local-first workspace app, **switched FROM Tauri BACK to Electron** due to webview limitations on macOS.

**Mitigation strategies:**
- Test extensively on all target platforms early
- Keep Tauri shell responsibilities minimal (just window management and IPC)
- Design the architecture so a shell swap is possible if absolutely necessary
- Monitor Tauri's development and webview improvements

---

#### 8.6.2.8 What Tauri Actually Does in This Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TAURI'S RESPONSIBILITIES                  â”‚
â”‚                    (Keep this list SHORT)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Create application window                                â”‚
â”‚  âœ“ Load the React UI                                        â”‚
â”‚  âœ“ Spawn Python backend process                             â”‚
â”‚  âœ“ Handle file system access (with permissions)             â”‚
â”‚  âœ“ Manage window state (minimize, maximize, etc.)           â”‚
â”‚  âœ“ Surface system metrics (GPU usage, memory)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    NOT TAURI'S JOB                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ— AI orchestration (Python does this)                      â”‚
â”‚  âœ— Data processing (Python/TypeScript)                      â”‚
â”‚  âœ— Business logic (React/Python)                            â”‚
â”‚  âœ— Model management (Python backend)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¡ **Tip:** Think of Tauri as a "thin wrapper"â€”it should do as little as possible. Complex logic stays in Python and TypeScript where iteration is easier.

---

#### 8.6.2.9 Key Takeaways

- âœ“ **Decision: Use Tauri** as the desktop shell
- âœ“ Primary reason: Memory efficiency for AI models
- âœ“ Secondary reasons: Security model, smaller installs, faster startup
- âœ“ Risk acknowledged: AFFiNE switched away; we mitigate by keeping Tauri's role minimal
- âœ“ Frontend code (React/TypeScript) works identically in both shells
- âœ“ If issues arise, shell swap is possible without rewriting business logic

**See Also:** [Section 8.6 - Overall System Architecture](#21-high-level-architecture)

---

#### 8.6.2.10 Overall System Architecture

**Prerequisites:** Section 8.6 (Foundation Concepts), Section 8.6 (Appendices)  
**Related to:** All implementation sections  
**Implements:** System blueprint  
**Read time:** ~8 minutes

**This section presents the complete system architecture: how all the pieces connect and communicate.**

---

#### 8.6.v02.12 Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Frontend** | The part users see and interact with (buttons, text, etc.) | React/TypeScript in the Tauri window |
| **Backend** | The "behind the scenes" code that does heavy lifting | Python: AI, file processing, orchestration |
| **API** | A set of "commands" one program can send to another | How frontend talks to backend |
| **REST API** | A common style for APIs using web requests (GET, POST, etc.) | Simple, well-understood pattern |
| **WebSocket** | A persistent connection for real-time, two-way communication | For streaming AI responses |
| **Monorepo** | One repository containing multiple related projects | Frontend and backend code together |
| **Microservices** | Breaking an app into separate, independent services | Each AI model could be its own service |

---

#### 8.6.2.12 The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER'S COMPUTER                                â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                        TAURI SHELL                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚                    REACT FRONTEND                          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚                                                            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚ Document â”‚   â”‚  Canvas  â”‚   â”‚  Sheets  â”‚   Â·Â·Â·       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚  Editor  â”‚   â”‚  Board   â”‚   â”‚  Grid    â”‚             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚(Tiptap)  â”‚   â”‚(Excali)  â”‚   â”‚(Hyper)   â”‚             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚                                                            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚              FILE TREE SIDEBAR                      â”‚  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚     (Workspace Navigator)                           â”‚  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                       â”‚
â”‚                    HTTP/WebSocket (localhost)                           â”‚
â”‚                                  â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      PYTHON BACKEND                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚                   ORCHESTRATOR                          â”‚    â”‚    â”‚
â”‚  â”‚   â”‚              (AutoGen or LangGraph)                     â”‚    â”‚    â”‚
â”‚  â”‚   â”‚                                                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚   Planner   â”‚   â”‚   Writer    â”‚   â”‚   Coder     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚   Agent     â”‚   â”‚   Agent     â”‚   â”‚   Agent     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                â”‚                                 â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚                  MODEL RUNTIMES                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚                                                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚  Ollama â”‚   â”‚  vLLM   â”‚   â”‚ComfyUI  â”‚   â”‚Cloud  â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚  (LLMs) â”‚   â”‚  (LLMs) â”‚   â”‚ (SDXL)  â”‚   â”‚Fallbckâ”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                â”‚                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                   â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     LOCAL FILE SYSTEM                             â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚   /Handshake/                                                    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ workspaces/                                                â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€ my-project/                                           â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ notes/           (Markdown files)                 â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ canvas/          (JSON board data)                â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ sheets/          (CSV/JSON data)                  â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ images/          (Generated + uploaded)           â”‚    â”‚
â”‚  â”‚   â”‚       â””â”€â”€ .handshake/      (Metadata, CRDT state)          â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ models/                  (Downloaded AI models)           â”‚    â”‚
â”‚  â”‚   â””â”€â”€ config/                  (User settings)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                         â”‚   OPTIONAL CLOUD   â”‚                           â”‚
â”‚                         â”‚  (Google Drive,    â”‚                           â”‚
â”‚                         â”‚   GPT-4 API, etc.) â”‚                           â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.2.13 Architecture Pattern: Monorepo with Hybrid Processes

â•â•â• CORE CONCEPT â•â•â•

> **One codebase, multiple processes.** Everything lives in one Git repository, but runs as separate programs that communicate over the network.
>
> ```
> /handshake-repo/
> â”œâ”€â”€ ui/              # React/TypeScript frontend
> â”œâ”€â”€ backend/         # Python orchestrator + APIs  
> â”œâ”€â”€ shared/          # Type definitions, schemas
> â””â”€â”€ docs/            # Documentation
> ```
>
> This gives us:
> - âœ“ Unified versioning (frontend and backend always match)
> - âœ“ Isolation (Python crash doesn't kill UI)
> - âœ“ Flexibility (can restart backend without UI reload)

---

#### 8.6.2.14 Communication Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERACTION FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User clicks "Summarize this document"                      â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  2. React sends HTTP POST to localhost:8000/api/summarize      â”‚
â”‚     {                                                          â”‚
â”‚       "document_id": "abc123",                                 â”‚
â”‚       "style": "brief"                                         â”‚
â”‚     }                                                          â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  3. Python backend receives, routes to orchestrator            â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  4. Orchestrator picks model: local Llama 3 (13B)             â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  5. Model generates summary, streaming via WebSocket           â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  6. React displays streaming text to user                      â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  7. Final result saved to document file                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.2.15 Why Not Full Microservices?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: How to structure backend services          â”‚
â”‚                                                                      â”‚
â”‚ Options researched:                                                  â”‚
â”‚   â€¢ Full microservices (each model in its own Docker container)     â”‚
â”‚   â€¢ Monolith (everything in one Python process)                     â”‚
â”‚   â€¢ Hybrid (multiple processes, no containers)                      â”‚
â”‚                                                                      â”‚
â”‚ Recommendation: HYBRID APPROACH                                      â”‚
â”‚                                                                      â”‚
â”‚ Rationale:                                                           â”‚
â”‚   â€¢ Full microservices adds Docker complexity                       â”‚
â”‚   â€¢ Monolith risks one crash killing everything                     â”‚
â”‚   â€¢ Hybrid: spawn Python processes for each service                 â”‚
â”‚                                                                      â”‚
â”‚ Implementation:                                                      â”‚
â”‚   â€¢ Main orchestrator process                                        â”‚
â”‚   â€¢ Model runtimes as separate processes (can restart independently)â”‚
â”‚   â€¢ Communication via localhost HTTP (simple, debuggable)           â”‚
â”‚                                                                      â”‚
â”‚ Tradeoffs:                                                           â”‚
â”‚   â€¢ Slightly more complex than monolith                             â”‚
â”‚   â€¢ Less isolated than Docker (shared filesystem)                   â”‚
â”‚   â€¢ Good balance for desktop app context                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.2.16 Startup Sequence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APP STARTUP SEQUENCE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. User double-clicks Handshake.app                         â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  2. Tauri shell starts                                       â”‚
â”‚     â€¢ Creates application window                             â”‚
â”‚     â€¢ Loads React frontend                                   â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  3. Tauri spawns Python backend                              â”‚
â”‚     â€¢ python -m handshake.server                            â”‚
â”‚     â€¢ Backend starts on localhost:8000                       â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  4. Backend initializes orchestrator                         â”‚
â”‚     â€¢ Loads model registry (what models are available)       â”‚
â”‚     â€¢ Does NOT load models yet (wait for demand)             â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  5. Frontend polls /health endpoint                          â”‚
â”‚     â€¢ Shows "Loading..." until backend ready                 â”‚
â”‚     â€¢ Then displays workspace                                â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  6. First AI request triggers model loading                  â”‚
â”‚     â€¢ Model loaded to GPU on first use                       â”‚
â”‚     â€¢ Subsequent requests are fast                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.2.25 Key Takeaways

- âœ“ **Files are the source of truth**, not a database
- âœ“ Standard formats (Markdown, CSV, JSON) = portable, readable data
- âœ“ Sidecar files store metadata without modifying originals
- âœ“ SQLite used only for fast search/indexing
- âœ“ Folder structure mirrors logical organization
- âœ“ AI generation parameters stored for reproducibility

**See Also:** [Section 3 - Collaboration and Sync](#73-collaboration-and-sync)

---
---

### 8.6.3 Plugin and Extension System (Expanded)

*This appendix provides additional detail on plugin system design and extensibility patterns.*

#### 8.6.3.1 Plugin and Extension System Overview

This section covers how to design Handshake as an extensible platform.

---

#### 8.6.3.2 Plugin Architecture Patterns

**Prerequisites:** Section 2.1 (High-Level Architecture)  
**Related to:** Section 4.2 (Security)  
**Implements:** Extensibility foundation  
**Read time:** ~5 minutes

**A good plugin system lets third parties (and you) extend the app without modifying core code.**

---

#### 8.6.3.3 Lessons from Reference Apps

Based on research of existing apps:

| App | Plugin Approach | Lesson for Handshake |
|-----|-----------------|---------------------|
| **Obsidian** | JS plugins in main process | Large ecosystem, some stability risks |
| **Joplin** | Sandboxed, separate process | Safer but more complex |
| **Logseq** | JS API, ClojureScript | Good API, some breaking changes |
| **VS Code** | Extension host process | Gold standard, but complex |

---

#### 8.6.3.4 Recommended Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PLUGIN ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PHASE 1 (MVP): Internal Extension Points                   â”‚
â”‚  â€¢ Define stable internal APIs                              â”‚
â”‚  â€¢ Build core features as "internal plugins"                â”‚
â”‚  â€¢ Establishes patterns for later                           â”‚
â”‚                                                              â”‚
â”‚  PHASE 2: User Scripts                                      â”‚
â”‚  â€¢ Allow simple automation scripts                          â”‚
â”‚  â€¢ Sandboxed JavaScript/Python execution                    â”‚
â”‚  â€¢ Limited API surface                                      â”‚
â”‚                                                              â”‚
â”‚  PHASE 3: Full Plugin System                                â”‚
â”‚  â€¢ Public plugin API                                        â”‚
â”‚  â€¢ Plugin marketplace                                        â”‚
â”‚  â€¢ Sandboxed execution (like Joplin)                        â”‚
â”‚  â€¢ Permission model                                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8.6.3.5 Extension Categories to Plan For

| Category | Examples | API Needed |
|----------|----------|-----------|
| **Custom Blocks** | New editor block types | Block registration, rendering |
| **AI Agents** | Specialized AI workflows | Agent API, model access |
| **Integrations** | Third-party services | HTTP, auth storage |
| **Views** | New database views | View registration, data access |
| **Themes** | Visual customization | CSS variables, style hooks |

---

#### 8.6.3.10 Key Takeaways

- âœ“ Sandbox plugin execution
- âœ“ Explicit permission requests
- âœ“ User approval for sensitive permissions
- âœ“ Tauri's security model helps here

---
---

### 8.6.4 Docling Feature Comparison Tables

*(See Section 7.2 for detailed format support matrices)*

### 8.6.5 ASR Risk Matrix

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Model accuracy insufficient | Medium | High | Multiple model tiers; cloud fallback option |
| GPU resource contention | Medium | Medium | Model tiering; CPU fallback path |
| Whisper license changes | Low | High | Monitor; alternative runtimes available |
| Diarization complexity | High | Medium | Defer to post-MVP; document limitation |
| Audio format compatibility | Low | Low | ffmpeg handles most formats |

### 8.6.6 Works Cited (Docling)

- Docling GitHub Repository: https://github.com/DS4SD/docling
- DocLayNet: Deep Learning Based Document Layout Analysis (IBM Research)
- TableFormer: Table Structure Understanding with Transformers
- LF AI & Data Foundation: Docling Project Page

### 8.6.7 Works Cited (ASR)

- OpenAI Whisper: Robust Speech Recognition via Large-Scale Weak Supervision
- Faster-Whisper: CTranslate2-based Whisper Implementation
- whisper.cpp: High-performance C++ Whisper Inference
- NVIDIA NeMo ASR Documentation
- PaddleSpeech Documentation

---
