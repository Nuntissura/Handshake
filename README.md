# PROJECT HANDSHAKE - Research Document

**Version:** 1.0 | **Date:** November 29, 2025 | **Status:** Foundation Research  
**Purpose:** Complete technical research compilation for building a local-first AI-powered desktop application  
**Target:** Non-technical founder building an intelligent productivity platform

---

## Project Context

- **What we're building:** A desktop application that runs multiple AI models locally on your computer, with collaborative document editing, plugin extensibility, and privacy-first design. Think Notion meets local ChatGPT with the extensibility of VS Code.
- **Current stage:** Pre-development research phase
- **Key constraints:** Single RTX 3090 GPU (24GB VRAM), Windows-first, solo/small team development, local-first (works offline)
- **Success criteria:** A working desktop app where users can edit documents/boards/spreadsheets, use AI assistants for various tasks, install third-party plugins safely, and sync across devicesâ€”all while keeping data primarily on their own machine.

## Document History

- v1.0 (November 29, 2025): Initial research compilation from 8 source documents covering LLM infrastructure, data sync, plugins, security, and observability

---

## How to Use This Document

ğŸ“Œ **For Learning:** Read sections in orderâ€”each builds on previous concepts  
ğŸ“Œ **For Reference:** Use the Table of Contents to jump to specific topics  
ğŸ“Œ **For Implementation:** Look for `âœ“ Action Items` and `Decision Points` boxes  
ğŸ“Œ **For LLM Context:** Include relevant section anchors when asking for help

**Reading Time Estimates:**
- Quick skim (headers + key takeaways): ~30 minutes
- Core concepts only (`[CORE]` sections): ~2 hours
- Complete read-through: ~4-5 hours

---

## Table of Contents

### Part I: Foundations
- [1.0 Understanding the Big Picture](#1-understanding-the-big-picture)
  - [1.1 What is a Local-First Application?](#11-what-is-a-local-first-application)
  - [1.2 Project Architecture Overview](#12-project-architecture-overview)
  - [1.3 Hardware Context: The RTX 3090 Setup](#13-hardware-context-the-rtx-3090-setup)

### Part II: LLM Infrastructure
- [2.0 LLM Fundamentals](#2-llm-fundamentals)
  - [2.1 How LLMs Work (Simplified)](#21-how-llms-work-simplified)
  - [2.2 Key Concepts: Tokens, VRAM, Quantization](#22-key-concepts-tokens-vram-quantization)
  - [2.3 Model Sizes and What Fits](#23-model-sizes-and-what-fits)
- [3.0 LLM Inference Runtimes](#3-llm-inference-runtimes)
  - [3.1 What is an Inference Runtime?](#31-what-is-an-inference-runtime)
  - [3.2 Runtime Comparison: Ollama vs vLLM vs TGI vs Others](#32-runtime-comparison)
  - [3.3 Recommended Runtime Strategy](#33-recommended-runtime-strategy)
- [4.0 Model Selection & Roles](#4-model-selection-and-roles)
  - [4.1 Specialized Models for Different Tasks](#41-specialized-models-for-different-tasks)
  - [4.2 Model Recommendations by Role](#42-model-recommendations-by-role)
  - [4.3 GPU Memory Management](#43-gpu-memory-management)
  - [4.4 Scheduling & Contention](#44-scheduling-and-contention)
- [5.0 Image Generation (Stable Diffusion)](#5-image-generation)
  - [5.1 SD vs SDXL Overview](#51-sd-vs-sdxl-overview)
  - [5.2 VRAM Requirements & Performance](#52-vram-requirements-and-performance)
  - [5.3 Integrating with LLM Workloads](#53-integrating-with-llm-workloads)

### Part III: Data Architecture
- [6.0 Local-First Data Fundamentals](#6-local-first-data-fundamentals)
  - [6.1 What "Local-First" Really Means](#61-what-local-first-really-means)
  - [6.2 The Problem: Concurrent Editing](#62-the-problem-concurrent-editing)
  - [6.3 Solution: CRDTs Explained](#63-solution-crdts-explained)
- [7.0 CRDT Libraries Comparison](#7-crdt-libraries-comparison)
  - [7.1 Yjs Deep Dive](#71-yjs-deep-dive)
  - [7.2 Automerge Deep Dive](#72-automerge-deep-dive)
  - [7.3 Loro and Emerging Options](#73-loro-and-emerging-options)
  - [7.4 Recommendation: Which CRDT Library?](#74-recommendation-which-crdt-library)
- [8.0 Database & Sync Patterns](#8-database-and-sync-patterns)
  - [8.1 Local Database Options (SQLite)](#81-local-database-options)
  - [8.2 Combining CRDT + Database](#82-combining-crdt-and-database)
  - [8.3 Sync Topologies](#83-sync-topologies)
- [9.0 Conflict Resolution UX](#9-conflict-resolution-ux)
  - [9.1 User-Facing Conflict Patterns](#91-user-facing-conflict-patterns)
  - [9.2 Version History UI](#92-version-history-ui)

### Part IV: Plugin & Extension System
- [10.0 Plugin Architecture Fundamentals](#10-plugin-architecture-fundamentals)
  - [10.1 Why Plugins Matter](#101-why-plugins-matter)
  - [10.2 Learning from Existing Systems](#102-learning-from-existing-systems)
- [11.0 Plugin System Design](#11-plugin-system-design)
  - [11.1 Manifest & Registration](#111-manifest-and-registration)
  - [11.2 Plugin Types & Categories](#112-plugin-types-and-categories)
  - [11.3 API Design Patterns](#113-api-design-patterns)
- [12.0 Sandboxing & Security](#12-sandboxing-and-security)
  - [12.1 Why Sandbox Untrusted Code](#121-why-sandbox-untrusted-code)
  - [12.2 Sandboxing Technologies Compared](#122-sandboxing-technologies-compared)
  - [12.3 Permission Models](#123-permission-models)
  - [12.4 Recommended Security Architecture](#124-recommended-security-architecture)

### Part V: Observability & Testing
- [13.0 AI Observability](#13-ai-observability)
  - [13.1 What to Monitor in AI Apps](#131-what-to-monitor-in-ai-apps)
  - [13.2 Tools Comparison](#132-tools-comparison)
  - [13.3 Privacy-Sensitive Logging](#133-privacy-sensitive-logging)
  - [13.4 Metrics & Dashboards](#134-metrics-and-dashboards)
- [14.0 Evaluation & Quality](#14-evaluation-and-quality)
  - [14.1 Testing LLM Outputs](#141-testing-llm-outputs)
  - [14.2 Multi-Agent Tracing](#142-multi-agent-tracing)
- [15.0 Benchmark Harness](#15-benchmark-harness)
  - [15.1 Benchmark Architecture](#151-benchmark-architecture)
  - [15.2 Scenarios & Adapters](#152-scenarios-and-adapters)
  - [15.3 Reporting & Analysis](#153-reporting-and-analysis)

### Part VI: Implementation
- [16.0 Technology Stack Summary](#16-technology-stack-summary)
- [17.0 Implementation Roadmap](#17-implementation-roadmap)
- [18.0 Gap Analysis & Open Questions](#18-gap-analysis)

### End Matter
- [Consolidated Glossary](#consolidated-glossary)
- [Sources Referenced](#sources-referenced)

---
---

### Part VII: Consolidated Architecture & Roadmap
- [19. Executive Summary ](#19-executive-summary)
- [20. Foundation Concepts ](#20-foundation-concepts)
  - [20.1 What is a Desktop Application Shell? ](#201-what-is-a-desktop-application-shell)
  - [20.2 Understanding Local-First Software ](#202-understanding-local-first-software)
  - [20.3 What are AI Models and How Do They Run Locally? ](#203-what-are-ai-models-and-how-do-they-run-locally)
  - [20.4 Multi-Model Orchestration Explained ](#204-multi-model-orchestration-explained)
- [21. Architecture Decisions ](#21-architecture-decisions)
  - [21.1 Desktop Shell: Tauri vs Electron ](#211-desktop-shell-tauri-vs-electron)
  - [21.2 Overall System Architecture ](#212-overall-system-architecture)
  - [21.3 Data Architecture: File-Tree Model ](#213-data-architecture-file-tree-model)
- [22. User Interface Components ](#22-user-interface-components)
  - [22.1 Rich Text Editor (Notion-like) ](#221-rich-text-editor-notion-like)
  - [22.2 Freeform Canvas (Milanote-like) ](#222-freeform-canvas-milanote-like)
  - [22.3 Spreadsheet Engine (Excel-like) ](#223-spreadsheet-engine-excel-like)
  - [22.4 Additional Views: Kanban, Calendar, Timeline ](#224-additional-views-kanban-calendar-timeline)
- [23. AI Model Strategy ](#23-ai-model-strategy)
  - [23.1 Model Categories and Recommendations ](#231-model-categories-and-recommendations)
  - [23.2 Local Model Runtimes ](#232-local-model-runtimes)
  - [23.3 Cloud Fallback Strategy ](#233-cloud-fallback-strategy)
  - [23.4 Image Generation with ComfyUI ](#234-image-generation-with-comfyui)
- [24. Multi-Agent Orchestration ](#24-multi-agent-orchestration)
  - [24.1 Framework Comparison: AutoGen vs LangGraph vs CrewAI ](#241-framework-comparison-autogen-vs-langgraph-vs-crewai)
  - [24.2 The Lead/Worker Pattern ](#242-the-leadworker-pattern)
  - [24.3 Shared Context and Memory ](#243-shared-context-and-memory)
  - [24.4 Task Routing and Fallback Logic ](#244-task-routing-and-fallback-logic)
- [25. Collaboration and Sync ](#25-collaboration-and-sync)
  - [25.1 Understanding CRDTs ](#251-understanding-crdts)
  - [25.2 Offline-First Architecture ](#252-offline-first-architecture)
  - [25.3 Google Workspace Integration ](#253-google-workspace-integration)
- [26. Plugin and Extension System ](#26-plugin-and-extension-system)
  - [26.1 Plugin Architecture Patterns ](#261-plugin-architecture-patterns)
  - [26.2 Security and Sandboxing ](#262-security-and-sandboxing)
- [27. Reference Application Analysis ](#27-reference-application-analysis)
  - [27.1 AppFlowy ](#271-appflowy)
  - [27.2 AFFiNE ](#272-affine)
  - [27.3 Obsidian ](#273-obsidian)
  - [27.4 Logseq ](#274-logseq)
  - [27.5 Lessons Learned ](#275-lessons-learned)
- [28. Development Workflow ](#28-development-workflow)
  - [28.1 Using AI Coding Assistants Effectively ](#281-using-ai-coding-assistants-effectively)
  - [28.2 Project Health and Hygiene ](#282-project-health-and-hygiene)
  - [28.3 CI/CD and Testing Strategy ](#283-cicd-and-testing-strategy)
- [29. Development Roadmap ](#29-development-roadmap)
  - [29.1 Phase Overview ](#291-phase-overview)
  - [29.2 MVP Definition ](#292-mvp-definition)
  - [29.3 Build Order and Dependencies ](#293-build-order-and-dependencies)
- [30. Risk Assessment ](#30-risk-assessment)
- [31. Technology Stack Summary ](#31-technology-stack-summary)
- [32. Consolidated Glossary ](#32-consolidated-glossary)
- [33. Open Questions and Next Steps ](#33-open-questions-and-next-steps)
- [34. Sources Referenced ](#34-sources-referenced)

# PART I: FOUNDATIONS

---

## 1.0 Understanding the Big Picture {#1-understanding-the-big-picture}

**Prerequisites:** None - foundational  
**Related to:** All subsequent sections  
**Implements:** Core project understanding  
**Read time:** ~15 minutes

**This section explains what we're building and why, establishing the mental model you'll use throughout this document.**

---

### 1.1 What is a Local-First Application? {#11-what-is-a-local-first-application}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters for This Project |
|------|---------------|--------------------------------|
| **Local-First** | Your data lives primarily on your computer, not in "the cloud" (someone else's computer). The app works fully offline. | This is our core philosophyâ€”users own their data, AI runs locally, and the app works without internet |
| **Cloud-First** | The oppositeâ€”your data lives on company servers, and you need internet to use the app (like Google Docs) | What we're NOT building. Understand this to understand our tradeoffs |
| **Offline-Capable** | Can work without internet temporarily, but really needs the cloud | Weaker than local-first. We want TRUE local-first |
| **Sync** | Keeping data consistent across multiple devices (your laptop and phone showing the same notes) | We want this eventually, but local-first makes it harder |

#### The Core Idea

**Local-first means your computer is the primary home for your data, and the cloud is just a backup or sync helper.** Traditional apps like Google Docs work the opposite way: Google's servers hold the "real" copy, and your browser just shows you a window into it.

```
CLOUD-FIRST (Google Docs):                LOCAL-FIRST (What we're building):
                                          
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google's Servers  â”‚ â† "Real" data     â”‚   YOUR Computer     â”‚ â† "Real" data
â”‚   (the cloud)       â”‚                   â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                         â”‚
          â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Browser      â”‚ â† Just a window   â”‚   Cloud (optional)  â”‚ â† Backup/sync
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Why Local-First for This Project?

ğŸ“Œ **Privacy:** AI processes your documents locally. Your private notes never leave your machine.

ğŸ“Œ **Speed:** No waiting for server round-trips. The AI model is right there on your GPU.

ğŸ“Œ **Ownership:** Your data is literally files on your computer. No company can lock you out.

ğŸ“Œ **Offline:** Works on airplanes, in basements, anywhere. No "you're offline" errors.

âš ï¸ **The Tradeoff:** Syncing between devices becomes much harder. When two devices edit the same document offline, we need special technology (CRDTs) to merge the changes. This is covered in [Part III](#6-local-first-data-fundamentals).

#### Key Takeaways

- Local-first = your computer holds the authoritative data, cloud is secondary
- This gives us privacy, speed, and offline capability
- The main challenge is syncing between devices (covered later)
- We're building a desktop app, not a web app, which makes local-first natural

---

### 1.2 Project Architecture Overview {#12-project-architecture-overview}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **Desktop App** | An application you install on your computer (like Word or Photoshop), not one you use in a browser | We're building this, not a website |
| **Tauri** | A framework for building desktop apps using web technologies (HTML, CSS, JavaScript) with a Rust backend | Our likely app frameworkâ€”lighter than Electron |
| **Electron** | Another desktop app framework (used by VS Code, Slack, Discord) | Alternative to Tauri, heavier but more mature |
| **Frontend** | The part users see and interact with (buttons, text fields, etc.) | Our user interface |
| **Backend/Orchestrator** | The "brain" that handles logic, talks to AI models, manages data | Where the complex stuff happens |
| **GPU** | Graphics Processing Unitâ€”originally for games, now also runs AI models very fast | Our RTX 3090 runs the AI |

#### The Big Picture

Our app has four major layers that work together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE (Frontend)                    â”‚
â”‚         Documents | Boards | Spreadsheets | Chat | Settings     â”‚
â”‚                        [Tauri + React/Vue]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Commands & Events
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ORCHESTRATOR (Python Backend)                 â”‚
â”‚  â€¢ Routes requests to appropriate AI models                     â”‚
â”‚  â€¢ Manages which models are loaded                              â”‚
â”‚  â€¢ Handles plugin execution                                     â”‚
â”‚  â€¢ Coordinates data sync                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                  â”‚                 â”‚
            â–¼                  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM RUNTIMES    â”‚ â”‚  LOCAL DATA    â”‚ â”‚    PLUGIN SYSTEM     â”‚
â”‚ (Ollama, vLLM)    â”‚ â”‚ (SQLite+CRDT)  â”‚ â”‚  (Sandboxed code)    â”‚
â”‚                   â”‚ â”‚                â”‚ â”‚                      â”‚
â”‚ â€¢ Mistral-7B      â”‚ â”‚ â€¢ Documents    â”‚ â”‚ â€¢ User automations   â”‚
â”‚ â€¢ CodeLlama       â”‚ â”‚ â€¢ Boards       â”‚ â”‚ â€¢ AI tools           â”‚
â”‚ â€¢ Creative LLM    â”‚ â”‚ â€¢ Spreadsheets â”‚ â”‚ â€¢ Integrations       â”‚
â”‚ â€¢ SDXL (images)   â”‚ â”‚ â€¢ Sync state   â”‚ â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚
          â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RTX 3090 GPU    â”‚ â”‚   Hard Drive   â”‚
â”‚   (24GB VRAM)     â”‚ â”‚   (Files)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Component Breakdown

**1. User Interface (Frontend)**
- What users see: text editor, kanban boards, spreadsheets, chat interface
- Built with web technologies (HTML/CSS/JavaScript) inside a desktop wrapper
- Communicates with the backend through local messages (IPC)

**2. Python Orchestrator (Backend)**
- The "brain" that coordinates everything
- Decides which AI model to use for each task
- Manages GPU memory (can't run everything at once)
- Handles plugin permissions and execution

**3. LLM Runtimes**
- Software that actually runs AI models
- We'll likely use Ollama (easy) and/or vLLM (fast)
- Exposes models through a standardized API

**4. Local Data Layer**
- SQLite database for structured data
- CRDT library (Yjs) for collaborative editing
- Files stored on local disk

**5. Plugin System**
- Lets users/developers extend the app
- Runs in a sandbox for security
- Can add new AI tools, automations, integrations

#### Key Takeaways

- Four main layers: UI â†’ Orchestrator â†’ Services â†’ Hardware
- Python orchestrator is the central coordinator
- AI models run on the GPU via runtime software
- Data lives locally in SQLite + files
- Plugins extend functionality in a sandboxed environment

---

### 1.3 Hardware Context: The RTX 3090 Setup {#13-hardware-context-the-rtx-3090-setup}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **VRAM** | Video RAMâ€”memory on your graphics card. AI models must fit here to run fast | Our 24GB limit determines which/how many models we can run |
| **System RAM** | Regular computer memory (your 128GB) | Backup when VRAM is full, but much slower |
| **GPU** | The graphics card processor itself | Does the actual AI computation |
| **CUDA** | NVIDIA's technology for running non-graphics computations on GPUs | Required for our AI workloads |
| **Bandwidth** | How fast data can move (like a pipe's width) | GPU memory is ~6x faster than system RAM |

#### Your Hardware Profile

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   YOUR SETUP                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CPU:  AMD Ryzen 5950X (16 cores, 32 threads)          â”‚
â”‚  RAM:  128 GB DDR4                                      â”‚
â”‚  GPU:  NVIDIA RTX 3090 (24 GB VRAM)                    â”‚
â”‚  OS:   Windows                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### What 24GB VRAM Means for Us

**VRAM is the critical constraint.** AI models must be loaded into VRAM to run at full speed. Think of VRAM like a deskâ€”you can only have so many documents open at once.

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: VRAM BUDGET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  24 GB Total VRAM
  â”œâ”€â”€ ~1-2 GB: System/driver overhead (always used)
  â”œâ”€â”€ Remaining: ~22 GB for models
  â”‚
  â”‚   Example allocations:
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚ Option A: Two medium models + headroom         â”‚
  â”‚   â”‚   Mistral-7B (4GB) + CodeLlama-7B (4GB)       â”‚
  â”‚   â”‚   = 8GB used, 14GB free for context/images    â”‚
  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚   â”‚ Option B: One large model                      â”‚
  â”‚   â”‚   Llama2-70B-4bit (17GB)                       â”‚
  â”‚   â”‚   = 17GB used, 5GB free (tight!)              â”‚
  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚   â”‚ Option C: Medium model + image generation      â”‚
  â”‚   â”‚   Mistral-7B (4GB) + SDXL (7-10GB)            â”‚
  â”‚   â”‚   = 11-14GB used                               â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### The Speed Difference: GPU vs CPU

âš¡ **Critical:** Running models from GPU VRAM is approximately 6x faster than running them from system RAM.

| Where Model Lives | Speed | When to Use |
|-------------------|-------|-------------|
| GPU VRAM | ~50-130 tokens/sec | Always prefer this |
| System RAM (CPU) | ~8-20 tokens/sec | Last resort / fallback |

This is why we obsess over VRAM managementâ€”moving models to CPU makes the app feel sluggish.

#### Practical Rules of Thumb

ğŸ“Œ **Model Size Formula:** A 7B parameter model at 4-bit quantization â‰ˆ 4GB VRAM

ğŸ“Œ **Safe Concurrent Limit:** 2-3 small models (7B) OR 1-2 medium models (13B) at once

ğŸ“Œ **Don't Mix Heavy Workloads:** Running SDXL image generation while querying a large LLM will likely exceed VRAM

ğŸ“Œ **Buffer for Context:** Long conversations use extra VRAM for "context" (what the model remembers). Budget 2-4GB headroom.

#### Key Takeaways

- 24GB VRAM is generous but not unlimited
- GPU memory is ~6x faster than system RAMâ€”avoid CPU fallback
- Plan to run 2-3 small models OR 1-2 medium models concurrently
- Heavy image generation (SDXL) competes with LLMs for VRAM
- Always leave headroom for context and system overhead

---
---

# PART II: LLM INFRASTRUCTURE

---

## 2.0 LLM Fundamentals {#2-llm-fundamentals}

**Prerequisites:** Section 1.3 (Hardware Context)  
**Related to:** Sections 3, 4, 5  
**Implements:** Understanding needed to choose models and runtimes  
**Read time:** ~20 minutes

**This section explains how Large Language Models work at the level needed to make good decisions about which models to use and how to run them.**

---

### 2.1 How LLMs Work (Simplified) {#21-how-llms-work-simplified}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **LLM** | Large Language Modelâ€”AI that generates human-like text by predicting the next word | The core AI technology we're using |
| **Parameters** | The "knowledge" of a model, stored as numbers. More parameters = more knowledge but more memory | Determines model capability and size |
| **Inference** | Using a trained model to generate outputs (vs. "training" which creates the model) | We do inference, not training |
| **Prompt** | The text you give the model as input | What users type |
| **Completion** | The text the model generates in response | What the AI responds with |

#### The Basic Idea

**An LLM is a very sophisticated autocomplete.** Given some text, it predicts what text should come nextâ€”but it's so good at this that it can write essays, code, answer questions, and more.

```
You type:       "Write a haiku about programming"
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   LLM Model     â”‚
                  â”‚  (Billions of   â”‚
                  â”‚   parameters)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
Model outputs:  "Code flows like water
                 Bugs emerge from the depths below
                 Debug, rinse, repeat"
```

#### What "Parameters" Mean

Think of parameters as the model's "brain cells"â€”connections that store patterns learned from training data.

```
Model Size Guide:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  3B-4B   â”‚  Small  â”‚  Fast, limited capability     
  7B-8B   â”‚  Medium â”‚  Good balance, our sweet spot 
  13B     â”‚  Large  â”‚  Better quality, slower       
  27B-30B â”‚  XL     â”‚  Near-GPT-3.5 quality         
  70B+    â”‚  XXL    â”‚  Best quality, very demanding 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

ğŸ’¡ **For our project:** 7B-13B models hit the sweet spot of quality vs. resource usage on a 3090.

#### Key Takeaways

- LLMs predict "what text comes next" so well they seem intelligent
- More parameters = smarter but hungrier for resources
- We'll use 7B-13B models as our primary workhorses
- We do "inference" (using models), not "training" (creating models)

---

### 2.2 Key Concepts: Tokens, VRAM, Quantization {#22-key-concepts-tokens-vram-quantization}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **Token** | A chunk of text (roughly Â¾ of a word). Models think in tokens, not letters or words | How we measure input/output size and cost |
| **Context Window** | How many tokens a model can "see" at once (its working memory) | Limits how much conversation history or document text we can include |
| **Quantization** | Compressing a model to use less memory by reducing number precision | How we fit big models into limited VRAM |
| **Q4/Q5/Q8** | Quantization levels: Q4 = 4-bit (smallest), Q8 = 8-bit (highest quality) | Trade-off between size and quality |
| **GGUF** | A file format for quantized models, works with llama.cpp | The format most local models use |

#### Understanding Tokens

**Tokens are how models measure text.** One token â‰ˆ 4 characters â‰ˆ 0.75 words.

```
Example tokenization:
"Hello, how are you today?" 
â†’ ["Hello", ",", " how", " are", " you", " today", "?"]
â†’ 7 tokens

Rough conversion:
  100 tokens  â‰ˆ 75 words   â‰ˆ 1 short paragraph
  1000 tokens â‰ˆ 750 words  â‰ˆ 1.5 pages
  4000 tokens â‰ˆ 3000 words â‰ˆ 6 pages
```

ğŸ“Œ **Why tokens matter:** 
- Models have a maximum context window (e.g., 4096 or 8192 tokens)
- Cloud APIs charge per token
- More tokens = slower responses and more memory

#### Understanding Context Windows

**The context window is the model's "working memory."** It includes BOTH your prompt AND the model's response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4096 TOKEN CONTEXT WINDOW                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  System prompt (instructions)     â”‚  ~200 tokens       â”‚
â”‚  Conversation history             â”‚  ~2000 tokens      â”‚
â”‚  Current user message             â”‚  ~300 tokens       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Space for model's response       â”‚  ~1596 tokens      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âš ï¸ **Warning:** Long conversations eventually "forget" earlier messages when context fills up.

#### Understanding Quantization

**Quantization shrinks models by reducing number precision.** Like saving a photo as JPEG instead of RAWâ€”smaller file, slight quality loss.

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: QUANTIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  Original model: 7B parameters at 16-bit = ~14 GB
  
  Quantized versions:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Format   â”‚ Bits     â”‚ Size        â”‚ Quality Loss       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Q8_0     â”‚ 8-bit    â”‚ ~7 GB       â”‚ Minimal (<1%)      â”‚
  â”‚ Q5_K_M   â”‚ 5-bit    â”‚ ~5 GB       â”‚ Very small (~1-2%) â”‚
  â”‚ Q4_K_M   â”‚ 4-bit    â”‚ ~4 GB       â”‚ Small (~2-3%)      â”‚ â† Sweet spot
  â”‚ Q3_K_M   â”‚ 3-bit    â”‚ ~3 GB       â”‚ Noticeable (~5%)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  ğŸ“Œ Q4_K_M is the most common choice: good quality, big savings

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

ğŸ’¡ **For our project:** We'll primarily use Q4_K_M quantized models in GGUF format.

#### VRAM Usage: Putting It Together

```
Formula for VRAM estimate:
  VRAM â‰ˆ (Parameters in billions) Ã— (Bits Ã· 2) GB
  
  Examples with Q4 (4-bit):
  â€¢ 7B model:  7 Ã— (4Ã·2) = 7 Ã— 2 = ~3.5-4 GB
  â€¢ 13B model: 13 Ã— (4Ã·2) = 13 Ã— 2 = ~6.5-8 GB  
  â€¢ 70B model: 70 Ã— (4Ã·2) = 70 Ã— 2 = ~35 GB... but actually fits in ~17-18GB 
                          (due to efficient formats)
```

#### Key Takeaways

- Tokens â‰ˆ 0.75 words; context window limits total conversation length
- Quantization (Q4/Q5) shrinks models 3-4x with minimal quality loss
- GGUF is the standard format for local quantized models
- 7B Q4 model â‰ˆ 4GB VRAM; this is our planning baseline

---

### 2.3 Model Sizes and What Fits {#23-model-sizes-and-what-fits}

`[CORE]`

#### Quick Reference Table

| Model Size | Quantization | VRAM Needed | Speed (tokens/sec) | Quality Level |
|------------|--------------|-------------|-------------------|---------------|
| 3-4B | Q4 | ~2-3 GB | 60-200 | Basic tasks |
| 7-8B | Q4 | ~4-5 GB | 50-130 | Good general use |
| 13B | Q4 | ~7-9 GB | 30-70 | Very good |
| 27B | Q4 | ~14 GB | 20-30 | Excellent |
| 70B | Q4 | ~17-18 GB | 10-15 | Near GPT-3.5 |

#### What Fits on Our 24GB RTX 3090?

```
Scenario Planning for 24 GB VRAM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ COMFORTABLE (with headroom):
  â€¢ 3Ã— 7B models (12 GB) + context buffer
  â€¢ 2Ã— 13B models (16 GB) + some headroom  
  â€¢ 1Ã— 7B + 1Ã— 13B + 1Ã— 4B (15 GB)

âš¡ TIGHT (works but careful):
  â€¢ 1Ã— 70B model (17-18 GB) alone
  â€¢ 1Ã— 27B + 1Ã— 7B (18 GB)
  â€¢ 2Ã— 7B + SDXL image generation (8 + 10 = 18 GB)

âœ— WON'T FIT:
  â€¢ 2Ã— 70B models (34+ GB)
  â€¢ 70B + any substantial other model
  â€¢ Multiple 27B+ models
```

#### Key Takeaways

- Our sweet spot: 2-3 models in the 7B-13B range loaded simultaneously
- One 70B model is possible but leaves little room for anything else
- Always budget 2-4GB headroom for context and system overhead

---

## 3.0 LLM Inference Runtimes {#3-llm-inference-runtimes}

**Prerequisites:** Section 2.0 (LLM Fundamentals)  
**Related to:** Sections 4.0, 5.0  
**Implements:** Runtime infrastructure decisions  
**Read time:** ~25 minutes

**This section compares the software that actually runs LLM models, helping you choose the right tool for different scenarios.**

---

### 3.1 What is an Inference Runtime? {#31-what-is-an-inference-runtime}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **Runtime** | Software that loads and runs AI models | We need this to use any LLM |
| **API** | Application Programming Interfaceâ€”a way for programs to talk to each other | How our app communicates with the runtime |
| **OpenAI-compatible API** | An API that works like OpenAI's, so code written for ChatGPT works locally | Makes integration easy |
| **Streaming** | Sending response tokens one at a time as they're generated (vs. waiting for the full response) | Better user experienceâ€”text appears progressively |
| **Batching** | Processing multiple requests together for efficiency | Important for handling many users/requests |

#### The Role of an Inference Runtime

**A runtime is the software layer between your application and the AI model.** It handles:

```
Your App                    Runtime                     GPU
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP API    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   CUDA/GPU    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Write  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ â€¢ Load   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Matrix  â”‚
â”‚  me a   â”‚                â”‚   model  â”‚              â”‚ math on â”‚
â”‚  poem"  â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â€¢ Run    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ tensors â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Streaming    â”‚   infer  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Response     â”‚ â€¢ Manage â”‚
                          â”‚   memory â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Why Runtime Choice Matters

Different runtimes optimize for different things:

| Priority | Best Runtime | Trade-off |
|----------|-------------|-----------|
| Ease of use | Ollama | Lower max throughput |
| Maximum speed | vLLM | More complex setup |
| Enterprise features | TGI | Heavier infrastructure |
| Simplicity (single model) | llamafile | Very limited features |

#### Key Takeaways

- Runtime = software that loads and runs your AI models
- All major runtimes now support OpenAI-compatible APIs
- Choice depends on: ease of use vs. performance vs. features

---

### 3.2 Runtime Comparison {#32-runtime-comparison}

`[CORE]`

#### Overview Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Runtime     â”‚ Multi-Model â”‚ Performance  â”‚ Ease of Use   â”‚ Best For      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ollama      â”‚ Yes (swap)  â”‚ Moderate     â”‚ â­â­â­â­â­ Easy   â”‚ Development   â”‚
â”‚ vLLM        â”‚ No (1 each) â”‚ â­â­â­â­â­ Best  â”‚ â­â­ Complex   â”‚ Production    â”‚
â”‚ TGI         â”‚ No (1 each) â”‚ Very Good    â”‚ â­â­â­ Medium   â”‚ Enterprise    â”‚
â”‚ LM Studio   â”‚ Yes (GUI)   â”‚ Moderate     â”‚ â­â­â­â­â­ Easy   â”‚ Exploration   â”‚
â”‚ llamafile   â”‚ No          â”‚ Low          â”‚ â­â­â­â­â­ Easy   â”‚ Distribution  â”‚
â”‚ llama.cpp   â”‚ No          â”‚ Good         â”‚ â­â­â­ Medium   â”‚ Embedding     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Ollama â€” The Easy Choice

**What it is:** A user-friendly CLI tool and server for running local LLMs. Think "Docker for AI models."

**How it works:**
```bash
# Install and run a model in one command
ollama run mistral

# Or start as a server
ollama serve
# Then call via API at localhost:11434
```

**Pros:**
- â­ Incredibly easy to set up (one-line install)
- â­ Built-in model management (download, update, delete)
- â­ OpenAI-compatible API out of the box
- â­ Automatic GPU/CPU fallback
- â­ Supports multiple models (swaps them in/out of VRAM)

**Cons:**
- âš ï¸ Not optimized for high concurrency (~41 tokens/sec under load vs vLLM's ~793)
- âš ï¸ No advanced batching (processes one request fully before next)
- âš ï¸ Model switching has latency (unload/load takes seconds)

**Performance Numbers:**
```
Ollama on RTX 3090 (single user):
  â€¢ Mistral-7B Q4:  ~100-130 tokens/sec
  â€¢ Llama2-13B Q4:  ~40-50 tokens/sec
  â€¢ Under heavy load: drops to ~41 tokens/sec (no batching)
```

**Best for:** Development, personal use, low-concurrency production

---

#### vLLM â€” The Performance Champion

**What it is:** A high-performance inference engine from UC Berkeley, optimized for throughput.

**How it works:**
```bash
# Start vLLM server
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-v0.1 \
  --port 8000
```

**Pros:**
- â­ Extremely fast: ~793 tokens/sec under load (vs Ollama's ~41)
- â­ Continuous batching: efficiently handles many concurrent requests
- â­ PagedAttention: optimizes memory usage
- â­ Scales almost linearly with more requests
- â­ OpenAI-compatible API

**Cons:**
- âš ï¸ One model per process (need multiple processes for multiple models)
- âš ï¸ More complex setup than Ollama
- âš ï¸ GPU-only (no CPU fallback)
- âš ï¸ Python-based (adds some overhead to embed)

**Performance Numbers:**
```
vLLM on RTX 3090:
  â€¢ Single request:   Similar to Ollama
  â€¢ 10 concurrent:    ~793 tokens/sec total (vs Ollama's ~41)
  â€¢ Scales to 100s of concurrent requests efficiently
```

**Best for:** High-concurrency production, batch processing, when speed matters most

---

#### HuggingFace TGI (Text Generation Inference)

**What it is:** HuggingFace's production-grade inference server, used in their cloud offerings.

**How it works:**
```bash
# Run via Docker
docker run --gpus all -p 8080:80 \
  ghcr.io/huggingface/text-generation-inference \
  --model-id mistralai/Mistral-7B-v0.1
```

**Pros:**
- â­ Production-tested at scale (powers HuggingFace Inference Endpoints)
- â­ Continuous batching like vLLM
- â­ Built-in metrics (Prometheus) and tracing
- â­ Supports many quantization formats (GPTQ, AWQ, bitsandbytes)
- â­ OpenAI-compatible API

**Cons:**
- âš ï¸ One model per container
- âš ï¸ Requires Docker (adds complexity on Windows)
- âš ï¸ Heavier setup than Ollama

**Best for:** Enterprise production, when you need built-in observability

---

#### Other Options (Brief)

**LM Studio:**
- GUI application for exploring models
- Has a server mode with OpenAI API
- Great for testing, not ideal for production automation
- Closed-source

**llamafile:**
- Single executable per model (bundles model + runtime)
- Just download and runâ€”no installation
- Limited features, single-threaded
- Best for distributing a pre-packaged model to end users

**llama.cpp (via Python bindings):**
- The engine under Ollama and many others
- Can embed directly in your code
- More control, more complexity
- Good for custom integrations

---

### 3.3 Recommended Runtime Strategy {#33-recommended-runtime-strategy}

`[CORE]`

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Runtime Strategy
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED APPROACH: Ollama Primary + vLLM for Heavy Loads

Phase 1 (Development & MVP):
  â””â”€â”€ Use Ollama exclusively
      â€¢ Fastest to set up
      â€¢ Easy model management
      â€¢ Good enough for single-user
      
Phase 2 (Multi-user or batch processing):
  â””â”€â”€ Add vLLM for specific high-throughput needs
      â€¢ Route "fast lane" traffic to Ollama
      â€¢ Route "batch" or "heavy" jobs to vLLM

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Integration Pattern

```python
# Conceptual routing logic
def route_request(request):
    if request.type == "interactive_chat":
        # Quick responses, single user
        return call_ollama(request)
    elif request.type == "batch_process":
        # Processing many documents
        return call_vllm(request)
    elif request.type == "code_generation":
        # Code needs fast iteration
        return call_ollama(request, model="codellama")
    else:
        # Fallback
        return call_ollama(request)
```

#### Key Takeaways

- **Start with Ollama:** Easy setup, good enough for development and single-user
- **Add vLLM later if needed:** When you need high concurrency or batch processing
- **Both expose OpenAI-compatible APIs:** Your code works with either
- **Model management:** Ollama handles it; vLLM requires manual setup per model

---

## 4.0 Model Selection & Roles {#4-model-selection-and-roles}

**Prerequisites:** Sections 2.0, 3.0  
**Related to:** Section 5.0 (Image Generation)  
**Implements:** Which specific models to use  
**Read time:** ~20 minutes

**This section recommends specific models for different tasks and explains how to manage multiple models on limited VRAM.**

---

### 4.1 Specialized Models for Different Tasks {#41-specialized-models-for-different-tasks}

`[CORE]`

#### Why Not One Model for Everything?

**Specialized models outperform generalists at specific tasks while using less resources.**

```
Analogy: Hiring Staff

Option A: One expensive expert who does everything "pretty well"
  â””â”€â”€ 70B generalist model (17GB VRAM, slow)

Option B: Team of specialists, each excellent at their job
  â””â”€â”€ 7B code model (4GB) + 7B chat model (4GB) + 7B creative (4GB)
  â””â”€â”€ Total: 12GB, all running simultaneously, each faster at their specialty

For our project: Option B is better
```

#### Role Categories

| Role | What It Does | Characteristics Needed |
|------|--------------|------------------------|
| **Orchestrator** | General reasoning, routing decisions, conversation | Fast, good instruction-following |
| **Code Assistant** | Writing and explaining code | Trained on code, good at syntax |
| **Creative Writer** | Long-form content, stories, marketing | Larger context, creative outputs |
| **Utility/Fast** | Simple tasks: classification, extraction, yes/no | Tiny, extremely fast |

---

### 4.2 Model Recommendations by Role {#42-model-recommendations-by-role}

`[CORE]`

#### Orchestrator / General Purpose

**Primary Pick: Mistral-7B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MISTRAL-7B (Q4_K_M)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters:  7.3B                                         â”‚
â”‚  VRAM:        ~4.1 GB                                      â”‚
â”‚  Speed:       ~130 tokens/sec on 3090                      â”‚
â”‚  Context:     4K tokens (limited) or 8K with some variantsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Strengths:                                                â”‚
â”‚    â€¢ Outperforms Llama2-13B despite being smaller         â”‚
â”‚    â€¢ Excellent instruction following                       â”‚
â”‚    â€¢ Very fast inference                                   â”‚
â”‚  Weaknesses:                                               â”‚
â”‚    â€¢ 4K context can be limiting for long conversations    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alternative: Llama2-13B** (when you need more capability or longer context)
- ~9 GB VRAM, ~40-50 tokens/sec
- 8K context window
- Better for complex reasoning

---

#### Code Generation

**Primary Pick: CodeLlama-7B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODELLAMA-7B (Q4_K_M)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters:  7B                                           â”‚
â”‚  VRAM:        ~3.8 GB                                      â”‚
â”‚  Speed:       ~100 tokens/sec on 3090                      â”‚
â”‚  Context:     16K tokens                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Strengths:                                                â”‚
â”‚    â€¢ Fine-tuned specifically for code                      â”‚
â”‚    â€¢ Supports Python, JS, C++, and more                   â”‚
â”‚    â€¢ Large context for reading whole files                â”‚
â”‚  Weaknesses:                                               â”‚
â”‚    â€¢ Less capable at general conversation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Alternatives:**
- **StarCoder-7B:** Open-source, 16K context
- **WizardCoder-15B:** Higher quality (~8-9GB), better for complex tasks

---

#### Creative / Long-Form Writing

**Primary Pick: Llama2-13B or Mistral-7B**

For most creative tasks, the orchestrator model works fine. For serious long-form writing:

**Consider: Llama2-70B (4-bit)** â€” Best quality, but uses ~17-18GB
- Only load when specifically needed for creative work
- Unload other models first
- ~15 tokens/sec (slower but higher quality)

---

#### Utility / Fast Tasks

**Primary Pick: Phi-4 Mini (3.8B) or Gemma-3-4B**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SMALL UTILITY MODELS                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phi-4 Mini (3.8B Q4):    ~2.5 GB, ~60 tokens/sec         â”‚
â”‚  Gemma-3 4B (4-bit):      ~2.6 GB, ~200+ tokens/sec       â”‚
â”‚  Qwen2.5-3B (Q4):         ~2-3 GB, ~40 tokens/sec         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Use for:                                                  â”‚
â”‚    â€¢ Classification ("is this spam?")                      â”‚
â”‚    â€¢ Extraction ("find the date in this text")            â”‚
â”‚    â€¢ Simple Q&A                                            â”‚
â”‚    â€¢ Routing decisions                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Recommended Starting Configuration

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Initial Model Setup
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: "Always Hot" + "On-Demand" Strategy

Always Loaded ("Hot"):
  â”œâ”€â”€ Mistral-7B (4GB)      â†’ General orchestrator, fast chat
  â””â”€â”€ CodeLlama-7B (4GB)    â†’ Code assistance
  Total: ~8 GB (leaves 14GB free)

Load On-Demand:
  â”œâ”€â”€ Llama2-13B (9GB)      â†’ Complex reasoning when needed
  â”œâ”€â”€ Llama2-70B (17GB)     â†’ Best quality (swap out others first)
  â””â”€â”€ SDXL (7-10GB)         â†’ Image generation

Rationale:
  â€¢ Two 7B models handle 90% of tasks
  â€¢ Fast switching between chat and code
  â€¢ Load larger models only for complex work
  â€¢ Preserves VRAM for image generation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### 4.3 GPU Memory Management {#43-gpu-memory-management}

`[CORE]`

#### The Loading Problem

**Models must be in VRAM to run fast.** Loading a model takes time:
- 7B model: ~3-5 seconds
- 13B model: ~5-10 seconds  
- 70B model: ~15-30 seconds

This creates a user experience challenge: if users request a model that isn't loaded, they wait.

#### Strategies

**1. Keep "Hot" Models Resident**
```
Always keep your most-used models in VRAM:
  â€¢ Set Ollama: OLLAMA_MAX_LOADED_MODELS=2
  â€¢ These stay loaded even when idle
  â€¢ Instant response for common tasks
```

**2. On-Demand Loading with Feedback**
```
When user needs a different model:
  â€¢ Show loading indicator: "Loading creative writing model..."
  â€¢ Expected wait: 5-15 seconds
  â€¢ Consider preloading if you can predict need
```

**3. Never Use CPU Fallback for Primary Tasks**
```
CPU inference is ~6x slower:
  â€¢ GPU: 100 tokens/sec
  â€¢ CPU: ~15 tokens/sec
  
Only use CPU for:
  â€¢ Truly background tasks
  â€¢ When GPU is fully occupied with priority work
  â€¢ Emergency fallback (better slow than nothing)
```

#### KV Cache: The Hidden Memory User

**Context uses extra VRAM beyond model weights.**

```
VRAM breakdown for a 7B model with long conversation:

  Model weights:        ~4 GB
  KV cache (context):   +2-4 GB for 4K tokens
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:                ~6-8 GB actual usage

âš ï¸ Long conversations can DOUBLE your VRAM usage!
```

ğŸ’¡ **Tip:** For multi-model setups, keep conversations shorter or implement context summarization.

---

### 4.4 Scheduling & Contention {#44-scheduling-and-contention}

`[CORE]`

#### The Core Problem

**Only one heavy task can use the GPU efficiently at a time.** Running two things simultaneously doesn't make each run at half speedâ€”it makes both run poorly or crash.

#### Priority Rules

```
Priority Queue (highest to lowest):

  1. Interactive Chat    â†’ User is waiting, <100ms latency matters
  2. Code Generation     â†’ User is waiting, but can tolerate 1-2sec
  3. Image Generation    â†’ User expects to wait (5-30 seconds)
  4. Background Tasks    â†’ Batch processing, can run overnight
```

#### Practical Scheduling Pattern

```python
# Pseudocode for GPU scheduling
class GPUScheduler:
    def handle_request(self, request):
        if request.priority == "interactive":
            # Pause any batch jobs
            self.pause_background_tasks()
            # Run immediately
            return self.run_now(request)
            
        elif request.priority == "image":
            if self.vram_available() < 10_GB:
                # Not enough VRAM, queue it
                return self.queue(request, 
                    message="Waiting for VRAM...")
            else:
                return self.run_now(request)
                
        else:  # background
            # Only run if GPU is idle
            if self.gpu_is_idle():
                return self.run_now(request)
            else:
                return self.queue(request)
```

#### Key Takeaways

- Keep 2 small "hot" models loaded for instant response
- Load larger models on-demand with user feedback
- Never rely on CPU fallback for user-facing tasks
- Context (KV cache) can double VRAM usage
- Implement priority queuing: interactive > image > background

---

## 5.0 Image Generation {#5-image-generation}

**Prerequisites:** Section 4.0 (Model Selection)  
**Related to:** Section 4.4 (Scheduling)  
**Implements:** Image generation capability  
**Read time:** ~10 minutes

**This section covers Stable Diffusion integration for generating images from text prompts.**

---

### 5.1 SD vs SDXL Overview {#51-sd-vs-sdxl-overview}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **Stable Diffusion (SD)** | Open-source AI that generates images from text descriptions | Our image generation capability |
| **SD 1.5/2.1** | Older versions, smaller, faster | Good for quick generations |
| **SDXL** | Newest version, higher quality, larger | Best quality but heavier |
| **U-Net** | The neural network architecture SD uses | Understanding helps with VRAM planning |
| **ComfyUI** | A visual workflow tool for Stable Diffusion | Efficient way to run SD |
| **Automatic1111** | Popular SD web interface | Alternative to ComfyUI, less efficient |

#### Quick Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚    SD 1.5/2.1    â”‚         SDXL             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output Size    â”‚ 512Ã—512          â”‚ 1024Ã—1024                â”‚
â”‚ VRAM Needed    â”‚ 6-8 GB           â”‚ 7-16 GB (varies)         â”‚
â”‚ Speed (3090)   â”‚ ~0.2-0.3s/image  â”‚ ~4-10s/image             â”‚
â”‚ Quality        â”‚ Good             â”‚ Excellent                â”‚
â”‚ Best For       â”‚ Quick previews   â”‚ Final outputs            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.2 VRAM Requirements & Performance {#52-vram-requirements-and-performance}

`[CORE]`

#### Detailed VRAM Breakdown

```
SD 1.5 (512Ã—512, 25 steps):
  â€¢ VRAM:    ~6-8 GB
  â€¢ Speed:   ~0.2-0.3 seconds per image
  â€¢ Rate:    ~4-5 images/second possible on 3090

SDXL Base (1024Ã—1024, 30 steps):
  â€¢ VRAM:    ~6-14 GB (depends on optimizations)
  â€¢ Speed:   ~4-10 seconds per image
  â€¢ With optimizations (OneDiff + Tiny VAE): 
    - VRAM drops to ~6.9 GB
    - Speed improves to ~4 seconds

SDXL with Refiner:
  â€¢ VRAM:    ~7-16 GB
  â€¢ Speed:   ~6-12 seconds per image
  â€¢ Higher quality details
```

âš¡ **Key Finding:** With optimizations, SDXL can run alongside a 7B LLM (4GB + 7GB = 11GB total).

---

### 5.3 Integrating with LLM Workloads {#53-integrating-with-llm-workloads}

`[CORE]`

#### The Contention Problem

**Image generation and LLM inference compete for the same GPU.**

```
Scenario: User chatting while generating an image

WRONG approach (simultaneous):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Mistral-7B (4GB) + SDXL (10GB) = 14GB â”‚
  â”‚  Both running = GPU thrashing          â”‚
  â”‚  Result: Both slow, possible crash     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RIGHT approach (serialized + priority):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. Chat request arrives               â”‚
  â”‚  2. Pause/queue image generation       â”‚
  â”‚  3. Process chat (fast, <1 sec)        â”‚
  â”‚  4. Resume image generation            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Recommended Strategy

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Image Generation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: ComfyUI + Sequential Processing

Setup:
  â€¢ Run ComfyUI as a separate process
  â€¢ Call it via HTTP API when images needed
  â€¢ Keep LLM models hot; unload for big image jobs

Priority:
  â€¢ Chat/code requests ALWAYS preempt image generation
  â€¢ Queue images, show progress to user
  â€¢ Run image generation when GPU is otherwise idle

VRAM Management:
  â€¢ For quick SD 1.5: Can run alongside 7B model
  â€¢ For quality SDXL: Unload secondary LLM, keep orchestrator

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Key Takeaways

- SD 1.5: Fast, lower VRAM, good for previews
- SDXL: Higher quality, needs more VRAM and time
- Never run heavy image generation and LLM simultaneously
- Use ComfyUI for efficiency (better than Automatic1111)
- Implement job queuing with priority for interactive requests

---
---

# PART III: DATA ARCHITECTURE

---

## 6.0 Local-First Data Fundamentals {#6-local-first-data-fundamentals}

**Prerequisites:** Section 1.1 (Local-First Concept)  
**Related to:** Sections 7, 8, 9  
**Implements:** Data storage and sync strategy  
**Read time:** ~20 minutes

**This section explains the core challenge of local-first appsâ€”keeping data consistent across devicesâ€”and introduces the technology that solves it.**

---

### 6.1 What "Local-First" Really Means {#61-what-local-first-really-means}

`[CORE]`

#### The Promise

**Local-first software keeps your data on your devices, with optional cloud sync.** This gives you:

- **Ownership:** Your files are literally on your computer
- **Speed:** No network round-trip for every action
- **Offline:** Works without internet
- **Privacy:** Data doesn't have to touch company servers

#### The Challenge

**What happens when you edit the same document on two devices while offline?**

```
Timeline of a Conflict:

Monday 9am:  Both laptop and tablet sync â†’ same document state
Monday 10am: You go offline on both devices
Monday 11am: On laptop, you add paragraph A
Monday 11am: On tablet, you add paragraph B
Monday 2pm:  Both come online again

QUESTION: What should the document look like now?

  Option 1: Last-write-wins â†’ One person's work is LOST âŒ
  Option 2: Keep both versions, ask user to choose â†’ Annoying âŒ  
  Option 3: Automatically merge both changes â†’ âœ“ This is what CRDTs do
```

---

### 6.2 The Problem: Concurrent Editing {#62-the-problem-concurrent-editing}

`[CORE]`

#### Why This is Hard

**Traditional databases assume one "source of truth."** When you save a document, you overwrite what was there. If two people edit simultaneously, one overwrites the other.

```
Traditional Approach (Google Docs style):
  
  [Device A]                    [Server]                    [Device B]
      â”‚                            â”‚                            â”‚
      â”‚â”€â”€â”€â”€ Edit: "Hello" â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                            â”‚
      â”‚                            â”‚<â”€â”€â”€â”€ Edit: "World" â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚                            â”‚                            â”‚
      â”‚                   Server decides order                  â”‚
      â”‚                   "Hello" then "World"                  â”‚
      â”‚                   OR "World" then "Hello"               â”‚
      â”‚                            â”‚                            â”‚
      
  Problem: Server is required. No offline support.
```

```
Local-First Challenge:
  
  [Device A - Offline]                              [Device B - Offline]
      â”‚                                                  â”‚
      â”‚ Edit: "Hello"                                    â”‚ Edit: "World"
      â”‚     (no server to ask!)                          â”‚     (no server!)
      â”‚                                                  â”‚
      â–¼                                                  â–¼
  Local state: "Hello"                          Local state: "World"
  
  Later, when both reconnect... now what?
```

---

### 6.3 Solution: CRDTs Explained {#63-solution-crdts-explained}

`[CORE]`

#### Jargon Glossary

| Term | Plain English | Why It Matters |
|------|---------------|----------------|
| **CRDT** | Conflict-free Replicated Data Typeâ€”a special data structure that can merge automatically | The technology that makes local-first sync possible |
| **Merge** | Combining two versions into one | CRDTs guarantee merges always produce the same result |
| **Eventual Consistency** | All devices eventually have the same data, even if they're temporarily different | What CRDTs guarantee |
| **Operation-based (Op-based)** | A CRDT style that syncs by sharing operations ("insert 'A' at position 3") | One approach |
| **State-based** | A CRDT style that syncs by sharing entire state snapshots | Another approach |

#### The Magic of CRDTs

**CRDTs are data structures designed so that merging always works and always produces the same result.**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: How CRDTs Work
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key insight: Instead of storing "the text is Hello", store 
"character H was inserted by device A at time T1, character e 
was inserted by device A at time T2..."

This extra information lets us ALWAYS merge correctly:

Device A's operations:              Device B's operations:
  1. Insert "H" at start             1. Insert "W" at start
  2. Insert "e" after "H"            2. Insert "o" after "W"
  3. Insert "l" after "e"            3. Insert "r" after "o"
  ...                                ...
  
When merging:
  â€¢ Each operation has a unique ID
  â€¢ We can replay ALL operations in a deterministic order
  â€¢ Both devices end up with: "HelloWorld" (or "WorldHello")
  â€¢ The SAME result regardless of which device syncs first!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Types of CRDT Data Structures

```
For Text Documents:
  â€¢ Tracks each character insertion/deletion
  â€¢ Handles concurrent typing in different places
  â€¢ Libraries: Yjs (Y.Text), Automerge (Text type)

For JSON-like Objects:
  â€¢ Tracks changes to keys and values
  â€¢ Handles concurrent edits to different fields
  â€¢ Libraries: Yjs (Y.Map), Automerge (objects)

For Lists/Arrays:
  â€¢ Tracks insertions, deletions, moves
  â€¢ Handles concurrent list modifications
  â€¢ Libraries: Yjs (Y.Array), Loro (MovableList)

For Rich Text:
  â€¢ Tracks formatting (bold, italic, etc.)
  â€¢ Handles concurrent formatting changes
  â€¢ Libraries: Yjs + editor bindings
```

#### What CRDTs DON'T Solve

âš ï¸ **CRDTs merge automatically, but "automatic" doesn't mean "smart."**

```
Example: Two users both edit the SAME sentence:

Original:        "The quick brown fox"
User A changes:  "The fast brown fox"      (quick â†’ fast)
User B changes:  "The quick red fox"       (brown â†’ red)

CRDT merge:      "The fast red fox"        (both changes applied)

Is this right? Maybe! But maybe User A wanted to keep "brown" and 
User B wanted to keep "quick". The CRDT doesn't understand INTENT,
it just merges the characters.
```

ğŸ’¡ **Key insight:** CRDTs prevent data loss and conflicts, but users may still need to review merged results for semantic correctness.

#### Key Takeaways

- CRDTs are special data structures that merge automatically
- They track operations (not just state) to enable consistent merging
- Different CRDT types for different data: text, objects, lists
- CRDTs merge mechanicallyâ€”they don't understand meaning
- This is the foundation for local-first sync

---

## 7.0 CRDT Libraries Comparison {#7-crdt-libraries-comparison}

**Prerequisites:** Section 6.3 (CRDTs Explained)  
**Related to:** Section 8 (Database Integration)  
**Implements:** Choosing a CRDT library  
**Read time:** ~20 minutes

**This section compares the main CRDT libraries and recommends which to use.**

---

### 7.1 Yjs Deep Dive {#71-yjs-deep-dive}

`[CORE]`

#### What is Yjs?

**Yjs is the most popular CRDT library for JavaScript/TypeScript applications.** It's battle-tested, fast, and has excellent editor integrations.

#### Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Yjs                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     JavaScript/TypeScript                         â”‚
â”‚ Also:         Rust port (Yrs), Python, Swift, Ruby          â”‚
â”‚ Data Types:   Y.Text, Y.Map, Y.Array, Y.XmlFragment         â”‚
â”‚ Performance:  Excellent (~260K inserts: 1s, 10MB memory)    â”‚
â”‚ History:      No full history (snapshots optional)          â”‚
â”‚ Sync:         WebSocket, WebRTC, custom providers           â”‚
â”‚ Editors:      ProseMirror, TipTap, Monaco, Quill, more      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### How Yjs Works

```javascript
// Basic Yjs usage
import * as Y from 'yjs'

// Create a document
const doc = new Y.Doc()

// Get a shared text type
const text = doc.getText('content')

// Make changes
text.insert(0, 'Hello World')

// Observe changes (for updating UI)
text.observe(event => {
  console.log('Text changed:', text.toString())
})

// Export for sync/storage
const update = Y.encodeStateAsUpdate(doc)  // Binary format
```

#### Pros and Cons

**Pros:**
- â­ Best performance and memory efficiency
- â­ Rich editor integrations (drop-in for popular editors)
- â­ Large community, many examples
- â­ Multiple sync options (WebSocket, WebRTC, file-based)
- â­ Cross-platform via ports (Yrs for Rust/Tauri)

**Cons:**
- âš ï¸ No built-in full history (only current state)
- âš ï¸ Learning curve for understanding shared types
- âš ï¸ Need to manually handle persistence

---

### 7.2 Automerge Deep Dive {#72-automerge-deep-dive}

#### What is Automerge?

**Automerge is an academically rigorous CRDT library with full history tracking.** Version 2 is written in Rust with JavaScript bindings.

#### Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Automerge                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     Rust core, JS/WASM bindings                   â”‚
â”‚ Data Types:   JSON-like objects, lists, text, counters      â”‚
â”‚ Performance:  Slower (~260K inserts: 1.8s, 44MB memory)     â”‚
â”‚ History:      Full operation history (like Git)             â”‚
â”‚ Sync:         Custom sync protocol                          â”‚
â”‚ Best For:     When you need complete version history        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Pros and Cons

**Pros:**
- â­ Full version historyâ€”can reconstruct any past state
- â­ Cleaner APIâ€”works like normal JS objects
- â­ Academic backingâ€”provably correct
- â­ Good for debugging (can replay history)

**Cons:**
- âš ï¸ Higher memory usage (~4x more than Yjs)
- âš ï¸ Slower for large documents
- âš ï¸ Larger storage requirements (keeps all operations)
- âš ï¸ Fewer editor integrations

---

### 7.3 Loro and Emerging Options {#73-loro-and-emerging-options}

#### What is Loro?

**Loro is a new CRDT library aiming to combine the best of Yjs and Automerge.** It offers high performance AND full history.

#### Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Loro                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Language:     Rust core, JS/WASM bindings                   â”‚
â”‚ Data Types:   MovableList, Map, Tree, Text, Counter         â”‚
â”‚ Performance:  Very high (designed to beat both Yjs/Automerge)â”‚
â”‚ History:      Full version DAG (like Git)                   â”‚
â”‚ Unique:       Movable trees (great for outlines/kanban)     â”‚
â”‚ Maturity:     Newer, less battle-tested                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Pros and Cons

**Pros:**
- â­ Rust-native (great for Tauri)
- â­ Full history like Automerge, speed like Yjs (claimed)
- â­ Movable trees perfect for hierarchical data (outlines, kanban)
- â­ Time-travel debugging possible

**Cons:**
- âš ï¸ Newer, less proven in production
- âš ï¸ Smaller community and fewer integrations
- âš ï¸ API may still change

---

### 7.4 Recommendation: Which CRDT Library? {#74-recommendation-which-crdt-library}

`[CORE]`

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: CRDT Library Choice
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FOR ELECTRON + TypeScript:
  â””â”€â”€ Use Yjs
      â€¢ Best performance and editor integrations
      â€¢ Largest community, most resources
      â€¢ Add snapshots for version history if needed

FOR TAURI + Rust:
  â””â”€â”€ Consider Loro or Yrs (Yjs Rust port)
      â€¢ Loro: If you need movable trees and version history
      â€¢ Yrs: If you want Yjs compatibility across platforms

RECOMMENDATION FOR THIS PROJECT (starting):
  â””â”€â”€ Start with Yjs
      â€¢ Proven, fast, well-documented
      â€¢ Works in both Electron and Tauri (via Yrs)
      â€¢ Easiest path to editor integration
      â€¢ Migrate to Loro later if needed for hierarchical data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Comparison Summary Table

| Aspect | Yjs | Automerge | Loro |
|--------|-----|-----------|------|
| Performance | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ (claimed) |
| Memory | â­â­â­â­â­ (10MB) | â­â­â­ (44MB) | â­â­â­â­ |
| Full History | âŒ (snapshots only) | âœ… | âœ… |
| Editor Integration | â­â­â­â­â­ | â­â­ | â­â­ |
| Rust Native | Via Yrs | Via WASM | âœ… Native |
| Maturity | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| Movable Trees | âŒ | âŒ | âœ… |

---

## 8.0 Database & Sync Patterns {#8-database-and-sync-patterns}

**Prerequisites:** Sections 6, 7  
**Related to:** Section 9 (Conflict Resolution UX)  
**Implements:** Data storage architecture  
**Read time:** ~15 minutes

**This section explains how to combine CRDT with a local database for querying and persistence.**

---

### 8.1 Local Database Options {#81-local-database-options}

`[CORE]`

#### Why Use a Database with CRDT?

**CRDTs handle sync, but databases handle queries.** You often need both:

```
CRDT alone:
  âœ“ Sync across devices
  âœ“ Merge concurrent edits
  âœ— "Find all documents containing 'budget'" â†’ Slow (must scan all)
  âœ— "Sort documents by date" â†’ Not built-in
  âœ— Complex queries â†’ Difficult

Database alone:
  âœ“ Fast queries with indexes
  âœ“ Sort, filter, aggregate
  âœ— Sync across devices â†’ Conflicts!
  âœ— Offline merge â†’ Data loss

CRDT + Database:
  âœ“ Sync via CRDT
  âœ“ Query via database
  âœ“ Best of both worlds
```

#### SQLite: The Recommended Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SQLite                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:         Embedded SQL database                         â”‚
â”‚ Storage:      Single file on disk                           â”‚
â”‚ Performance:  Very fast for local operations                â”‚
â”‚ Features:     Full SQL, indexes, full-text search (FTS)     â”‚
â”‚ Size:         Tiny (library is ~1MB)                        â”‚
â”‚ Reliability:  Extremely battle-tested                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why SQLite for this project:**
- â­ Standard across all platforms (Windows, macOS, Linux)
- â­ Works great with Electron AND Tauri
- â­ Full-text search for finding documents
- â­ ACID guarantees (data integrity)
- â­ Single file = easy backup

---

### 8.2 Combining CRDT and Database {#82-combining-crdt-and-database}

`[CORE]`

#### Architecture Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚    User Edit                                                â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚   CRDT    â”‚  â—„â”€â”€â”€ Handles: Sync, Merge, Collaboration   â”‚
â”‚  â”‚  (Yjs)    â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â”‚ On every CRDT change:                              â”‚
â”‚        â”‚ â€¢ Update SQLite                                    â”‚
â”‚        â”‚ â€¢ Update indexes                                   â”‚
â”‚        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚  SQLite   â”‚  â—„â”€â”€â”€ Handles: Queries, Search, Indexes     â”‚
â”‚  â”‚           â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â”‚ Query results                                      â”‚
â”‚        â–¼                                                    â”‚
â”‚    UI Display                                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### What Goes Where?

```
In CRDT (Yjs):
  â€¢ Document content (text, rich text)
  â€¢ Board/canvas positions
  â€¢ List ordering
  â€¢ Everything that needs to sync and merge

In SQLite:
  â€¢ Document metadata (title, dates, tags)
  â€¢ Search indexes
  â€¢ User preferences
  â€¢ Derived/computed data
  â€¢ Anything that needs fast querying

Example Schema:
  documents:
    - id (primary key)
    - title (indexed)
    - created_at
    - updated_at
    - tags (indexed)
    - crdt_id (reference to CRDT document)
    - content_preview (first 200 chars for search)
```

#### Sync Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE SYNC FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. USER MAKES EDIT (Device A)
   â”‚
   â”œâ”€â”€â–º CRDT update applied locally
   â”‚
   â”œâ”€â”€â–º SQLite updated with new content/metadata
   â”‚
   â””â”€â”€â–º CRDT update sent to sync server (or peer)

2. SYNC UPDATE RECEIVED (Device B)
   â”‚
   â”œâ”€â”€â–º CRDT merges incoming update
   â”‚
   â”œâ”€â”€â–º SQLite updated to reflect merged state
   â”‚
   â””â”€â”€â–º UI refreshes to show changes

3. CONFLICT HANDLED AUTOMATICALLY
   â”‚
   â””â”€â”€â–º CRDT merge is deterministic
       â”‚
       â””â”€â”€â–º Same SQLite state on all devices (eventually)
```

---

### 8.3 Sync Topologies {#83-sync-topologies}

#### Options for Syncing Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNC TOPOLOGY OPTIONS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION A: Peer-to-Peer                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚Dev Aâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Dev Bâ”‚   Direct device-to-device          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚     â”‚               â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚  Pros: No server needed, private                             â”‚
â”‚  Cons: Both devices must be online simultaneously            â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION B: Central Server                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚Dev Aâ”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚Serverâ”‚â—„â”€â”€â”€â”€â”€â”€â”‚Dev Bâ”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚  Pros: Works when only one device online                    â”‚
â”‚  Cons: Requires running/paying for server                   â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  OPTION C: File Sync (OneDrive/Dropbox)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚Dev Aâ”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚OneDriveâ”‚â—„â”€â”€â”€â”€â”€â”€â”‚Dev Bâ”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  Pros: No custom server, leverages existing sync             â”‚
â”‚  Cons: File-level conflicts, coarse merging                  â”‚
â”‚        (need CRDT on top to handle conflicts)               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Recommendation

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Sync Topology
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 1 (MVP): File Sync + CRDT
  â€¢ Store CRDT updates as files in a synced folder
  â€¢ Let OneDrive/Dropbox/iCloud handle file sync
  â€¢ CRDT handles merge when files conflict
  â€¢ Zero server infrastructure needed

PHASE 2 (Multi-user): Central Sync Server
  â€¢ Build or use WebSocket sync server
  â€¢ Real-time collaboration possible
  â€¢ More complex but better UX

Libraries that help:
  â€¢ y-indexeddb: Local persistence for Yjs
  â€¢ y-websocket: WebSocket sync for Yjs
  â€¢ ElectricSQL: Postgres â†” SQLite sync
  â€¢ Replicache: Client-server sync framework

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 9.0 Conflict Resolution UX {#9-conflict-resolution-ux}

**Prerequisites:** Sections 6, 7, 8  
**Related to:** User interface design  
**Implements:** How users experience data sync  
**Read time:** ~10 minutes

**This section covers how to show sync status and handle conflicts in the UI.**

---

### 9.1 User-Facing Conflict Patterns {#91-user-facing-conflict-patterns}

`[CORE]`

#### The Good News

**Most of the time, users shouldn't see conflicts at all.** CRDTs merge automatically, and if users edit different parts of a document, everything "just works."

#### When to Show Something

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHEN TO SHOW SYNC FEEDBACK                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ALWAYS SHOW:                                                â”‚
â”‚    â€¢ Sync status indicator (synced âœ“, syncing â†», offline âš¡)â”‚
â”‚    â€¢ "X minutes ago" last sync time                         â”‚
â”‚                                                              â”‚
â”‚  SHOW ON EVENT:                                              â”‚
â”‚    â€¢ "Document updated by another device" notification      â”‚
â”‚    â€¢ Highlight recently changed sections (briefly)          â”‚
â”‚                                                              â”‚
â”‚  SHOW ON POTENTIAL ISSUE:                                    â”‚
â”‚    â€¢ "This section was edited while you were offline.       â”‚
â”‚       Review the changes?" (when same paragraph edited)     â”‚
â”‚                                                              â”‚
â”‚  DON'T BOTHER USER WITH:                                     â”‚
â”‚    â€¢ Every automatic merge (too noisy)                      â”‚
â”‚    â€¢ Technical details ("CRDT vector clock updated")        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Simple Sync Status UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    âœ“ Synced 2 min ago    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

OR when syncing:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    â†» Syncing...          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

OR when offline:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Document.md                    âš¡ Working offline     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

---

### 9.2 Version History UI {#92-version-history-ui}

`[CORE]`

#### Why Provide Version History

Even with automatic merging, users want:
- **Safety net:** "I accidentally deleted something, can I get it back?"
- **Audit trail:** "What changed since yesterday?"
- **Comparison:** "What's different from the old version?"

#### Implementation Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 VERSION HISTORY PANEL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â—„ Document History                                          â”‚
â”‚                                                              â”‚
â”‚  TODAY                                                       â”‚
â”‚  â”œâ”€â”€ 3:45 PM - You edited (current)                        â”‚
â”‚  â”œâ”€â”€ 2:30 PM - Synced from MacBook                         â”‚
â”‚  â””â”€â”€ 10:15 AM - You edited                                  â”‚
â”‚                                                              â”‚
â”‚  YESTERDAY                                                   â”‚
â”‚  â”œâ”€â”€ 8:00 PM - You edited                                   â”‚
â”‚  â””â”€â”€ 2:00 PM - Created                                      â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  [Preview Selected] [Restore to This Version]               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Technical Implementation

```
For Yjs (no built-in history):
  â€¢ Take periodic snapshots (every N minutes or on significant changes)
  â€¢ Store snapshots in SQLite with timestamps
  â€¢ To restore: load old snapshot, create new CRDT state
  
For Automerge/Loro (built-in history):
  â€¢ History is automatically tracked
  â€¢ Can "time travel" to any past state
  â€¢ Trade-off: larger storage requirements
```

#### Key Takeaways

- CRDTs handle most conflicts invisibly
- Show sync status but don't over-communicate
- Highlight concurrent edits from other devices
- Provide version history as a safety net
- For Yjs: implement snapshots manually; for Automerge/Loro: built-in

---
---

# PART IV: PLUGIN & EXTENSION SYSTEM

---

## 10.0 Plugin Architecture Fundamentals {#10-plugin-architecture-fundamentals}

**Prerequisites:** Section 1.2 (Project Overview)  
**Related to:** Sections 11, 12  
**Implements:** Extensibility strategy  
**Read time:** ~15 minutes

**This section explains why plugins matter and what we can learn from existing plugin systems.**

---

### 10.1 Why Plugins Matter {#101-why-plugins-matter}

`[CORE]`

#### The Power of Extensibility

**Plugins let your users (and you) add features without changing the core application.**

```
Without Plugins:
  â€¢ Every feature request requires core development
  â€¢ One-size-fits-all: everyone gets everything or nothing
  â€¢ Slow iteration: changes go through your release cycle
  â€¢ Limited: can only do what YOU thought of

With Plugins:
  â€¢ Users can add their own integrations
  â€¢ Personalization: each user's setup is unique
  â€¢ Community innovation: features you never imagined
  â€¢ Faster: plugins ship independently of core app
```

#### Examples of Plugin Value

```
Your app with no plugins:
  â””â”€â”€ Basic AI chat + documents
  
Your app with plugins:
  â”œâ”€â”€ Todoist integration (someone's plugin)
  â”œâ”€â”€ Custom AI model loader (power user)
  â”œâ”€â”€ Citation manager (academic user)
  â”œâ”€â”€ Code formatter (developer)
  â”œâ”€â”€ Voice commands (accessibility)
  â””â”€â”€ [Hundreds more possibilities]
```

---

### 10.2 Learning from Existing Systems {#102-learning-from-existing-systems}

`[CORE]`

#### VS Code: The Gold Standard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                VS CODE EXTENSION MODEL                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Separate "Extension Host" process              â”‚
â”‚  Language:    JavaScript/TypeScript                          â”‚
â”‚  Manifest:    package.json with "contributes" section        â”‚
â”‚  Security:    No sandboxâ€”full Node.js access                â”‚
â”‚  Trust:       "Trust this publisher?" prompt                 â”‚
â”‚                                                              â”‚
â”‚  What they got right:                                        â”‚
â”‚    âœ“ Rich API for extending UI                              â”‚
â”‚    âœ“ Lazy loading (activation events)                       â”‚
â”‚    âœ“ Declarative contributions (commands, menus)            â”‚
â”‚    âœ“ Huge ecosystem (50,000+ extensions)                    â”‚
â”‚                                                              â”‚
â”‚  What we'd do differently:                                   â”‚
â”‚    â€¢ Add sandboxing (they have none)                        â”‚
â”‚    â€¢ Require permission declarations                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Figma: Security-First

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FIGMA PLUGIN MODEL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Sandboxed JavaScript (no DOM, no XHR)          â”‚
â”‚  UI:          Separate iframe for plugin UI                  â”‚
â”‚  API:         Only Figma document access via figma.*         â”‚
â”‚  Network:     Must whitelist domains in manifest             â”‚
â”‚                                                              â”‚
â”‚  What they got right:                                        â”‚
â”‚    âœ“ True sandboxâ€”plugins can't escape                      â”‚
â”‚    âœ“ UI separated from logic                                â”‚
â”‚    âœ“ Explicit network permissions                           â”‚
â”‚    âœ“ User can cancel runaway plugins                        â”‚
â”‚                                                              â”‚
â”‚  What we'd adapt:                                            â”‚
â”‚    â€¢ Similar sandbox model                                   â”‚
â”‚    â€¢ Manifest-declared network permissions                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Browser Extensions: Permission Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             BROWSER EXTENSION MODEL (Manifest V3)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Key Innovation: Explicit permissions                        â”‚
â”‚                                                              â”‚
â”‚  manifest.json:                                              â”‚
â”‚  {                                                           â”‚
â”‚    "permissions": ["storage", "tabs"],                      â”‚
â”‚    "host_permissions": ["https://api.example.com/*"]        â”‚
â”‚  }                                                           â”‚
â”‚                                                              â”‚
â”‚  User sees at install:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ "MyExtension" wants to:              â”‚                   â”‚
â”‚  â”‚ â€¢ Read and change your browsing data â”‚                   â”‚
â”‚  â”‚   on api.example.com                 â”‚                   â”‚
â”‚  â”‚ â€¢ Store data locally                 â”‚                   â”‚
â”‚  â”‚                                      â”‚                   â”‚
â”‚  â”‚  [Add Extension]  [Cancel]           â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  What we'd copy:                                             â”‚
â”‚    âœ“ Manifest-declared permissions                          â”‚
â”‚    âœ“ User consent at install                                â”‚
â”‚    âœ“ Clear permission descriptions                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Obsidian: Cautionary Tale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OBSIDIAN PLUGIN MODEL                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Runtime:     Main Electron process (no isolation!)          â”‚
â”‚  Access:      Full Node.jsâ€”plugins can do ANYTHING          â”‚
â”‚  Trust:       Community ratings + open source review         â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ Security Issue:                                         â”‚
â”‚  "Obsidian plugins have all the same permissions you do     â”‚
â”‚  to read/write all the files in your vault"                 â”‚
â”‚                                                              â”‚
â”‚  A malicious plugin could:                                   â”‚
â”‚    â€¢ Read any file on your computer                         â”‚
â”‚    â€¢ Send data to external servers                          â”‚
â”‚    â€¢ Install malware                                         â”‚
â”‚    â€¢ Encrypt your files (ransomware)                        â”‚
â”‚                                                              â”‚
â”‚  What NOT to copy:                                           â”‚
â”‚    âœ— No sandboxing                                          â”‚
â”‚    âœ— Full system access                                     â”‚
â”‚    âœ— Trust based only on community review                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Takeaways

- VS Code: Rich API, lazy loading, declarative contributions (good); no sandbox (bad)
- Figma: True sandbox, UI separation, explicit permissions (all good)
- Browser: Permission model with user consent (excellent)
- Obsidian: No security (avoid this pattern)

---

## 11.0 Plugin System Design {#11-plugin-system-design}

**Prerequisites:** Section 10  
**Related to:** Section 12 (Security)  
**Implements:** Plugin API and structure  
**Read time:** ~15 minutes

**This section designs our plugin manifest, registration, and API patterns.**

---

### 11.1 Manifest & Registration {#111-manifest-and-registration}

`[CORE]`

#### Plugin Manifest Format

```json
{
  "manifestVersion": 1,
  "id": "com.example.my-plugin",
  "name": "My Awesome Plugin",
  "version": "1.2.3",
  "description": "Does something useful",
  "author": "Your Name",
  "homepage": "https://github.com/you/plugin",
  
  "minAppVersion": "2.0.0",
  "main": "dist/index.js",
  "ui": "dist/ui.html",
  
  "type": ["automation", "ui"],
  
  "permissions": {
    "readData": ["documents", "boards"],
    "writeData": ["documents"],
    "filesystem": false,
    "network": ["https://api.myservice.com"],
    "ai": {
      "models": ["local"],
      "maxTokensPerDay": 10000
    }
  },
  
  "contributes": {
    "commands": [
      {
        "id": "myplugin.doThing",
        "title": "Do the Thing",
        "shortcut": "Ctrl+Shift+T"
      }
    ],
    "menus": [
      {
        "location": "tools",
        "items": [{ "command": "myplugin.doThing" }]
      }
    ]
  }
}
```

#### Key Manifest Sections Explained

| Section | Purpose |
|---------|---------|
| `id` | Unique identifier (reverse domain style) |
| `main` | Entry point JavaScript file |
| `ui` | Optional HTML file for plugin UI panel |
| `permissions` | What the plugin is allowed to access |
| `contributes` | What UI elements the plugin adds |

---

### 11.2 Plugin Types & Categories {#112-plugin-types-and-categories}

#### Three Main Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PLUGIN TYPES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. AUTOMATION PLUGINS                                       â”‚
â”‚     â€¢ Background tasks and macros                           â”‚
â”‚     â€¢ Triggered by events or commands                       â”‚
â”‚     â€¢ May not have UI                                        â”‚
â”‚     Example: "Auto-backup to Dropbox"                       â”‚
â”‚                                                              â”‚
â”‚  2. UI PLUGINS                                               â”‚
â”‚     â€¢ Add panels, views, or widgets                         â”‚
â”‚     â€¢ Render custom interfaces                               â”‚
â”‚     â€¢ Interact with user directly                           â”‚
â”‚     Example: "Kanban board view"                            â”‚
â”‚                                                              â”‚
â”‚  3. AI TOOL PLUGINS                                          â”‚
â”‚     â€¢ Add new AI capabilities                               â”‚
â”‚     â€¢ May integrate external models or APIs                 â”‚
â”‚     â€¢ Often combine UI + automation                         â”‚
â”‚     Example: "AI image generator", "Translation tool"       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 11.3 API Design Patterns {#113-api-design-patterns}

`[CORE]`

#### Registration API Example

```javascript
// Plugin code (index.js)
export function activate(api) {
  // Register a command
  api.registerCommand("myplugin.sayHello", {
    title: "Say Hello",
    handler: async () => {
      api.showNotification("Hello from my plugin!");
    }
  });
  
  // Register a view
  api.registerView("myplugin.dashboard", {
    title: "My Dashboard",
    location: "sidebar",
    render: (container) => {
      container.innerHTML = "<h1>Dashboard</h1>";
    }
  });
  
  // Subscribe to events
  api.onDocumentSaved((doc) => {
    console.log("Document saved:", doc.id);
  });
}

export function deactivate() {
  // Cleanup when plugin is disabled
}
```

#### Workspace Data API

```javascript
// Reading data
const docs = await api.workspace.query({
  type: "document",
  where: { tags: { contains: "important" } },
  limit: 10
});

// Writing data
await api.workspace.update("document", docId, {
  title: "New Title"
});

// Subscribing to changes
api.workspace.onDidChange((change) => {
  if (change.type === "document") {
    // Handle document change
  }
});
```

#### Key Design Principles

ğŸ“Œ **Explicit Registration:** Plugins declare what they contribute via manifest AND register at runtime

ğŸ“Œ **Namespaced:** All plugin commands/views prefixed with plugin ID (`myplugin.command`)

ğŸ“Œ **Promise-based:** All async operations return Promises

ğŸ“Œ **Observable:** Plugins can subscribe to app events

ğŸ“Œ **Permission-gated:** API calls check permissions before executing

---

## 12.0 Sandboxing & Security {#12-sandboxing-and-security}

**Prerequisites:** Sections 10, 11  
**Related to:** Plugin implementation  
**Implements:** Plugin security architecture  
**Read time:** ~25 minutes

**This section covers how to run untrusted plugin code safely.**

---

### 12.1 Why Sandbox Untrusted Code {#121-why-sandbox-untrusted-code}

`[CORE]`

#### The Risk

**Any code you run can do anything your user can do** (unless sandboxed).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHAT UNSANDBOXED CODE CAN DO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  âš ï¸ A malicious plugin WITHOUT sandboxing could:            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Read ANY file on the computer                            â”‚
â”‚    - Browser passwords                                       â”‚
â”‚    - SSH keys                                                â”‚
â”‚    - Financial documents                                     â”‚
â”‚                                                              â”‚
â”‚  â€¢ Send data to external servers                            â”‚
â”‚    - Steal personal information                             â”‚
â”‚    - Exfiltrate business documents                          â”‚
â”‚                                                              â”‚
â”‚  â€¢ Modify or delete files                                   â”‚
â”‚    - Ransomware (encrypt and demand payment)                â”‚
â”‚    - Destroy data                                            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Install malware                                          â”‚
â”‚    - Keyloggers                                              â”‚
â”‚    - Cryptocurrency miners                                   â”‚
â”‚                                                              â”‚
â”‚  This is NOT hypotheticalâ€”it happens regularly              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Defense Layers

```
Security = Multiple Layers

Layer 1: PERMISSION MODEL
  â€¢ Plugin declares what it needs
  â€¢ User consents at install
  â€¢ App only grants what was approved

Layer 2: SANDBOX
  â€¢ Plugin code runs in isolation
  â€¢ Cannot access system outside sandbox
  â€¢ Even if code is malicious, damage is limited

Layer 3: REVIEW PROCESS
  â€¢ Marketplace review before listing
  â€¢ Automated security scanning
  â€¢ Community reporting

Layer 4: MONITORING
  â€¢ Track plugin behavior
  â€¢ Alert on suspicious activity
  â€¢ Ability to remotely disable malicious plugins
```

---

### 12.2 Sandboxing Technologies Compared {#122-sandboxing-technologies-compared}

`[CORE]`

#### Overview Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Technology   â”‚ Security   â”‚ Performance â”‚ Complexity   â”‚ Best For     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WASM         â”‚ â­â­â­â­â­    â”‚ â­â­â­â­      â”‚ â­â­â­ Medium  â”‚ Most plugins â”‚
â”‚ Pyodide      â”‚ â­â­â­â­â­    â”‚ â­â­â­        â”‚ â­â­â­ Medium  â”‚ Python AI    â”‚
â”‚ OS Subprocessâ”‚ â­â­â­â­      â”‚ â­â­â­â­â­     â”‚ â­â­ Complex  â”‚ Legacy code  â”‚
â”‚ Containers   â”‚ â­â­â­â­â­    â”‚ â­â­          â”‚ â­ Very High  â”‚ Heavy/risky  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### WebAssembly (WASM) â€” Recommended

**What it is:** A binary instruction format that runs in a secure sandbox.

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    CORE CONCEPT: WASM Sandbox
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Plugin code compiles to WASM (from Rust, C++, AssemblyScript)
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    YOUR APPLICATION                      â”‚
  â”‚                                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚              WASM SANDBOX                        â”‚    â”‚
  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
  â”‚  â”‚  â”‚         PLUGIN CODE                       â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot access filesystem               â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot make network requests           â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Cannot read memory outside sandbox     â”‚  â”‚    â”‚
  â”‚  â”‚  â”‚  â€¢ Can ONLY call functions YOU expose     â”‚  â”‚    â”‚
  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
  â”‚  â”‚                                                  â”‚    â”‚
  â”‚  â”‚  Exposed Functions (your API):                  â”‚    â”‚
  â”‚  â”‚  â€¢ readDocument(id) â†’ document                  â”‚    â”‚
  â”‚  â”‚  â€¢ saveDocument(id, content)                    â”‚    â”‚
  â”‚  â”‚  â€¢ showUI(html)                                 â”‚    â”‚
  â”‚  â”‚  â€¢ [nothing elseâ€”no system access]             â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why WASM is secure:**
- Memory is completely isolated (can't read/write outside sandbox)
- No system calls unless explicitly provided
- Even buggy code can't escape
- Industry-proven (used by Figma, Cloudflare, etc.)

**Performance:**
- Near-native speed (JIT compiled)
- Fast startup (milliseconds)
- Small overhead

**Complexity:**
- Plugins must be compiled to WASM
- Need to design the host API carefully
- Debugging is harder than native code

---

#### Pyodide (Python in WASM)

**What it is:** Full Python interpreter compiled to WASM.

```
Pyodide gives you Python plugins with WASM security.

Pros:
  âœ“ Full Python ecosystem (numpy, pandas, etc.)
  âœ“ Inherits WASM sandbox properties
  âœ“ Plugin authors write normal Python

Cons:
  âœ— Slower than native Python
  âœ— Large initial download (~10MB+)
  âœ— Startup time can be significant
```

**Best for:** AI/data plugins that need Python libraries.

---

#### OS Subprocess Sandboxing

**What it is:** Running plugins as separate OS processes with restricted permissions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 OS-LEVEL SANDBOXING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Main App                    Plugin Process                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           â”‚  IPC/Pipes   â”‚ Restricted by:            â”‚   â”‚
â”‚  â”‚   Your    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ seccomp (Linux)         â”‚   â”‚
â”‚  â”‚   App     â”‚              â”‚ â€¢ AppArmor (Linux)        â”‚   â”‚
â”‚  â”‚           â”‚              â”‚ â€¢ sandbox-exec (macOS)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â€¢ AppContainer (Windows)  â”‚   â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Can block:                                                  â”‚
â”‚    â€¢ File access outside allowed paths                      â”‚
â”‚    â€¢ Network access                                          â”‚
â”‚    â€¢ Process spawning                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- Plugins written in normal languages (Python, Node)
- Native performance
- Familiar debugging

**Cons:**
- Different implementation per OS
- Easier to misconfigure (weaker guarantee)
- Heavier than WASM (process overhead)

---

### 12.3 Permission Models {#123-permission-models}

`[CORE]`

#### Capability Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PERMISSION CATEGORIES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  FILESYSTEM SCOPES                                           â”‚
â”‚  â”œâ”€â”€ fs.read[/workspace/*]     Read specific paths          â”‚
â”‚  â”œâ”€â”€ fs.write[/workspace/out]  Write to specific paths      â”‚
â”‚  â””â”€â”€ fs.none                   No filesystem access         â”‚
â”‚                                                              â”‚
â”‚  NETWORK SCOPES                                              â”‚
â”‚  â”œâ”€â”€ net.none                  No network (default)         â”‚
â”‚  â”œâ”€â”€ net.host[api.example.com] Specific domains only        â”‚
â”‚  â””â”€â”€ net.any                   Unrestricted (dangerous)     â”‚
â”‚                                                              â”‚
â”‚  AI/MODEL SCOPES                                             â”‚
â”‚  â”œâ”€â”€ ai.none                   Cannot use AI                â”‚
â”‚  â”œâ”€â”€ ai.local                  Local models only            â”‚
â”‚  â”œâ”€â”€ ai.cloud                  Can call cloud APIs          â”‚
â”‚  â””â”€â”€ ai.budget[10000]          Token limit per day          â”‚
â”‚                                                              â”‚
â”‚  WORKSPACE DATA SCOPES                                       â”‚
â”‚  â”œâ”€â”€ workspace.read            Read documents/boards        â”‚
â”‚  â”œâ”€â”€ workspace.write           Modify data                  â”‚
â”‚  â””â”€â”€ workspace.none            No access to user data       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Install-Time Permission Dialog

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Install "AI Writing Assistant"?                            â”‚
â”‚                                                              â”‚
â”‚  This plugin requests:                                       â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ Read your documents                                      â”‚
â”‚     To analyze and improve your writing                     â”‚
â”‚                                                              â”‚
â”‚  ğŸŒ Network access to api.grammarly.com                     â”‚
â”‚     To check grammar and spelling                           â”‚
â”‚                                                              â”‚
â”‚  ğŸ¤– Use local AI models                                      â”‚
â”‚     To generate writing suggestions                         â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ This plugin cannot:                                     â”‚
â”‚     â€¢ Access files outside your workspace                   â”‚
â”‚     â€¢ Access other websites                                 â”‚
â”‚     â€¢ Modify system settings                                â”‚
â”‚                                                              â”‚
â”‚         [Cancel]                [Install]                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 12.4 Recommended Security Architecture {#124-recommended-security-architecture}

`[CORE]`

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    DECISION POINT: Security Architecture
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RECOMMENDED: WASM-First with Permission Model

Phase 1 (Internal Plugins):
  â””â”€â”€ Simple process isolation
      â€¢ Run plugins as subprocesses
      â€¢ Limit via OS mechanisms where easy
      â€¢ Internal plugins are trusted (from your team)

Phase 2 (Community Plugins):
  â””â”€â”€ WASM sandbox for all third-party code
      â€¢ Compile plugins to WASM
      â€¢ Expose only necessary APIs
      â€¢ Manifest-declared permissions
      â€¢ User consent dialog at install

Phase 3 (Marketplace):
  â””â”€â”€ Full security pipeline
      â€¢ Automated security scanning
      â€¢ Manual review for sensitive permissions
      â€¢ Code signing
      â€¢ Remote disable capability

DEFAULT STANCE: Deny Everything
  â€¢ No filesystem access by default
  â€¢ No network by default
  â€¢ No AI access by default
  â€¢ Plugin must request; user must grant

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Key Takeaways

- Plugins are a major security risk if not sandboxed
- WASM provides strong, proven isolation
- Implement permission model like browser extensions
- Default deny: plugins only get what they explicitly request and user approves
- Phase in security: start simple, add WASM sandbox for community plugins

---
---

# PART V: OBSERVABILITY & TESTING

---

## 13.0 AI Observability {#13-ai-observability}

**Prerequisites:** Sections 2-5 (LLM Infrastructure)  
**Related to:** Section 14 (Evaluation), Section 15 (Benchmarking)  
**Implements:** Monitoring and debugging AI behavior  
**Read time:** ~20 minutes

**This section covers how to monitor, debug, and understand what your AI systems are doing.**

---

### 13.1 What to Monitor in AI Apps {#131-what-to-monitor-in-ai-apps}

`[CORE]`

#### Why AI Needs Different Observability

**Traditional apps are deterministic; AI apps are probabilistic.** The same input might produce different outputs. This makes debugging harder.

```
Traditional App:
  Input: login("user", "pass")
  Output: Always same result (success or specific error)
  
AI App:
  Input: "Write me a poem about cats"
  Output: Different poem every time
  Problem: How do you know if it's working "correctly"?
```

#### Key Metrics to Track

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI OBSERVABILITY METRICS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PERFORMANCE METRICS                                         â”‚
â”‚  â”œâ”€â”€ Latency (p50, p95, p99)   How long requests take       â”‚
â”‚  â”œâ”€â”€ Tokens per second         Throughput measure           â”‚
â”‚  â”œâ”€â”€ Time to first token       Perceived responsiveness     â”‚
â”‚  â””â”€â”€ Queue depth               Backlog of requests          â”‚
â”‚                                                              â”‚
â”‚  RESOURCE METRICS                                            â”‚
â”‚  â”œâ”€â”€ GPU memory usage          Are we close to OOM?         â”‚
â”‚  â”œâ”€â”€ GPU utilization %         Is GPU being used?           â”‚
â”‚  â”œâ”€â”€ CPU/RAM usage             System health                â”‚
â”‚  â””â”€â”€ Model load/unload events  Memory management working?   â”‚
â”‚                                                              â”‚
â”‚  QUALITY SIGNALS                                             â”‚
â”‚  â”œâ”€â”€ Error rate                Model failures               â”‚
â”‚  â”œâ”€â”€ Retry rate                Had to try again             â”‚
â”‚  â”œâ”€â”€ Fallback rate             Localâ†’cloud switches         â”‚
â”‚  â”œâ”€â”€ User feedback             Thumbs up/down               â”‚
â”‚  â””â”€â”€ Task completion           Did user accomplish goal?    â”‚
â”‚                                                              â”‚
â”‚  COST METRICS (if using cloud APIs)                         â”‚
â”‚  â”œâ”€â”€ Tokens consumed           Input + output               â”‚
â”‚  â”œâ”€â”€ API spend                 Actual money                 â”‚
â”‚  â””â”€â”€ Local vs cloud ratio      How much offloaded?         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 13.2 Tools Comparison {#132-tools-comparison}

`[CORE]`

#### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool        â”‚ Type           â”‚ Local-First? â”‚ Best For      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OTel+Prom   â”‚ General obs.   â”‚ âœ“ Yes        â”‚ Core metrics  â”‚
â”‚ Langfuse    â”‚ LLM-specific   â”‚ Self-hosted  â”‚ Full tracing  â”‚
â”‚ LangSmith   â”‚ LLM-specific   â”‚ Cloud only   â”‚ LangChain     â”‚
â”‚ Helicone    â”‚ LLM proxy      â”‚ Self-hosted  â”‚ Caching       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### OpenTelemetry + Prometheus + Grafana â€” Recommended Core

**What it is:** Industry-standard observability stack.

```
The "boring but reliable" choice:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Your App â”€â”€â–º OpenTelemetry â”€â”€â–º Prometheus â”€â”€â–º Grafana     â”‚
â”‚  (metrics)    (collection)      (storage)     (dashboards) â”‚
â”‚                                                              â”‚
â”‚  Also:                                                       â”‚
â”‚  Your App â”€â”€â–º OTel â”€â”€â–º Jaeger/Tempo â”€â”€â–º Grafana            â”‚
â”‚  (traces)                (storage)      (visualization)     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- â­ Fully localâ€”no data leaves your machine
- â­ Vendor-neutral standard
- â­ Works with any backend (vLLM, TGI expose Prometheus metrics)
- â­ Flexibleâ€”you define what to track

**Cons:**
- âš ï¸ No LLM-specific features out of box
- âš ï¸ Must design your own metrics/spans
- âš ï¸ Setup requires several components

---

#### Langfuse â€” Best LLM-Specific (Self-Hosted)

**What it is:** Open-source LLM observability platform.

```
Langfuse tracks:
  â€¢ Every prompt and response
  â€¢ Token counts and costs
  â€¢ Latency breakdowns
  â€¢ Tool calls within agents
  â€¢ User feedback
```

**Pros:**
- â­ Open-source, self-hostable
- â­ Purpose-built for LLM debugging
- â­ Tracks costs and tokens automatically
- â­ Integrates via OpenTelemetry

**Cons:**
- âš ï¸ Requires running Postgres + Langfuse server
- âš ï¸ Heavier setup than plain OTel

---

### 13.3 Privacy-Sensitive Logging {#133-privacy-sensitive-logging}

`[CORE]`

#### The Problem

**LLM logs contain user prompts, which may contain sensitive information.**

```
Example dangerous log:

{
  "timestamp": "2024-01-15T10:30:00Z",
  "prompt": "Write an email to john.doe@company.com about 
             my salary negotiation. My current salary is 
             $85,000 and I want to ask for $100,000",
  "response": "..."
}

This log contains:
  â€¢ Email address (PII)
  â€¢ Salary information (sensitive)
  â€¢ Professional context (private)
```

#### Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PRIVACY-SAFE LOGGING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. REDACT BEFORE LOGGING                                    â”‚
â”‚     â€¢ Use regex/libraries to detect PII                     â”‚
â”‚     â€¢ Replace: "john.doe@company.com" â†’ "[EMAIL]"           â”‚
â”‚     â€¢ Tools: llm-guard Anonymize scanner                    â”‚
â”‚                                                              â”‚
â”‚  2. LOG METADATA, NOT CONTENT                                â”‚
â”‚     Good: { task: "email_draft", tokens_in: 50, success: T }â”‚
â”‚     Bad:  { prompt: "Write email to john...", ... }         â”‚
â”‚                                                              â”‚
â”‚  3. SAMPLE, DON'T LOG EVERYTHING                             â”‚
â”‚     â€¢ Log 10% of interactions for debugging                 â”‚
â”‚     â€¢ Full logs only with explicit user consent             â”‚
â”‚                                                              â”‚
â”‚  4. SHORT RETENTION                                          â”‚
â”‚     â€¢ Delete detailed logs after 7-30 days                  â”‚
â”‚     â€¢ Keep aggregated metrics longer                        â”‚
â”‚                                                              â”‚
â”‚  5. LOCAL ONLY                                               â”‚
â”‚     â€¢ Never send raw prompts to cloud services              â”‚
â”‚     â€¢ If cloud needed, anonymize first                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Safe Logging Schema

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "task_type": "email_draft",
  "agent": "writing_assistant",
  "model": "mistral-7b",
  "tokens_in": 50,
  "tokens_out": 120,
  "latency_ms": 850,
  "success": true,
  "error": null,
  "pii_detected": false,
  "user_feedback": null
}
```

Note: No actual prompt or response content logged.

---

### 13.4 Metrics & Dashboards {#134-metrics-and-dashboards}

`[CORE]`

#### Essential Dashboard Panels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRAFANA DASHBOARD                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ROW 1: HEALTH AT A GLANCE                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Requests/minâ”‚ â”‚ Error Rate  â”‚ â”‚ p95 Latency â”‚            â”‚
â”‚  â”‚    42       â”‚ â”‚   0.5%      â”‚ â”‚   850ms     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  ROW 2: LATENCY OVER TIME                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  â”€â”€â”€â”€p50   â”€â”€â”€â”€p95   â”€â”€â”€â”€p99                 â”‚           â”‚
â”‚  â”‚     â•­â”€â”€â”€â”€â”€â”€â•®      â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                â”‚           â”‚
â”‚  â”‚  â”€â”€â”€â•¯      â•°â”€â”€â”€â”€â”€â”€â•¯         â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â”‚  ROW 3: RESOURCES                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ GPU Memory           â”‚ â”‚ GPU Utilization      â”‚          â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 75%    â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 67%     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚  ROW 4: BY MODEL                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Model      â”‚ Requests â”‚ Avg Latency â”‚ Errors â”‚           â”‚
â”‚  â”‚ mistral-7b â”‚ 1,234    â”‚ 340ms       â”‚ 0.2%   â”‚           â”‚
â”‚  â”‚ codellama  â”‚ 567      â”‚ 520ms       â”‚ 0.8%   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Instrumentation Example

```python
from opentelemetry import trace, metrics

tracer = trace.get_tracer(__name__)
meter = metrics.get_meter(__name__)

# Define metrics
request_counter = meter.create_counter(
    "llm_requests_total",
    description="Total LLM requests"
)
latency_histogram = meter.create_histogram(
    "llm_latency_seconds",
    description="LLM request latency"
)

# Instrument a function
async def call_llm(prompt, model):
    with tracer.start_as_current_span("llm_call") as span:
        span.set_attribute("model", model)
        
        start = time.time()
        try:
            response = await model.generate(prompt)
            
            request_counter.add(1, {"model": model, "status": "success"})
            latency_histogram.record(time.time() - start, {"model": model})
            
            return response
        except Exception as e:
            request_counter.add(1, {"model": model, "status": "error"})
            span.record_exception(e)
            raise
```

---

## 14.0 Evaluation & Quality {#14-evaluation-and-quality}

**Prerequisites:** Section 13  
**Related to:** Section 15 (Benchmarking)  
**Implements:** Quality assurance for AI outputs  
**Read time:** ~15 minutes

**This section covers how to test and evaluate LLM output quality.**

---

### 14.1 Testing LLM Outputs {#141-testing-llm-outputs}

`[CORE]`

#### The Challenge

**LLM outputs are non-deterministic.** Traditional unit tests expect exact outputs:

```python
# Traditional test (deterministic)
def test_add():
    assert add(2, 3) == 5  # Always passes or fails consistently

# LLM test (non-deterministic)
def test_poem():
    poem = llm("Write a haiku about code")
    assert poem == "???"  # What do we check?
```

#### Testing Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM TESTING STRATEGIES                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. GOLDEN TEST SUITES                                       â”‚
â”‚     â€¢ Define representative test prompts                    â”‚
â”‚     â€¢ For deterministic tasks: check exact output           â”‚
â”‚     â€¢ For generative tasks: check key properties            â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Prompt: "What is 2+2?"                                   â”‚
â”‚    Assert: "4" in response.lower()                          â”‚
â”‚                                                              â”‚
â”‚  2. PROPERTY-BASED TESTS                                     â”‚
â”‚     â€¢ Check structural properties, not exact content        â”‚
â”‚     â€¢ Response length in expected range                     â”‚
â”‚     â€¢ Contains required keywords                            â”‚
â”‚     â€¢ Valid JSON/format                                     â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Prompt: "Write JSON with name and age"                   â”‚
â”‚    Assert: valid JSON, has "name" key, has "age" key        â”‚
â”‚                                                              â”‚
â”‚  3. LLM-AS-JUDGE                                             â”‚
â”‚     â€¢ Use another LLM to evaluate output quality            â”‚
â”‚     â€¢ Rate on criteria: correctness, coherence, helpfulness â”‚
â”‚     â€¢ Scalable but adds latency/cost                        â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Ask GPT-4: "Rate this response 1-5 for helpfulness: ..." â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Golden Test Example

```python
# tests/test_llm_golden.py

GOLDEN_TESTS = [
    {
        "name": "math_simple",
        "prompt": "What is 15 + 27?",
        "expected_contains": ["42"],
    },
    {
        "name": "code_function",
        "prompt": "Write a Python function that adds two numbers",
        "expected_contains": ["def ", "return"],
    },
    {
        "name": "json_extraction",
        "prompt": "Extract the name and date from: 'Meeting with Alice on Jan 5th'",
        "validate": lambda r: "alice" in r.lower() and "jan" in r.lower(),
    },
]

def test_golden_suite():
    for test in GOLDEN_TESTS:
        response = call_llm(test["prompt"])
        
        if "expected_contains" in test:
            for expected in test["expected_contains"]:
                assert expected in response, f"Failed {test['name']}"
        
        if "validate" in test:
            assert test["validate"](response), f"Failed {test['name']}"
```

---

### 14.2 Multi-Agent Tracing {#142-multi-agent-tracing}

#### The Complexity

**Multi-agent systems have many components talking to each other.** Debugging requires seeing the full flow.

```
User Request: "Summarize this document and create action items"

Agent Flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Orchestratorâ”‚â”€â”€â–º "This needs summarization + extraction"
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Summaryâ”‚ â”‚Extractorâ”‚
â”‚ Agent â”‚ â”‚  Agent  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Mistralâ”‚ â”‚CodeLlamaâ”‚
â”‚  LLM  â”‚ â”‚   LLM   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Combine â”‚
    â”‚ Results â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Tracing with OpenTelemetry

```python
# Each agent action becomes a span
with tracer.start_as_current_span("user_request") as root:
    root.set_attribute("request_type", "summarize_and_extract")
    
    with tracer.start_as_current_span("orchestrator_decision") as span:
        span.set_attribute("decision", "parallel_agents")
    
    # These run in parallel but are child spans
    with tracer.start_as_current_span("summary_agent") as span:
        with tracer.start_as_current_span("llm_call_mistral") as llm:
            summary = await call_mistral(document)
            
    with tracer.start_as_current_span("extractor_agent") as span:
        with tracer.start_as_current_span("llm_call_codellama") as llm:
            actions = await call_codellama(document)
    
    with tracer.start_as_current_span("combine_results") as span:
        result = combine(summary, actions)
```

#### Trace Visualization

```
In Jaeger/Tempo, you'd see:

user_request                     [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•] 2.5s
  â””â”€ orchestrator_decision       [â•â•]                               0.1s
  â””â”€ summary_agent               [â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                   1.2s
       â””â”€ llm_call_mistral       [â•â•â•â•â•â•â•â•â•â•â•â•]                     1.0s
  â””â”€ extractor_agent             [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                 1.5s
       â””â”€ llm_call_codellama     [â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                   1.3s
  â””â”€ combine_results             [â•â•]                               0.1s
```

---

## 15.0 Benchmark Harness {#15-benchmark-harness}

**Prerequisites:** Sections 13, 14  
**Related to:** Performance optimization  
**Implements:** Systematic performance testing  
**Read time:** ~15 minutes

**This section describes a benchmark system for measuring and comparing model/runtime performance.**

---

### 15.1 Benchmark Architecture {#151-benchmark-architecture}

`[CORE]`

#### Why Build a Benchmark Harness?

**Reproducible performance testing** lets you:
- Compare runtimes (Ollama vs vLLM)
- Compare models (Mistral-7B vs Llama2-7B)
- Measure impact of configuration changes
- Track performance over time

#### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BENCHMARK HARNESS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  CONFIG FILES (YAML)                                         â”‚
â”‚  â”œâ”€â”€ models.yml      Model endpoints and settings           â”‚
â”‚  â”œâ”€â”€ scenarios.yml   Test scenarios to run                  â”‚
â”‚  â””â”€â”€ prompts.yml     Standard prompts for testing           â”‚
â”‚                                                              â”‚
â”‚  ADAPTERS                                                    â”‚
â”‚  â”œâ”€â”€ OllamaAdapter   Talks to Ollama                        â”‚
â”‚  â”œâ”€â”€ VLLMAdapter     Talks to vLLM                          â”‚
â”‚  â”œâ”€â”€ TGIAdapter      Talks to TGI                           â”‚
â”‚  â””â”€â”€ ImageAdapter    Talks to ComfyUI                       â”‚
â”‚                                                              â”‚
â”‚  RUNNERS                                                     â”‚
â”‚  â”œâ”€â”€ SingleLLMRunner      One model, one prompt             â”‚
â”‚  â”œâ”€â”€ ConcurrentRunner     Multiple parallel requests        â”‚
â”‚  â””â”€â”€ MixedWorkloadRunner  LLM + Image together              â”‚
â”‚                                                              â”‚
â”‚  OUTPUT                                                      â”‚
â”‚  â”œâ”€â”€ results.jsonl   Raw timing data                        â”‚
â”‚  â””â”€â”€ report.md       Summary statistics                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 15.2 Scenarios & Adapters {#152-scenarios-and-adapters}

#### Example Configuration

```yaml
# models.yml
models:
  - id: mistral-7b-ollama
    type: ollama
    endpoint: http://localhost:11434
    model_name: mistral
    
  - id: mistral-7b-vllm
    type: vllm
    endpoint: http://localhost:8000
    model_name: mistralai/Mistral-7B-v0.1

# scenarios.yml
scenarios:
  - id: single_chat
    type: single_llm
    models: [mistral-7b-ollama, mistral-7b-vllm]
    prompts: [short_qa, medium_qa, long_generation]
    iterations: 10
    
  - id: concurrent_load
    type: load_sweep
    models: [mistral-7b-vllm]
    prompts: [medium_qa]
    concurrency_levels: [1, 2, 4, 8, 16]
    iterations: 5

# prompts.yml
prompts:
  - id: short_qa
    text: "What is the capital of France?"
    max_tokens: 50
    
  - id: medium_qa
    text: "Explain how photosynthesis works in 3 paragraphs."
    max_tokens: 300
```

#### Adapter Interface

```python
# adapters.py
class LLMAdapter:
    """Base class for model adapters"""
    
    async def generate(self, prompt: str, params: dict) -> Result:
        raise NotImplementedError

class OllamaAdapter(LLMAdapter):
    async def generate(self, prompt: str, params: dict) -> Result:
        start = time.time()
        response = await httpx.post(
            f"{self.endpoint}/api/generate",
            json={"model": self.model, "prompt": prompt, **params}
        )
        elapsed = time.time() - start
        
        data = response.json()
        return Result(
            text=data["response"],
            tokens_in=data["prompt_eval_count"],
            tokens_out=data["eval_count"],
            latency=elapsed
        )
```

---

### 15.3 Reporting & Analysis {#153-reporting-and-analysis}

#### Output Format

```
# Benchmark Report - 2024-01-15

## Summary

| Scenario       | Model             | Avg Latency | p50    | p95    | Tokens/sec |
|----------------|-------------------|-------------|--------|--------|------------|
| single_chat    | mistral-7b-ollama | 340ms       | 320ms  | 450ms  | 88         |
| single_chat    | mistral-7b-vllm   | 310ms       | 300ms  | 420ms  | 97         |
| concurrent_8   | mistral-7b-vllm   | 180ms       | 170ms  | 250ms  | 620        |

## Findings

- vLLM is ~10% faster for single requests
- vLLM scales much better under load (620 vs ~100 tokens/sec at 8 concurrent)
- Ollama shows consistent latency regardless of load (no batching)

## Recommendations

- Use Ollama for development/single-user
- Use vLLM for production/multi-user scenarios
```

---
---

# PART VI: IMPLEMENTATION

---

## 16.0 Technology Stack Summary {#16-technology-stack-summary}

**Read time:** ~5 minutes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPLETE TECHNOLOGY STACK                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DESKTOP FRAMEWORK                                           â”‚
â”‚  â”œâ”€â”€ Primary: Tauri (Rust) + React/Vue                      â”‚
â”‚  â””â”€â”€ Alternative: Electron (if more JS ecosystem needed)    â”‚
â”‚                                                              â”‚
â”‚  LLM INFRASTRUCTURE                                          â”‚
â”‚  â”œâ”€â”€ Runtime: Ollama (dev) + vLLM (production)              â”‚
â”‚  â”œâ”€â”€ Models: Mistral-7B, CodeLlama-7B, Llama2-13B           â”‚
â”‚  â””â”€â”€ Images: ComfyUI + SDXL                                 â”‚
â”‚                                                              â”‚
â”‚  DATA LAYER                                                  â”‚
â”‚  â”œâ”€â”€ CRDT: Yjs (or Loro for Rust)                           â”‚
â”‚  â”œâ”€â”€ Database: SQLite                                        â”‚
â”‚  â””â”€â”€ Sync: Yjs WebSocket provider (later)                   â”‚
â”‚                                                              â”‚
â”‚  PLUGIN SYSTEM                                               â”‚
â”‚  â”œâ”€â”€ Sandbox: WASM (Wasmtime)                               â”‚
â”‚  â”œâ”€â”€ Language: AssemblyScript/Rust â†’ WASM                   â”‚
â”‚  â””â”€â”€ Permissions: Manifest-based capability model           â”‚
â”‚                                                              â”‚
â”‚  OBSERVABILITY                                               â”‚
â”‚  â”œâ”€â”€ Telemetry: OpenTelemetry                               â”‚
â”‚  â”œâ”€â”€ Metrics: Prometheus                                    â”‚
â”‚  â”œâ”€â”€ Visualization: Grafana                                 â”‚
â”‚  â””â”€â”€ Traces: Jaeger or Grafana Tempo                        â”‚
â”‚                                                              â”‚
â”‚  LANGUAGES                                                   â”‚
â”‚  â”œâ”€â”€ Backend: Python (orchestrator) + Rust (Tauri)          â”‚
â”‚  â”œâ”€â”€ Frontend: TypeScript + React/Vue                       â”‚
â”‚  â””â”€â”€ Plugins: AssemblyScript â†’ WASM                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 17.0 Implementation Roadmap {#17-implementation-roadmap}

`[CORE]`

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    IMPLEMENTATION PHASES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 0: Foundation (Weeks 1-4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Set up Tauri project with basic UI shell
  âœ“ Install Ollama, download Mistral-7B
  âœ“ Basic chat interface calling local model
  âœ“ SQLite setup for storing chat history
  
  Deliverable: "Hello world" AI chat app

PHASE 1: Core Editor (Weeks 5-8)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Integrate Yjs for collaborative editing
  âœ“ Build rich text editor (TipTap + Yjs)
  âœ“ Document storage in SQLite
  âœ“ Basic AI commands in editor (summarize, rewrite)
  
  Deliverable: Local-first document editor with AI

PHASE 2: Multi-Model (Weeks 9-12)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Add CodeLlama for code tasks
  âœ“ Build model routing logic in orchestrator
  âœ“ GPU memory management (load/unload)
  âœ“ Basic observability (Prometheus + Grafana)
  
  Deliverable: Specialized AI for different tasks

PHASE 3: Plugin System MVP (Weeks 13-16)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Design plugin manifest format
  âœ“ Build simple subprocess sandbox
  âœ“ Create plugin API (register commands, access docs)
  âœ“ 1-2 sample plugins (internal)
  
  Deliverable: Working internal plugin system

PHASE 4: Polish & Security (Weeks 17-20)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ WASM sandbox for third-party plugins
  âœ“ Permission model and install dialogs
  âœ“ Sync between devices (file-based or server)
  âœ“ Performance optimization
  
  Deliverable: Beta-ready application

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 18.0 Gap Analysis & Open Questions {#18-gap-analysis}

`[CORE]`

### What the Research DOESN'T Cover

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RESEARCH GAPS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  USER INTERFACE                                              â”‚
â”‚  â€¢ No detailed UI/UX designs                                â”‚
â”‚  â€¢ No accessibility considerations                          â”‚
â”‚  â€¢ No mobile/responsive strategy                            â”‚
â”‚  Action: Need separate UI design research                   â”‚
â”‚                                                              â”‚
â”‚  AUTHENTICATION & MULTI-USER                                 â”‚
â”‚  â€¢ No user account system design                            â”‚
â”‚  â€¢ No team/sharing model                                    â”‚
â”‚  â€¢ No encryption for sensitive data                         â”‚
â”‚  Action: Research if/when adding cloud sync                 â”‚
â”‚                                                              â”‚
â”‚  BUSINESS MODEL                                              â”‚
â”‚  â€¢ No pricing strategy                                      â”‚
â”‚  â€¢ No marketplace economics for plugins                     â”‚
â”‚  Action: Business planning separate from technical          â”‚
â”‚                                                              â”‚
â”‚  SPECIFIC MODEL FINE-TUNING                                  â”‚
â”‚  â€¢ Research covers pre-trained models only                  â”‚
â”‚  â€¢ No guidance on fine-tuning for specific use cases        â”‚
â”‚  Action: May need if default models insufficient            â”‚
â”‚                                                              â”‚
â”‚  WINDOWS-SPECIFIC ISSUES                                     â”‚
â”‚  â€¢ Limited coverage of Windows sandboxing options           â”‚
â”‚  â€¢ No Windows installer/distribution guidance               â”‚
â”‚  Action: Platform-specific research needed                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Open Technical Questions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPEN QUESTIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Tauri vs Electron final decision?                       â”‚
â”‚     â€¢ Tauri: Smaller, faster, Rust backend                  â”‚
â”‚     â€¢ Electron: More mature, larger ecosystem               â”‚
â”‚     â†’ Recommendation: Start with Tauri, reconsider if       â”‚
â”‚       ecosystem limitations become blocking                 â”‚
â”‚                                                              â”‚
â”‚  2. How to handle very long documents?                      â”‚
â”‚     â€¢ Context windows are limited (4K-8K tokens)            â”‚
â”‚     â€¢ Options: Chunking, summarization, RAG                 â”‚
â”‚     â†’ Need: RAG (Retrieval Augmented Generation) research   â”‚
â”‚                                                              â”‚
â”‚  3. Offline-first sync strategy?                            â”‚
â”‚     â€¢ File sync (OneDrive/Dropbox) simple but limited       â”‚
â”‚     â€¢ Custom sync server more powerful but complex          â”‚
â”‚     â†’ Recommendation: Start with file sync, add server      â”‚
â”‚       when multi-user collaboration is priority             â”‚
â”‚                                                              â”‚
â”‚  4. Plugin language choice?                                 â”‚
â”‚     â€¢ WASM requires compilation (barrier to entry)          â”‚
â”‚     â€¢ JavaScript simpler but harder to sandbox              â”‚
â”‚     â†’ Recommendation: Support bothâ€”sandboxed JS for         â”‚
â”‚       simple plugins, WASM for advanced/untrusted           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
---

# END MATTER

---

## Consolidated Glossary {#consolidated-glossary}

| Term | Definition |
|------|------------|
| **API** | Application Programming Interfaceâ€”a defined way for programs to communicate with each other |
| **Automerge** | A CRDT library that stores full history, good for version tracking but higher memory usage |
| **Batching** | Processing multiple requests together for efficiency |
| **Context Window** | How many tokens an LLM can "see" at onceâ€”its working memory |
| **CRDT** | Conflict-free Replicated Data Typeâ€”data structures that can automatically merge without conflicts |
| **CUDA** | NVIDIA's technology for running computations on GPUs |
| **Electron** | A framework for building desktop apps with web technologies (HTML/CSS/JS) |
| **GGUF** | A file format for quantized LLM models, used by llama.cpp and Ollama |
| **GPU** | Graphics Processing Unitâ€”hardware that runs AI models very fast |
| **Inference** | Using a trained AI model to generate outputs (vs training which creates the model) |
| **KV Cache** | Key-Value cacheâ€”memory used to store conversation context during inference |
| **Langfuse** | Open-source LLM observability platform |
| **LLM** | Large Language Modelâ€”AI that generates text by predicting the next word |
| **Local-First** | Software design where data lives primarily on user's device, not in the cloud |
| **Loro** | A new CRDT library with full history and movable trees, written in Rust |
| **Manifest** | A configuration file declaring a plugin's metadata, permissions, and capabilities |
| **Ollama** | A user-friendly tool for running LLMs locally |
| **OpenTelemetry (OTel)** | Industry standard for collecting metrics, traces, and logs |
| **Parameters** | The "knowledge" of an AI model, stored as numbers |
| **Prometheus** | Time-series database commonly used for metrics |
| **Quantization** | Compressing a model to use less memory by reducing number precision |
| **Q4/Q5/Q8** | Quantization levelsâ€”lower numbers mean smaller size but slightly lower quality |
| **Runtime** | Software that loads and executes AI models |
| **Sandbox** | An isolated environment where untrusted code can run without affecting the rest of the system |
| **SDXL** | Stable Diffusion XLâ€”the latest version of the Stable Diffusion image generation model |
| **SQLite** | A lightweight embedded database stored as a single file |
| **Streaming** | Sending response tokens one at a time as they're generated |
| **Tauri** | A framework for building desktop apps with web frontend and Rust backend |
| **TGI** | Text Generation Inferenceâ€”HuggingFace's production LLM server |
| **Token** | A chunk of text (roughly Â¾ of a word) that LLMs process |
| **vLLM** | A high-performance LLM inference engine optimized for throughput |
| **VRAM** | Video RAMâ€”memory on the GPU where models must be loaded to run fast |
| **WASM** | WebAssemblyâ€”a binary format that runs in a secure sandbox |
| **Yjs** | A popular CRDT library for JavaScript, known for performance and editor integrations |

---

## Sources Referenced {#sources-referenced}

This document synthesizes research from the following source documents:

1. **LLM Inference Runtimes** (8 pages) â€” Runtime comparison, model candidates, image generation, scheduling patterns

2. **Inference Runtimes** (7 pages) â€” Runtime comparison, model selection by role, GPU bottlenecks, recommendations

3. **Benchmark Harness Design** (5 pages) â€” Modular Python benchmark architecture, adapters, scenarios, reporting

4. **Local-First Data and Sync Architecture** (9 pages) â€” CRDT libraries, database patterns, sync topologies, conflict resolution UX

5. **Extension Platforms: Architectural Overview** (10 pages) â€” Plugin system analysis (VS Code, Obsidian, Figma, browsers), proposed architecture

6. **Local-First Multi-Model LLM Hosting** (8 pages) â€” Runtime survey, integration strategies, memory management, recommendations

7. **Sandboxing Options for Untrusted Code** (12 pages) â€” WASM, Pyodide, OS sandboxing, permission models, security architecture

8. **AI Observability and Evaluation** (10 pages) â€” Logging, metrics, privacy, evaluation methods, multi-agent tracing, phased rollout

---

## Document Navigation Tips

**For Claude/LLM Context:**
When referencing this document in future conversations, you can use section anchors:
- "See #3-llm-inference-runtimes for runtime details"
- "Reference #12-sandboxing-and-security for plugin security"

**For Quick Decisions:**
Search for "DECISION POINT" to find all major technical choices with recommendations.

**For Implementation:**
Search for "âœ“ Action Items" or follow the roadmap in Section 17.

---



---

# PART VII: CONSOLIDATED ARCHITECTURE & ROADMAP

---

# 19. Executive Summary {#19-executive-summary}

**Prerequisites:** None - start here  
**Related to:** All sections  
**Implements:** Project overview and orientation  
**Read time:** ~5 minutes

**This section provides a bird's-eye view of Project Handshake: what it is, why it matters, and the key decisions that have been made based on research.**

---

### TL;DR Box

> **Project Handshake** is a desktop application combining:
> - **Notion-like** document editing with databases
> - **Milanote-like** visual canvas/moodboards  
> - **Excel-like** spreadsheets with formulas
> - **Local AI models** for writing, coding, and image generation
> 
> **Tech Stack Decision:** Tauri + React + TypeScript (frontend) + Python (AI backend)
> 
> **Key Insight:** Run AI models locally for privacy, speed, and cost savingsâ€”with cloud fallback when needed.

---

### What We're Building

**Project Handshake is a "local AI cloud" on your desktop.** Instead of sending your documents, ideas, and data to cloud services like Notion or Google Docs, everything stays on your computer. AI assistants run locally too, meaning your sensitive information never leaves your machine.

The application combines three types of tools that creative professionals typically use separately:

| Tool Type | Inspiration | Use Case |
|-----------|-------------|----------|
| **Rich Documents** | Notion | Writing, planning, structured databases |
| **Visual Canvas** | Milanote | Mood boards, brainstorming, spatial organization |
| **Spreadsheets** | Excel | Data manipulation, calculations, analysis |

**What makes this different:** Local AI models collaborate to help you. One AI might plan your project, another writes the code, and a third generates imagesâ€”all coordinated automatically.

---

### Key Architecture Decisions (From Research)

Based on extensive research across multiple documents, the following decisions have been validated:

| Decision | Choice | Why |
|----------|--------|-----|
| Desktop Shell | **Tauri** (not Electron) | 90% less memory usage; critical when running AI models |
| Frontend | **React + TypeScript** | Rich ecosystem, same code works in both shells |
| Backend | **Python** | Best AI/ML library support, orchestration frameworks |
| AI Orchestration | **AutoGen or LangGraph** | Mature multi-agent coordination |
| Data Sync | **CRDTs (Yjs)** | Offline-first, conflict-free collaboration |
| Storage | **File-tree based** | Human-readable, portable, git-friendly |

---

### Why Local-First Matters

ğŸ“Œ **Key Point:** The entire architecture is designed around "local-first" principles:

1. **Privacy:** Your documents and AI conversations never leave your computer
2. **Speed:** No network latency for AI responses
3. **Cost:** After initial model download, AI usage is essentially free
4. **Reliability:** Works without internet, on airplanes, in poor connectivity
5. **Control:** You own your data in standard file formats

---

### Hardware Context

The target hardware for development and initial deployment:

| Component | Specification | Why It Matters |
|-----------|--------------|----------------|
| CPU | Ryzen 9 5950X (16 cores) | Handles multiple processes, CPU inference fallback |
| RAM | 128 GB | Multiple AI models can stay loaded in memory |
| GPU | RTX 3090 (24GB VRAM) | Runs large AI models, image generation |
| Storage | NVMe SSD | Fast model loading, responsive file operations |

âš ï¸ **Warning:** This hardware is above average. The app design must handle graceful degradation for users with less powerful systems, including cloud fallback options.

---

# 20. Foundation Concepts {#20-foundation-concepts}

Before diving into specific technical decisions, let's establish foundational understanding of the core concepts that appear throughout this document.

---

## 20.1 What is a Desktop Application Shell? {#201-what-is-a-desktop-application-shell}

**Prerequisites:** None - foundational  
**Related to:** Section 3.1 (Tauri vs Electron)  
**Implements:** Understanding architecture choices  
**Read time:** ~4 minutes

**A "shell" is the container that turns web code into a desktop application. It's the bridge between your web-based user interface and the operating system.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Desktop Shell** | A program that wraps website-style code so it runs as a regular desktop app (with window controls, file access, etc.) | We need to choose between Tauri and Electron as our shell |
| **Electron** | The most popular shell; used by VS Code, Slack, Discord. Bundles a complete Chrome browser inside your app | Higher memory usage but battle-tested and familiar |
| **Tauri** | A newer, lighter shell using Rust. Uses the operating system's built-in browser instead of bundling one | Much lower memory usageâ€”critical when AI models need that RAM |
| **WebView** | A "browser window without the browser"â€”just the part that displays web pages | Tauri uses the system's webview; Electron bundles its own |
| **IPC (Inter-Process Communication)** | How different parts of a program talk to each other | How the UI will communicate with the Python AI backend |

---

### The Mental Model

Think of building a desktop app like building a food truck:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DESKTOP SHELL                   â”‚
â”‚         (The food truck itself)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           YOUR WEB APP               â”‚    â”‚
â”‚  â”‚      (The kitchen equipment)         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚    React + TypeScript       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   (The menu & recipes)      â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â”‚                         â”‚
â”‚                    â–¼                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚     Operating System        â”‚          â”‚
â”‚    â”‚   (Where the truck parks)   â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Electron** = A food truck that brings its own generator, water supply, and waste systemâ€”self-contained but heavy.

**Tauri** = A food truck that plugs into the venue's electricity and plumbingâ€”lighter but depends on what's available.

---

### Why This Matters for Handshake

â•â•â• CORE CONCEPT â•â•â•

> Every megabyte of RAM the shell uses is a megabyte NOT available for AI models.
> 
> - Electron idle: ~150-300 MB RAM
> - Tauri idle: ~10-50 MB RAM
> 
> That 200+ MB difference could mean running a larger AI model or faster response times.

---

### Key Takeaways

- âœ“ A desktop shell turns web code into a native application
- âœ“ Electron is mature but memory-hungry; Tauri is lean but newer
- âœ“ For AI-heavy apps, memory efficiency becomes critical
- âœ“ Both shells run the same React/TypeScript frontend code

**See Also:** [Section 3.1 - Tauri vs Electron Decision](#211-desktop-shell-tauri-vs-electron)

---

## 20.2 Understanding Local-First Software {#202-understanding-local-first-software}

**Prerequisites:** None - foundational  
**Related to:** Section 7 (Collaboration and Sync)  
**Implements:** Core design philosophy  
**Read time:** ~5 minutes

**"Local-first" means your data lives on YOUR computer first, and optionally syncs to the cloudâ€”the opposite of how most modern apps work.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Local-First** | Data stored on your device as the "source of truth," not on company servers | Core philosophyâ€”you own your data |
| **Cloud-First** | Data lives on servers; your device just displays it (Google Docs, Notion) | What we're avoiding |
| **Offline-First** | App works without internet; syncs when connection returns | Handshake must work on airplanes |
| **Sync** | Keeping multiple copies of data up-to-date with each other | Needed for multi-device and collaboration |
| **Conflict Resolution** | Deciding what happens when two people edit the same thing | CRDTs handle this automatically |

---

### The Spectrum of Data Ownership

```
CLOUD-FIRST                                    LOCAL-FIRST
     â”‚                                              â”‚
     â–¼                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google  â”‚    â”‚ Notion  â”‚    â”‚Obsidian â”‚    â”‚Handshakeâ”‚
â”‚  Docs   â”‚    â”‚         â”‚    â”‚         â”‚    â”‚(our app)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚              â”‚
  Server       Server+Cache    Files+Sync    Files+AI+
  Required      Preferred       Local         Optional
                               Primary         Cloud
```

---

### Why Local-First for an AI Productivity App?

â•â•â• CORE CONCEPT â•â•â•

> **Privacy + Performance + Cost + Control**
> 
> 1. **Privacy:** AI sees your documents. Do you want that on someone else's servers?
> 2. **Performance:** No network round-trip for AI responses
> 3. **Cost:** Cloud AI APIs charge per request; local models are "free" after download
> 4. **Control:** Export everything to standard formats anytime

---

### Real-World Analogy

**Cloud-First (like Notion):**
- Your files are stored in a bank vault
- You need to visit the bank (internet) to see them
- The bank could close, change terms, or read your files
- Very secure from local theft, but you depend on the bank

**Local-First (like Handshake):**
- Your files are in a safe in your home
- You can access them anytime, even with no internet
- You can make copies anywhere you want
- You're responsible for backups

---

### The Challenge: Collaboration

The main trade-off: **If data is on your computer, how do multiple people edit together?**

Solution: **CRDTs** (Conflict-free Replicated Data Types)â€”special data structures that can merge edits from multiple sources without conflicts.

ğŸ’¡ **Tip:** Think of CRDTs like a Google Doc that works offline. Everyone types on their own copy, and when they reconnect, the document intelligently merges all changes.

**See Also:** [Section 7.1 - Understanding CRDTs](#251-understanding-crdts)

---

### Key Takeaways

- âœ“ Local-first = your data lives on your device primarily
- âœ“ Critical for privacy when AI models access your documents
- âœ“ Enables offline work and eliminates API costs
- âœ“ CRDTs enable collaboration without central servers
- âœ“ You can still sync to cloudâ€”it's just optional

---

## 20.3 What are AI Models and How Do They Run Locally? {#203-what-are-ai-models-and-how-do-they-run-locally}

**Prerequisites:** None - foundational  
**Related to:** Section 5 (AI Model Strategy)  
**Implements:** Understanding AI integration approach  
**Read time:** ~6 minutes

**An AI model is a very large mathematical formula that takes in text (or images) and produces intelligent-seeming responses. "Running locally" means this formula executes on YOUR computer, not a company's servers.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **LLM (Large Language Model)** | An AI trained on massive text to understand and generate language. ChatGPT is an LLM. | The "brain" that will write, summarize, and reason |
| **Parameters** | The "knobs" inside the AI model. More parameters = smarter but heavier. "7B" = 7 billion parameters | Determines which models fit on your hardware |
| **VRAM** | Video RAMâ€”memory on your graphics card | Where AI models live during use; RTX 3090 has 24GB |
| **Inference** | The AI actually doing its job (generating a response) | What happens when you ask the AI something |
| **Quantization** | Shrinking a model to fit in less memory (with some quality loss) | How we fit big models on consumer hardware |
| **GGUF** | A file format for quantized models | The format we'll download models in |

---

### How Big Are These Models?

```
Model Size vs. Quality vs. Hardware Requirements

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 70B (GPT-4 class)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ 140GB+ â”‚
â”‚  - Smartest, needs multiple GPUs or cloud    â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 34B (Very Good)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚ ~70GB  â”‚
â”‚  - Excellent quality, pushes 3090 limits     â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 13B (Good)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 â”‚ ~26GB  â”‚
â”‚  - Great balance, fits 3090 with room       â”‚ â† Sweetâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7B (Decent)        â–ˆâ–ˆâ–ˆâ–ˆ                     â”‚ ~14GB  â”‚
â”‚  - Fast, leaves room for other models        â”‚  Spot  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3B (Basic)         â–ˆâ–ˆ                       â”‚ ~6GB   â”‚
â”‚  - Quick tasks, limited capability           â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### The Model Zoo

Handshake needs DIFFERENT models for DIFFERENT tasks:

| Task | Model Type | Example | Size |
|------|-----------|---------|------|
| Writing & Reasoning | General LLM | Llama 3, Mistral | 7-13B |
| Code Generation | Code-specialized | Code Llama, StarCoder | 7-15B |
| Image Generation | Diffusion Model | SDXL | ~3B |
| Task Planning | Reasoning LLM | GPT-OSS-20B | 20B |

â•â•â• CORE CONCEPT â•â•â•

> **You won't run all models simultaneously.** The orchestrator loads/unloads models based on what's needed. The 3090 has 24GB; a 13B model uses ~14GB quantized, leaving 10GB for SDXL image generation.

---

### Local vs. Cloud AI

```
                    LOCAL                     CLOUD (API)
                      â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ âœ“ Private - data stays home   â”‚  â”‚ âœ— Data sent to  â”‚
    â”‚ âœ“ Free after download         â”‚  â”‚   company       â”‚
    â”‚ âœ“ Works offline               â”‚  â”‚ âœ— Per-request   â”‚
    â”‚ âœ— Limited by your hardware    â”‚  â”‚   cost          â”‚
    â”‚ âœ— Slower than cloud GPUs      â”‚  â”‚ âœ— Needs internetâ”‚
    â”‚                               â”‚  â”‚ âœ“ Latest models â”‚
    â”‚ GOOD FOR: Frequent,           â”‚  â”‚ âœ“ Most powerful â”‚
    â”‚ routine tasks                 â”‚  â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ GOOD FOR: Hard  â”‚
                                       â”‚ tasks, fallback â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### How It Actually Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR COMPUTER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PYTHON BACKEND                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚         MODEL RUNTIME                    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  (vLLM, Ollama, or llama.cpp)           â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                                          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”‚Llama 3â”‚  â”‚ Code  â”‚  â”‚ SDXL  â”‚       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â”‚ 13B   â”‚  â”‚ Llama â”‚  â”‚       â”‚       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚        â”‚          â”‚          â”‚          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                   â”‚                     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚           GPU (RTX 3090)                â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                     â”‚                           â”‚    â”‚
â”‚  â”‚           Orchestrator (AutoGen/LangGraph)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚  HTTP/WebSocket   â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                        â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              TAURI SHELL + REACT UI              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ AI models are mathematical formulas with billions of "knobs"
- âœ“ Larger models = smarter but need more GPU memory (VRAM)
- âœ“ The RTX 3090's 24GB can run 7-13B models comfortably
- âœ“ Quantization shrinks models to fit, with some quality loss
- âœ“ Different tasks need different specialized models
- âœ“ Models swap in/out of GPU memory as needed

**See Also:** [Section 5 - AI Model Strategy](#23-ai-model-strategy)

---

## 20.4 Multi-Model Orchestration Explained {#204-multi-model-orchestration-explained}

**Prerequisites:** Section 2.3 (AI Models)  
**Related to:** Section 6 (Multi-Agent Orchestration)  
**Implements:** Core AI collaboration approach  
**Read time:** ~5 minutes

**"Orchestration" means coordinating multiple AI models to work together on complex tasksâ€”like a conductor directing an orchestra where each instrument (model) plays its part.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Agent** | An AI model with a specific job and the ability to take actions | Our AI assistants for different tasks |
| **Multi-Agent System** | Multiple AI "agents" working together | How we'll coordinate writing, coding, and image AI |
| **Orchestrator** | The "boss" code that decides which agent handles what | The Python backend that manages everything |
| **Task Routing** | Sending a request to the right AI model | "Summarize this" â†’ text model; "Create diagram" â†’ image model |
| **Lead/Worker Pattern** | A smart model makes the plan; simpler models execute it | GPT-4 plans, local model implements |

---

### Why Multiple Models?

â•â•â• CORE CONCEPT â•â•â•

> **No single AI model is best at everything.** Just like you wouldn't ask a novelist to debug your code, you shouldn't ask a writing model to generate images.
>
> - **Writing AI:** Excellent at prose, summaries, creative content
> - **Code AI:** Trained specifically on programming languages
> - **Image AI:** Completely different architecture, generates pixels not text
> - **Reasoning AI:** Better at logic, planning, breaking down complex tasks

---

### The Orchestra Analogy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE ORCHESTRATOR                          â”‚
â”‚                    (The Conductor)                           â”‚
â”‚                          â”‚                                   â”‚
â”‚    "Build me a project   â”‚                                   â”‚
â”‚     management page"     â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚        TASK BREAKDOWN          â”‚                  â”‚
â”‚         â”‚ 1. Plan the page structure     â”‚                  â”‚
â”‚         â”‚ 2. Write the content           â”‚                  â”‚
â”‚         â”‚ 3. Generate header image       â”‚                  â”‚
â”‚         â”‚ 4. Create data schema          â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚     â–¼                    â–¼                    â–¼             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ â”‚Reasoningâ”‚          â”‚Writing â”‚          â”‚ Image  â”‚         â”‚
â”‚ â”‚  Model  â”‚          â”‚ Model  â”‚          â”‚ Model  â”‚         â”‚
â”‚ â”‚ (Plan)  â”‚          â”‚(Content)â”‚         â”‚(SDXL)  â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚     â”‚                    â”‚                    â”‚             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚   COMBINED RESULT   â”‚                        â”‚
â”‚              â”‚ (Page with content, â”‚                        â”‚
â”‚              â”‚  schema, and image) â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### The Lead/Worker Pattern

This is the key pattern for making local AI practical:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLEX REQUEST                       â”‚
â”‚           "Write a blog post series on AI"              â”‚
â”‚                         â”‚                                â”‚
â”‚                         â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              LEAD MODEL (GPT-4 Cloud)            â”‚    â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚  "Here's the plan:                              â”‚    â”‚
â”‚  â”‚   Post 1: Introduction - 500 words              â”‚    â”‚
â”‚  â”‚   Post 2: History - 700 words                   â”‚    â”‚
â”‚  â”‚   Post 3: Future - 600 words                    â”‚    â”‚
â”‚  â”‚   Each should have..."                          â”‚    â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚  [Complex reasoning, one-time API cost]         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚                                 â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚           â–¼            â–¼            â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   WORKER    â”‚ â”‚   WORKER    â”‚ â”‚   WORKER    â”‚        â”‚
â”‚  â”‚  (Local 7B) â”‚ â”‚  (Local 7B) â”‚ â”‚  (Local 7B) â”‚        â”‚
â”‚  â”‚             â”‚ â”‚             â”‚ â”‚             â”‚        â”‚
â”‚  â”‚ Write Post 1â”‚ â”‚ Write Post 2â”‚ â”‚ Write Post 3â”‚        â”‚
â”‚  â”‚             â”‚ â”‚             â”‚ â”‚             â”‚        â”‚
â”‚  â”‚ [Free, fast,â”‚ â”‚ [Free, fast,â”‚ â”‚ [Free, fast,â”‚        â”‚
â”‚  â”‚  local]     â”‚ â”‚  local]     â”‚ â”‚  local]     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¡ **Tip:** The lead/worker pattern balances cost and quality. Use expensive cloud AI for the hard thinking (once), then cheap local AI for the bulk work.

---

### Key Takeaways

- âœ“ Different AI models excel at different tasks
- âœ“ An "orchestrator" coordinates which model handles what
- âœ“ The lead/worker pattern: smart model plans, simple models execute
- âœ“ This approach balances quality, cost, and speed
- âœ“ All coordination happens in the Python backend

**See Also:** [Section 6 - Multi-Agent Orchestration](#24-multi-agent-orchestration)

---

# 21. Architecture Decisions {#21-architecture-decisions}

This section covers the major architectural choices for Project Handshake, based on research and multi-source analysis.

---

## 21.1 Desktop Shell: Tauri vs Electron {#211-desktop-shell-tauri-vs-electron}

**Prerequisites:** Section 2.1 (Desktop Shell concepts)  
**Related to:** Section 3.2 (Overall Architecture)  
**Implements:** Core technology choice  
**Read time:** ~7 minutes

**This section explains why Tauri was chosen over Electron as the desktop shell, based on consensus from multiple AI advisors and research documents.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Chromium** | The open-source browser that Chrome is built on | Electron bundles this; it's why Electron apps are large |
| **Rust** | A programming language focused on speed and safety | Tauri's backend is written in Rust |
| **System WebView** | The browser component already on your computer | Tauri uses this instead of bundling Chromium |
| **Binary Size** | How big the app installer is | Tauri: ~10-30MB; Electron: ~100-200MB |
| **Memory Footprint** | RAM used when app is running | Critical when AI models need that RAM |

---

### The Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Desktop application shell          â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Electron (used by VS Code, Slack, Discord)              â”‚
â”‚   â€¢ Tauri (newer, used by some AI apps)                     â”‚
â”‚   â€¢ Flutter (AppFlowy uses this - different paradigm)       â”‚
â”‚                                                              â”‚
â”‚ Recommendation: TAURI                                        â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ 90% less memory usage (crucial for AI models)           â”‚
â”‚   â€¢ Smaller install size                                     â”‚
â”‚   â€¢ Better security model for plugins                        â”‚
â”‚   â€¢ Python backend means shell is "just a wrapper"          â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Smaller ecosystem than Electron                          â”‚
â”‚   â€¢ Rust knowledge needed for advanced shell features       â”‚
â”‚   â€¢ Some webview quirks across operating systems            â”‚
â”‚   â€¢ AFFiNE actually switched FROM Tauri TO Electron         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Head-to-Head Comparison

| Factor | Electron | Tauri | Winner for Handshake |
|--------|----------|-------|---------------------|
| **Memory at idle** | 150-300 MB | 10-50 MB | âš¡ **Tauri** |
| **Install size** | 100-200 MB | 10-30 MB | **Tauri** |
| **Startup time** | 1-2 seconds | Sub-second | **Tauri** |
| **Ecosystem maturity** | Excellent | Growing | Electron |
| **Documentation** | Extensive | Good | Electron |
| **Security model** | Permissive | Deny-by-default | âš¡ **Tauri** |
| **Cross-platform consistency** | Very consistent | Some quirks | Electron |
| **Node.js integration** | Built-in | Not applicable | Electron |
| **Rust backend** | Not applicable | Built-in | Context-dependent |

---

### Why Memory Matters So Much

â•â•â• CORE CONCEPT â•â•â•

```
Available GPU Memory (RTX 3090): 24 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WITH ELECTRON (300MB shell overhead):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚  LLM Model (14GB)          â”‚  SDXL(~8GB)  â”‚
â”‚                            â”‚  Cramped!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
System RAM also constrained for model loading

WITH TAURI (30MB shell overhead):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚  LLM Model (14GB)          â”‚  SDXL (10GB) â”‚
â”‚                            â”‚  Comfortable â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
270MB more RAM available for models/context
```

---

### The Research Consensus

Three independent analyses (GPT-4, Claude, and Gemini) were asked to evaluate this decision. **All three recommended Tauri** for the following reasons:

ğŸ“Œ **Key Points from Multi-AI Analysis:**

1. **Resource Efficiency Under AI Load**
   > "Every megabyte of RAM you save in the shell is headroom for bigger models, more context windows, and smoother SDXL runs."

2. **Architecture Alignment**
   > "Your backend is Python, not Node. The hard logic is not written in Rust; it is in Python and TypeScript."

3. **Long-Term Product Vision**
   > "This is not a tiny helper tool; it is your primary local-first, multi-model AI workspace."

4. **Security for Plugins**
   > "Tauri has a stricter, deny-by-default permission model, which makes it safer to load third-party code."

---

### âš ï¸ Risk: AFFiNE's Tauri-to-Electron Switch

One research document notes that AFFiNE, a similar local-first workspace app, **switched FROM Tauri BACK to Electron** due to webview limitations on macOS.

**Mitigation strategies:**
- Test extensively on all target platforms early
- Keep Tauri shell responsibilities minimal (just window management and IPC)
- Design the architecture so a shell swap is possible if absolutely necessary
- Monitor Tauri's development and webview improvements

---

### What Tauri Actually Does in This Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TAURI'S RESPONSIBILITIES                  â”‚
â”‚                    (Keep this list SHORT)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Create application window                                â”‚
â”‚  âœ“ Load the React UI                                        â”‚
â”‚  âœ“ Spawn Python backend process                             â”‚
â”‚  âœ“ Handle file system access (with permissions)             â”‚
â”‚  âœ“ Manage window state (minimize, maximize, etc.)           â”‚
â”‚  âœ“ Surface system metrics (GPU usage, memory)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    NOT TAURI'S JOB                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ— AI orchestration (Python does this)                      â”‚
â”‚  âœ— Data processing (Python/TypeScript)                      â”‚
â”‚  âœ— Business logic (React/Python)                            â”‚
â”‚  âœ— Model management (Python backend)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¡ **Tip:** Think of Tauri as a "thin wrapper"â€”it should do as little as possible. Complex logic stays in Python and TypeScript where iteration is easier.

---

### Key Takeaways

- âœ“ **Decision: Use Tauri** as the desktop shell
- âœ“ Primary reason: Memory efficiency for AI models
- âœ“ Secondary reasons: Security model, smaller installs, faster startup
- âœ“ Risk acknowledged: AFFiNE switched away; we mitigate by keeping Tauri's role minimal
- âœ“ Frontend code (React/TypeScript) works identically in both shells
- âœ“ If issues arise, shell swap is possible without rewriting business logic

**See Also:** [Section 3.2 - Overall System Architecture](#212-overall-system-architecture)

---

## 21.2 Overall System Architecture {#212-overall-system-architecture}

**Prerequisites:** Section 2.1-2.4 (Foundation Concepts), Section 3.1 (Tauri Decision)  
**Related to:** All implementation sections  
**Implements:** System blueprint  
**Read time:** ~8 minutes

**This section presents the complete system architecture: how all the pieces connect and communicate.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Frontend** | The part users see and interact with (buttons, text, etc.) | React/TypeScript in the Tauri window |
| **Backend** | The "behind the scenes" code that does heavy lifting | Python: AI, file processing, orchestration |
| **API** | A set of "commands" one program can send to another | How frontend talks to backend |
| **REST API** | A common style for APIs using web requests (GET, POST, etc.) | Simple, well-understood pattern |
| **WebSocket** | A persistent connection for real-time, two-way communication | For streaming AI responses |
| **Monorepo** | One repository containing multiple related projects | Frontend and backend code together |
| **Microservices** | Breaking an app into separate, independent services | Each AI model could be its own service |

---

### The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER'S COMPUTER                                â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                        TAURI SHELL                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚                    REACT FRONTEND                          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚                                                            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚ Document â”‚   â”‚  Canvas  â”‚   â”‚  Sheets  â”‚   Â·Â·Â·       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚  Editor  â”‚   â”‚  Board   â”‚   â”‚  Grid    â”‚             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚(Tiptap)  â”‚   â”‚(Excali)  â”‚   â”‚(Hyper)   â”‚             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚                                                            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚              FILE TREE SIDEBAR                      â”‚  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â”‚     (Workspace Navigator)                           â”‚  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                       â”‚
â”‚                    HTTP/WebSocket (localhost)                           â”‚
â”‚                                  â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      PYTHON BACKEND                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚                   ORCHESTRATOR                          â”‚    â”‚    â”‚
â”‚  â”‚   â”‚              (AutoGen or LangGraph)                     â”‚    â”‚    â”‚
â”‚  â”‚   â”‚                                                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚   Planner   â”‚   â”‚   Writer    â”‚   â”‚   Coder     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚   Agent     â”‚   â”‚   Agent     â”‚   â”‚   Agent     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                â”‚                                 â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚                  MODEL RUNTIMES                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚                                                         â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚  Ollama â”‚   â”‚  vLLM   â”‚   â”‚ComfyUI  â”‚   â”‚Cloud  â”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â”‚  (LLMs) â”‚   â”‚  (LLMs) â”‚   â”‚ (SDXL)  â”‚   â”‚Fallbckâ”‚  â”‚    â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                â”‚                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                   â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     LOCAL FILE SYSTEM                             â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚   /Handshake/                                                    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ workspaces/                                                â”‚    â”‚
â”‚  â”‚   â”‚   â””â”€â”€ my-project/                                           â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ notes/           (Markdown files)                 â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ canvas/          (JSON board data)                â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ sheets/          (CSV/JSON data)                  â”‚    â”‚
â”‚  â”‚   â”‚       â”œâ”€â”€ images/          (Generated + uploaded)           â”‚    â”‚
â”‚  â”‚   â”‚       â””â”€â”€ .handshake/      (Metadata, CRDT state)          â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ models/                  (Downloaded AI models)           â”‚    â”‚
â”‚  â”‚   â””â”€â”€ config/                  (User settings)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                         â”‚   OPTIONAL CLOUD   â”‚                           â”‚
â”‚                         â”‚  (Google Drive,    â”‚                           â”‚
â”‚                         â”‚   GPT-4 API, etc.) â”‚                           â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Architecture Pattern: Monorepo with Hybrid Processes

â•â•â• CORE CONCEPT â•â•â•

> **One codebase, multiple processes.** Everything lives in one Git repository, but runs as separate programs that communicate over the network.
>
> ```
> /handshake-repo/
> â”œâ”€â”€ ui/              # React/TypeScript frontend
> â”œâ”€â”€ backend/         # Python orchestrator + APIs  
> â”œâ”€â”€ shared/          # Type definitions, schemas
> â””â”€â”€ docs/            # Documentation
> ```
>
> This gives us:
> - âœ“ Unified versioning (frontend and backend always match)
> - âœ“ Isolation (Python crash doesn't kill UI)
> - âœ“ Flexibility (can restart backend without UI reload)

---

### Communication Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERACTION FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User clicks "Summarize this document"                      â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  2. React sends HTTP POST to localhost:8000/api/summarize      â”‚
â”‚     {                                                          â”‚
â”‚       "document_id": "abc123",                                 â”‚
â”‚       "style": "brief"                                         â”‚
â”‚     }                                                          â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  3. Python backend receives, routes to orchestrator            â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  4. Orchestrator picks model: local Llama 3 (13B)             â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  5. Model generates summary, streaming via WebSocket           â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  6. React displays streaming text to user                      â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚  7. Final result saved to document file                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Why Not Full Microservices?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: How to structure backend services          â”‚
â”‚                                                                      â”‚
â”‚ Options researched:                                                  â”‚
â”‚   â€¢ Full microservices (each model in its own Docker container)     â”‚
â”‚   â€¢ Monolith (everything in one Python process)                     â”‚
â”‚   â€¢ Hybrid (multiple processes, no containers)                      â”‚
â”‚                                                                      â”‚
â”‚ Recommendation: HYBRID APPROACH                                      â”‚
â”‚                                                                      â”‚
â”‚ Rationale:                                                           â”‚
â”‚   â€¢ Full microservices adds Docker complexity                       â”‚
â”‚   â€¢ Monolith risks one crash killing everything                     â”‚
â”‚   â€¢ Hybrid: spawn Python processes for each service                 â”‚
â”‚                                                                      â”‚
â”‚ Implementation:                                                      â”‚
â”‚   â€¢ Main orchestrator process                                        â”‚
â”‚   â€¢ Model runtimes as separate processes (can restart independently)â”‚
â”‚   â€¢ Communication via localhost HTTP (simple, debuggable)           â”‚
â”‚                                                                      â”‚
â”‚ Tradeoffs:                                                           â”‚
â”‚   â€¢ Slightly more complex than monolith                             â”‚
â”‚   â€¢ Less isolated than Docker (shared filesystem)                   â”‚
â”‚   â€¢ Good balance for desktop app context                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Startup Sequence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APP STARTUP SEQUENCE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. User double-clicks Handshake.app                         â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  2. Tauri shell starts                                       â”‚
â”‚     â€¢ Creates application window                             â”‚
â”‚     â€¢ Loads React frontend                                   â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  3. Tauri spawns Python backend                              â”‚
â”‚     â€¢ python -m handshake.server                            â”‚
â”‚     â€¢ Backend starts on localhost:8000                       â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  4. Backend initializes orchestrator                         â”‚
â”‚     â€¢ Loads model registry (what models are available)       â”‚
â”‚     â€¢ Does NOT load models yet (wait for demand)             â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  5. Frontend polls /health endpoint                          â”‚
â”‚     â€¢ Shows "Loading..." until backend ready                 â”‚
â”‚     â€¢ Then displays workspace                                â”‚
â”‚                        â”‚                                      â”‚
â”‚                        â–¼                                      â”‚
â”‚  6. First AI request triggers model loading                  â”‚
â”‚     â€¢ Model loaded to GPU on first use                       â”‚
â”‚     â€¢ Subsequent requests are fast                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Three-layer architecture:** Tauri shell â†’ React frontend â†’ Python backend
- âœ“ **Monorepo structure:** All code in one repository, easier to manage
- âœ“ **Hybrid process model:** Multiple processes, no Docker complexity
- âœ“ **File-tree based storage:** Human-readable, portable data
- âœ“ **Lazy model loading:** Models load on first use, not at startup
- âœ“ **Local-first with cloud options:** Works offline, syncs when available

**See Also:** [Section 3.3 - Data Architecture](#213-data-architecture-file-tree-model)

---

## 21.3 Data Architecture: File-Tree Model {#213-data-architecture-file-tree-model}

**Prerequisites:** Section 2.2 (Local-First), Section 3.2 (Overall Architecture)  
**Related to:** Section 7 (Collaboration and Sync)  
**Implements:** Data storage approach  
**Read time:** ~6 minutes

**Instead of a traditional database, Handshake stores data as files in foldersâ€”like how you organize documents on your computer, but structured for the application.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **File-Tree Architecture** | Using folders and files instead of a database | Data is human-readable, portable, git-friendly |
| **Workspace** | A project or collection of related documents | Top-level folder for a user's project |
| **Sidecar File** | A small file that travels with another file (like subtitles with a video) | Stores metadata without modifying original files |
| **SQLite** | A lightweight database in a single file | Used for indexing/search, not primary storage |
| **CRDT State** | The sync information stored alongside content | Enables conflict-free collaboration |

---

### Why Files Instead of a Database?

â•â•â• CORE CONCEPT â•â•â•

> **Your data should be yours, in formats you can read.**
>
> | Database Approach | File-Tree Approach |
> |-------------------|-------------------|
> | Data locked in app-specific format | Data in Markdown, JSON, CSV |
> | Need special tools to read | Open in any text editor |
> | Backup requires export | Copy folder = backup |
> | Hard to version control | Git works perfectly |
> | App dies = data access complex | App dies = files remain |

---

### The Folder Structure

```
/Handshake/
â”‚
â”œâ”€â”€ workspaces/                          # All user projects
â”‚   â”‚
â”‚   â”œâ”€â”€ my-startup-project/              # One workspace
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ notes/                       # Document editor content
â”‚   â”‚   â”‚   â”œâ”€â”€ meeting-notes.md         # Markdown files
â”‚   â”‚   â”‚   â”œâ”€â”€ product-spec.md
â”‚   â”‚   â”‚   â””â”€â”€ .meta/                   # Metadata sidecar
â”‚   â”‚   â”‚       â”œâ”€â”€ meeting-notes.json   # Block IDs, timestamps
â”‚   â”‚   â”‚       â””â”€â”€ product-spec.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ canvas/                      # Moodboard/canvas content
â”‚   â”‚   â”‚   â”œâ”€â”€ brainstorm.json          # Board data
â”‚   â”‚   â”‚   â””â”€â”€ wireframes.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ sheets/                      # Spreadsheet data
â”‚   â”‚   â”‚   â”œâ”€â”€ budget.csv               # Actual data (portable!)
â”‚   â”‚   â”‚   â””â”€â”€ .meta/
â”‚   â”‚   â”‚       â””â”€â”€ budget.json          # Formulas, formatting
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ databases/                   # Notion-style databases
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.json               # Structured data
â”‚   â”‚   â”‚   â””â”€â”€ contacts.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ images/                      # All images
â”‚   â”‚   â”‚   â”œâ”€â”€ generated/               # AI-created
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ logo-v1.png
â”‚   â”‚   â”‚   â””â”€â”€ uploaded/                # User-added
â”‚   â”‚   â”‚       â””â”€â”€ reference.jpg
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ .handshake/                  # App-specific data
â”‚   â”‚       â”œâ”€â”€ workspace.json           # Settings, preferences
â”‚   â”‚       â”œâ”€â”€ crdt/                    # Sync state (if enabled)
â”‚   â”‚       â”‚   â””â”€â”€ sync-state.bin
â”‚   â”‚       â””â”€â”€ index.db                 # SQLite search index
â”‚   â”‚
â”‚   â””â”€â”€ personal-notes/                  # Another workspace
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                              # Downloaded AI models
â”‚   â”œâ”€â”€ llama-3-13b.gguf
â”‚   â”œâ”€â”€ codellama-7b.gguf
â”‚   â””â”€â”€ sdxl-base.safetensors
â”‚
â””â”€â”€ config/                              # Global settings
    â”œâ”€â”€ settings.json
    â”œâ”€â”€ api-keys.encrypted               # Google OAuth, etc.
    â””â”€â”€ model-registry.json              # What models are available
```

---

### File Formats by Content Type

| Content Type | Primary Format | Why This Format |
|-------------|----------------|-----------------|
| **Documents** | Markdown (.md) | Universal, readable, version-control friendly |
| **Canvas Boards** | JSON | Structured data, easy to parse |
| **Spreadsheets** | CSV + JSON sidecar | CSV = data (portable), JSON = formulas/formatting |
| **Databases** | JSON | Flexible schema, human-readable |
| **Images** | PNG/JPG + JSON sidecar | Standard formats, sidecar stores AI prompts |
| **Sync State** | Binary CRDT | Compact, efficient for sync algorithms |
| **Search Index** | SQLite | Fast full-text search |

---

### How AI-Generated Images Are Stored

```
/images/generated/
â”‚
â”œâ”€â”€ logo-v1.png                          # The actual image
â”‚
â””â”€â”€ logo-v1.json                         # Sidecar metadata
    {
      "generated_at": "2025-11-29T10:30:00Z",
      "model": "sdxl-1.0",
      "prompt": "minimalist tech startup logo, blue gradient",
      "negative_prompt": "text, watermark",
      "seed": 42,
      "steps": 30,
      "cfg_scale": 7.5,
      "workflow": "comfyui/basic-txt2img.json"
    }
```

ğŸ’¡ **Tip:** Storing generation parameters means you can recreate or tweak images later. The sidecar JSON acts like a "recipe" for the image.

---

### The Role of SQLite

âš ï¸ **Important:** SQLite is used for **indexing**, not as the primary data store.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA vs. INDEX                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  FILES (Source of Truth)           SQLite (Index/Cache)     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  â€¢ Markdown documents      â”€â”€â”€â–º   â€¢ Full-text search        â”‚
â”‚  â€¢ JSON databases          â”€â”€â”€â–º   â€¢ Tag lookups             â”‚
â”‚  â€¢ Canvas boards           â”€â”€â”€â–º   â€¢ Quick queries           â”‚
â”‚  â€¢ Spreadsheets            â”€â”€â”€â–º   â€¢ Recent files list       â”‚
â”‚                                                              â”‚
â”‚  If SQLite corrupts, rebuild from files.                    â”‚
â”‚  Files are authoritative; SQLite is derived.                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Files are the source of truth**, not a database
- âœ“ Standard formats (Markdown, CSV, JSON) = portable, readable data
- âœ“ Sidecar files store metadata without modifying originals
- âœ“ SQLite used only for fast search/indexing
- âœ“ Folder structure mirrors logical organization
- âœ“ AI generation parameters stored for reproducibility

**See Also:** [Section 7 - Collaboration and Sync](#25-collaboration-and-sync)

---

# 22. User Interface Components {#22-user-interface-components}

This section covers the frontend UI components that make up the Handshake user experience, combining the best features of Notion, Milanote, and Excel.

---

## 22.1 Rich Text Editor (Notion-like) {#221-rich-text-editor-notion-like}

**Prerequisites:** Section 3.2 (Overall Architecture)  
**Related to:** Section 4.4 (Additional Views)  
**Implements:** Core document editing  
**Read time:** ~6 minutes

**The document editor is the heart of Handshakeâ€”a "block-based" editor where every paragraph, image, and element is a separate, movable piece.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Block-Based Editor** | Instead of one continuous document, content is made of stackable "blocks" (paragraphs, images, lists, etc.) | Enables drag/drop, AI operations on specific sections |
| **Tiptap** | A popular open-source editor framework built on ProseMirror | Leading candidate for our editor |
| **BlockNote** | A Notion-style block editor built on Tiptap | Pre-built Notion-like components |
| **Slash Commands** | Type "/" to get a menu of things to insert (like /heading, /image) | Familiar UX from Notion |
| **Real-Time Collaboration** | Multiple people editing the same document simultaneously | Requires CRDT integration |

---

### The Block Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRADITIONAL DOCUMENT                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  One continuous blob of formatted text                      â”‚
â”‚  that flows from top to bottom. Hard to                     â”‚
â”‚  rearrange, hard for AI to understand                       â”‚
â”‚  structure.                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         vs.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BLOCK-BASED DOCUMENT                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Heading                                       â”‚ â˜°  â”‚
â”‚  â”‚ "Project Overview"                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Paragraph                                     â”‚ â˜°  â”‚
â”‚  â”‚ "This project aims to..."                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: AI-Generated Summary                         â”‚ â˜°  â”‚
â”‚  â”‚ "Key points: 1) ... 2) ... 3) ..."                 â”‚ ğŸ¤– â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ BLOCK: Image                                        â”‚ â˜°  â”‚
â”‚  â”‚ [diagram.png]                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â˜° = Drag handle (reorder blocks)                          â”‚
â”‚  ğŸ¤– = AI-generated content indicator                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Technology Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Rich text editor framework         â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Tiptap/ProseMirror - Most extensible, proven            â”‚
â”‚   â€¢ BlockNote - Notion-style, built on Tiptap               â”‚
â”‚   â€¢ Lexical (Meta) - Newer, less collaboration support      â”‚
â”‚   â€¢ Slate.js - Flexible but complex                         â”‚
â”‚                                                              â”‚
â”‚ Recommendation: TIPTAP with BLOCKNOTE components             â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ BlockNote provides Notion-style blocks out of the box   â”‚
â”‚   â€¢ Tiptap is highly extensible for custom AI blocks        â”‚
â”‚   â€¢ Yjs integration available for real-time collaboration   â”‚
â”‚   â€¢ Large community and good documentation                   â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Some learning curve                                      â”‚
â”‚   â€¢ May need custom extensions for AI features              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Block Types to Implement

| Block Type | Priority | Description |
|------------|----------|-------------|
| **Paragraph** | [CORE] | Basic text |
| **Heading** | [CORE] | H1, H2, H3 |
| **List** | [CORE] | Bullet, numbered, checklist |
| **Image** | [CORE] | With AI generation capability |
| **Code** | [CORE] | Syntax highlighting |
| **Quote** | [CORE] | Blockquotes |
| **Divider** | [CORE] | Horizontal rule |
| **Table** | [OPTIONAL] | Basic tables |
| **Callout** | [OPTIONAL] | Colored highlight boxes |
| **Toggle** | [OPTIONAL] | Collapsible sections |
| **Embed** | [ADVANCED] | YouTube, tweets, etc. |
| **Database View** | [ADVANCED] | Inline Notion-style databases |
| **AI Block** | [CORE] | AI-generated content with indicators |

---

### AI Integration Points

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI-ENHANCED EDITING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SLASH COMMAND MENU (type "/")                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ / Basic                         â”‚                        â”‚
â”‚  â”‚   Paragraph, Heading, List...   â”‚                        â”‚
â”‚  â”‚                                 â”‚                        â”‚
â”‚  â”‚ / AI Actions âœ¨                 â”‚                        â”‚
â”‚  â”‚   ğŸ“ Generate text              â”‚                        â”‚
â”‚  â”‚   ğŸ“‹ Summarize above            â”‚                        â”‚
â”‚  â”‚   ğŸ”„ Rewrite selection          â”‚                        â”‚
â”‚  â”‚   ğŸŒ Translate                  â”‚                        â”‚
â”‚  â”‚   ğŸ¨ Generate image             â”‚                        â”‚
â”‚  â”‚   ğŸ’» Generate code              â”‚                        â”‚
â”‚  â”‚   ğŸ“Š Create table from text     â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  CONTEXT MENU (select text, right-click)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Improve writing                 â”‚                        â”‚
â”‚  â”‚ Make shorter                    â”‚                        â”‚
â”‚  â”‚ Make longer                     â”‚                        â”‚
â”‚  â”‚ Fix grammar                     â”‚                        â”‚
â”‚  â”‚ Explain this                    â”‚                        â”‚
â”‚  â”‚ Ask AI...                       â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Block-based editing** enables flexible layouts and AI operations
- âœ“ **Tiptap + BlockNote** is the recommended stack
- âœ“ **Slash commands** provide quick access to AI features
- âœ“ Blocks can be drag-and-dropped, nested, and reordered
- âœ“ Real-time collaboration via Yjs integration

**See Also:** [Section 7.1 - Understanding CRDTs](#251-understanding-crdts)

---

## 22.2 Freeform Canvas (Milanote-like) {#222-freeform-canvas-milanote-like}

**Prerequisites:** Section 3.2 (Overall Architecture)  
**Related to:** Section 4.1 (Rich Text Editor)  
**Implements:** Visual brainstorming space  
**Read time:** ~5 minutes

**The canvas is an infinite whiteboard where you can drag notes, images, and shapes anywhereâ€”like a digital corkboard for visual thinkers.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Infinite Canvas** | A workspace that extends forever in all directions | No page boundaries, unlimited space |
| **Excalidraw** | Popular open-source whiteboard with hand-drawn look | Leading candidate for our canvas |
| **React-Konva** | Library for drawing graphics in React | Alternative for custom canvas needs |
| **Pan & Zoom** | Moving around and magnifying the canvas | Essential for large boards |

---

### The Canvas vs. Document Distinction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT EDITOR                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Text flows top-to-bottom                          â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Linear structure                                  â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Like a Word document or web page                  â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  BEST FOR: Writing, documentation, structured content       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CANVAS BOARD                              â”‚
â”‚                                                              â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚      â”‚ Note  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Image â”‚               â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚            \                                                â”‚
â”‚             \     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚              â”€â”€â”€â”€â”€â”‚ Idea Box  â”‚                             â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”‚                                    â”‚
â”‚    â”‚Sketch â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ Reference â”‚            â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  BEST FOR: Brainstorming, mood boards, spatial thinking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Technology Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Canvas/whiteboard library          â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Excalidraw - Mature, MIT-licensed, hand-drawn feel      â”‚
â”‚   â€¢ tldraw - Modern, React-focused, good collaboration      â”‚
â”‚   â€¢ React-Konva - Low-level, full control                   â”‚
â”‚   â€¢ Fabric.js - Canvas library, more work to integrate      â”‚
â”‚                                                              â”‚
â”‚ Recommendation: EXCALIDRAW                                   â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ Production-proven (used by many products)               â”‚
â”‚   â€¢ Built-in collaboration support                          â”‚
â”‚   â€¢ Familiar "whiteboard" UX                                â”‚
â”‚   â€¢ Can embed in React easily                               â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ "Hand-drawn" aesthetic may not fit all use cases        â”‚
â”‚   â€¢ May need customization for Milanote-style features      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Canvas Element Types

| Element | Priority | Description |
|---------|----------|-------------|
| **Sticky Note** | [CORE] | Text cards that can be moved |
| **Image** | [CORE] | Photos, generated images |
| **Shape** | [CORE] | Rectangles, circles, arrows |
| **Line/Arrow** | [CORE] | Connect elements |
| **Text** | [CORE] | Freestanding labels |
| **Drawing** | [OPTIONAL] | Freehand sketching |
| **Frame/Group** | [OPTIONAL] | Organize related items |
| **Embedded Note** | [ADVANCED] | Link to full document |
| **AI Image Generation** | [CORE] | Generate images directly on canvas |

---

### AI Integration for Canvas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI-ENHANCED CANVAS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  RIGHT-CLICK ON CANVAS:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ¨ Generate image here...       â”‚                        â”‚
â”‚  â”‚ ğŸ“ Add AI note about...         â”‚                        â”‚
â”‚  â”‚ ğŸ’¡ Brainstorm ideas about...    â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  SELECT MULTIPLE ITEMS:                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ“‹ Summarize selected items     â”‚                        â”‚
â”‚  â”‚ ğŸ”— Find connections             â”‚                        â”‚
â”‚  â”‚ ğŸ“Š Organize into categories     â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  DRAG IMAGE ONTO CANVAS:                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ” Describe this image          â”‚                        â”‚
â”‚  â”‚ ğŸ¨ Generate variations          â”‚                        â”‚
â”‚  â”‚ âœ‚ï¸ Remove background             â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Canvas = spatial thinking**, Documents = linear thinking
- âœ“ **Excalidraw** is the recommended library
- âœ“ Supports infinite pan/zoom, drag-and-drop
- âœ“ AI can generate images directly onto canvas
- âœ“ Works alongside (not replacing) the document editor

---

## 22.3 Spreadsheet Engine (Excel-like) {#223-spreadsheet-engine-excel-like}

**Prerequisites:** Section 3.2 (Overall Architecture)  
**Related to:** Section 4.1 (Rich Text Editor)  
**Implements:** Data manipulation capabilities  
**Read time:** ~5 minutes

**Spreadsheets let you organize data in rows and columns with formulasâ€”essential for budgets, project tracking, and any structured data work.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Cell** | One box in the grid (like A1, B2) | The basic unit of spreadsheets |
| **Formula** | An equation that calculates a value (=SUM(A1:A10)) | What makes spreadsheets powerful |
| **HyperFormula** | Open-source formula engine with 400+ functions | The "brain" that calculates formulas |
| **Data Grid** | UI component for displaying/editing cell tables | What the user sees and interacts with |
| **Handsontable** | Popular JavaScript spreadsheet grid | One option for the UI layer |

---

### The Separation: UI vs. Engine

â•â•â• CORE CONCEPT â•â•â•

> **Two separate pieces work together:**
>
> 1. **Data Grid (UI)** - What you see: cells, scrolling, selection, editing
> 2. **Formula Engine** - The math: calculating formulas, dependencies
>
> ```
> â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
> â”‚                    USER TYPES: =SUM(A1:A3)              â”‚
> â”‚                           â”‚                              â”‚
> â”‚                           â–¼                              â”‚
> â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
> â”‚  â”‚              DATA GRID (Handsontable)           â”‚    â”‚
> â”‚  â”‚  "User typed something in B1"                   â”‚    â”‚
> â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
> â”‚                           â”‚                              â”‚
> â”‚                           â–¼                              â”‚
> â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
> â”‚  â”‚            FORMULA ENGINE (HyperFormula)        â”‚    â”‚
> â”‚  â”‚  "=SUM(A1:A3) equals 150"                       â”‚    â”‚
> â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
> â”‚                           â”‚                              â”‚
> â”‚                           â–¼                              â”‚
> â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
> â”‚  â”‚              DATA GRID (Handsontable)           â”‚    â”‚
> â”‚  â”‚  "Display 150 in cell B1"                       â”‚    â”‚
> â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
> â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
> ```

---

### Technology Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Spreadsheet implementation         â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   Grid UI:                                                   â”‚
â”‚   â€¢ Handsontable - Feature-rich, some license concerns      â”‚
â”‚   â€¢ AG Grid - Professional, complex                         â”‚
â”‚   â€¢ Wolf-Table (x-spreadsheet) - Lightweight                â”‚
â”‚                                                              â”‚
â”‚   Formula Engine:                                            â”‚
â”‚   â€¢ HyperFormula - 400+ functions, open source              â”‚
â”‚                                                              â”‚
â”‚ Recommendation: WOLF-TABLE + HYPERFORMULA                    â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ HyperFormula is clearly the best formula engine         â”‚
â”‚   â€¢ Wolf-Table is lightweight and MIT-licensed              â”‚
â”‚   â€¢ Combination gives Excel-like functionality              â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Less polished than Handsontable out-of-box             â”‚
â”‚   â€¢ May need more custom work for advanced features         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Feature Scope

| Feature | Priority | Notes |
|---------|----------|-------|
| **Basic cells** | [CORE] | Text, numbers, dates |
| **Formulas** | [CORE] | HyperFormula's 400+ functions |
| **Cell formatting** | [CORE] | Bold, colors, alignment |
| **Copy/paste** | [CORE] | Including from Excel |
| **Sorting/filtering** | [CORE] | Column operations |
| **Multiple sheets** | [OPTIONAL] | Tabs within workbook |
| **Charts** | [OPTIONAL] | Basic visualizations |
| **Pivot tables** | [ADVANCED] | Data summarization |
| **Scripts/macros** | [ADVANCED] | Automation |

---

### AI Integration for Spreadsheets

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI-ENHANCED SPREADSHEETS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SELECT CELLS, RIGHT-CLICK:                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ“Š Analyze this data            â”‚                        â”‚
â”‚  â”‚ ğŸ“ Explain this formula         â”‚                        â”‚
â”‚  â”‚ ğŸ”§ Fix this formula             â”‚                        â”‚
â”‚  â”‚ ğŸ“ˆ Suggest visualizations       â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  NATURAL LANGUAGE FORMULAS:                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ User types: "total of column A" â”‚                        â”‚
â”‚  â”‚ AI suggests: =SUM(A:A)          â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â”‚  DATA GENERATION:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ "Fill with sample customer data"â”‚                        â”‚
â”‚  â”‚ AI generates realistic test dataâ”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Two components:** Data Grid (UI) + Formula Engine (HyperFormula)
- âœ“ **HyperFormula** provides Excel-compatible formulas
- âœ“ Data stored as CSV (portable) with JSON sidecar for formatting
- âœ“ AI can help write formulas and analyze data
- âœ“ Start simple, add advanced features later

---

## 22.4 Additional Views: Kanban, Calendar, Timeline {#224-additional-views-kanban-calendar-timeline}

**Prerequisites:** Section 4.1 (Rich Text Editor), Section 4.3 (Spreadsheets)  
**Related to:** Section 3.3 (Data Architecture)  
**Implements:** Notion-style database views  
**Read time:** ~4 minutes

**The same data can be viewed different ways: as a table, as Kanban cards, as calendar events, or as a timeline.**

---

### The "Views" Concept

â•â•â• CORE CONCEPT â•â•â•

> **One dataset, many presentations.** A list of tasks can be:
> - A **table** (spreadsheet-style rows)
> - A **Kanban board** (cards in columns like "To Do", "In Progress", "Done")
> - A **calendar** (if tasks have dates)
> - A **timeline/Gantt** (showing duration and dependencies)
>
> The underlying data is identical; only the visualization changes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SAME DATA, DIFFERENT VIEWS                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DATABASE: Tasks                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ID â”‚ Title        â”‚ Status      â”‚ Due Date â”‚ Owner   â”‚   â”‚
â”‚  â”‚ 1  â”‚ Design logo  â”‚ In Progress â”‚ Dec 1    â”‚ Alice   â”‚   â”‚
â”‚  â”‚ 2  â”‚ Write copy   â”‚ To Do       â”‚ Dec 3    â”‚ Bob     â”‚   â”‚
â”‚  â”‚ 3  â”‚ Launch site  â”‚ To Do       â”‚ Dec 10   â”‚ Alice   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚           â”‚                    â”‚                    â”‚        â”‚
â”‚           â–¼                    â–¼                    â–¼        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   TABLE      â”‚  â”‚   KANBAN     â”‚  â”‚   CALENDAR   â”‚       â”‚
â”‚  â”‚   VIEW       â”‚  â”‚   VIEW       â”‚  â”‚   VIEW       â”‚       â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚       â”‚
â”‚  â”‚ Spreadsheet  â”‚  â”‚ To Do â”‚ In   â”‚  â”‚    Dec       â”‚       â”‚
â”‚  â”‚ style rows   â”‚  â”‚       â”‚Progr â”‚  â”‚ 1 [Design]   â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ [Copy]â”‚[Logo]â”‚  â”‚ 3 [Copy]     â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ [Site]â”‚      â”‚  â”‚ 10 [Launch]  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Implementation Priority

| View Type | Priority | Library Options |
|-----------|----------|-----------------|
| **Table** | [CORE] | AG Grid, React Table |
| **Kanban** | [CORE] | react-beautiful-dnd, dnd-kit |
| **Calendar** | [OPTIONAL] | FullCalendar, react-big-calendar |
| **Timeline/Gantt** | [ADVANCED] | frappe-gantt, custom |
| **Gallery** | [OPTIONAL] | Custom grid layout |

---

### Key Takeaways

- âœ“ Views are different visualizations of the same data
- âœ“ Kanban is high priority (project management is a key use case)
- âœ“ Start with Table and Kanban, add Calendar later
- âœ“ Database structure stored in JSON files

---

# 23. AI Model Strategy {#23-ai-model-strategy}

This section details which AI models to use, how to run them locally, and when to fall back to cloud services.

---

## 23.1 Model Categories and Recommendations {#231-model-categories-and-recommendations}

**Prerequisites:** Section 2.3 (AI Models Basics)  
**Related to:** Section 5.2 (Local Model Runtimes)  
**Implements:** AI model selection  
**Read time:** ~7 minutes

**Different tasks need different AI models. This section recommends specific models for each task type based on the research.**

---

### Model Categories Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI MODELS BY TASK TYPE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GENERAL LLM     â”‚    â”‚ CODE MODEL      â”‚    â”‚ IMAGE MODEL     â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ Writing         â”‚    â”‚ Code generation â”‚    â”‚ Image generationâ”‚  â”‚
â”‚  â”‚ Summarizing     â”‚    â”‚ Code completion â”‚    â”‚ Image editing   â”‚  â”‚
â”‚  â”‚ Q&A             â”‚    â”‚ Bug fixing      â”‚    â”‚ Style transfer  â”‚  â”‚
â”‚  â”‚ Translation     â”‚    â”‚ Code review     â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚
â”‚  â”‚ Llama 3         â”‚    â”‚ Code Llama      â”‚    â”‚ SDXL            â”‚  â”‚
â”‚  â”‚ Mistral         â”‚    â”‚ StarCoder       â”‚    â”‚ Stable Diffusionâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ REASONING/      â”‚    â”‚ CREATIVE        â”‚                         â”‚
â”‚  â”‚ PLANNING        â”‚    â”‚ WRITING         â”‚                         â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚ Task breakdown  â”‚    â”‚ Fiction         â”‚                         â”‚
â”‚  â”‚ Decision making â”‚    â”‚ Storytelling    â”‚                         â”‚
â”‚  â”‚ Multi-step      â”‚    â”‚ Brainstorming   â”‚                         â”‚
â”‚  â”‚ planning        â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚                         â”‚
â”‚  â”‚ GPT-OSS-20B     â”‚    â”‚ NeuralStar      â”‚                         â”‚
â”‚  â”‚ DeepSeek        â”‚    â”‚ 4x7B MoE        â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Specific Model Recommendations

Based on the research, here are recommended models for each category:

#### General Writing & Reasoning

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **Llama 3 13B** | 13B | ~14GB (Q4) | Balanced quality/speed | Default text tasks |
| **Mistral 7B** | 7B | ~8GB (Q4) | Fast, efficient | Quick responses |
| **GPT-OSS-20B** | 20B | ~16GB | Strong reasoning | Complex planning |

ğŸ“Œ **Recommendation:** Start with **Llama 3 13B** as the default general model. Use Mistral 7B for fast, simple tasks.

---

#### Code Generation

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **Code Llama 13B** | 13B | ~14GB (Q4) | Multi-language | Primary code model |
| **Code Llama 7B** | 7B | ~7GB (Q4) | Fast completion | Autocomplete |
| **StarCoder 15B** | 15B | ~15GB | Broad language support | Alternative |

ğŸ“Œ **Recommendation:** **Code Llama 13B** for code generation, 7B variant for real-time autocomplete.

---

#### Image Generation

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **SDXL 1.0** | ~3B | ~10GB | Best quality | Primary image gen |
| **SD 1.5** | ~1B | ~4GB | Faster, lighter | Quick drafts |

ğŸ“Œ **Recommendation:** **SDXL 1.0** via ComfyUI for quality image generation.

---

#### Creative Writing (Specialized)

| Model | Size | VRAM Needed | Strengths | Use For |
|-------|------|-------------|-----------|---------|
| **NeuralStar AlphaWriter 4x7B** | 24B MoE | ~20GB (Q4) | Fiction-tuned | Stories, creative |

â”€â”€â”€ Nice to Know â”€â”€â”€

> **MoE (Mixture of Experts)** means the model has multiple "expert" sub-models inside. Only some experts activate for each request, making it more efficient than a dense 24B model.

---

### Memory Budget Planning

â•â•â• CORE CONCEPT â•â•â•

> **You can't run all models at once.** With 24GB VRAM on an RTX 3090, plan which models are loaded when:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VRAM BUDGET (24GB RTX 3090)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SCENARIO A: Text-focused work                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚   Llama 3 13B (14GB)        â”‚     Free (10GB)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  SCENARIO B: Image generation                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚         SDXL (10GB)         â”‚ Mistral 7B â”‚ Free  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  SCENARIO C: Code + Chat                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚     â”‚
â”‚  â”‚   Code Llama 13B    â”‚   Mistral 7B   â”‚   Free    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  âš¡ Models swap in/out based on task                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Llama 3 13B** is the recommended default general model
- âœ“ **Code Llama 13B** for code tasks, 7B for autocomplete
- âœ“ **SDXL 1.0** via ComfyUI for image generation
- âœ“ Models swap in/out of VRAM based on current task
- âœ“ The 24GB RTX 3090 can handle most scenarios with smart scheduling

---

## 23.2 Local Model Runtimes {#232-local-model-runtimes}

**Prerequisites:** Section 5.1 (Model Categories)  
**Related to:** Section 3.2 (Overall Architecture)  
**Implements:** How models actually run  
**Read time:** ~5 minutes

**A "runtime" is the software that loads AI models and runs them. Different runtimes have different strengths.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **Ollama** | Easy-to-use model runner, like "Docker for AI models" | Simplest way to run local LLMs |
| **vLLM** | High-performance model server from Berkeley | Best for production, supports batching |
| **llama.cpp** | Efficient CPU/GPU inference, uses GGUF format | Most flexible for quantized models |
| **ComfyUI** | Node-based UI for Stable Diffusion | Best for image generation workflows |
| **TGI** | HuggingFace's text generation server | Alternative to vLLM |

---

### Runtime Comparison

| Runtime | Ease of Use | Performance | Flexibility | Best For |
|---------|-------------|-------------|-------------|----------|
| **Ollama** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | Quick start, development |
| **vLLM** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Production, high throughput |
| **llama.cpp** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Custom setups, edge cases |
| **ComfyUI** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Image generation (required) |

---

### Recommended Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RUNTIME ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PYTHON ORCHESTRATOR                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚          â–¼                â–¼                â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   OLLAMA    â”‚  â”‚   COMFYUI   â”‚  â”‚   CLOUD     â”‚         â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   APIs      â”‚         â”‚
â”‚  â”‚ â€¢ Llama 3   â”‚  â”‚ â€¢ SDXL      â”‚  â”‚ â€¢ GPT-4     â”‚         â”‚
â”‚  â”‚ â€¢ Mistral   â”‚  â”‚ â€¢ SD 1.5    â”‚  â”‚ â€¢ Claude    â”‚         â”‚
â”‚  â”‚ â€¢ CodeLlama â”‚  â”‚ â€¢ Workflows â”‚  â”‚ (fallback)  â”‚         â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚         â”‚
â”‚  â”‚ Port: 11434 â”‚  â”‚ Port: 8188  â”‚  â”‚ HTTPS       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                              â”‚
â”‚  Development: Start with Ollama (easiest)                   â”‚
â”‚  Production: Consider vLLM for better performance           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Ollama** for LLMs (easiest to set up and manage)
- âœ“ **ComfyUI** for image generation (required for SDXL workflows)
- âœ“ Cloud APIs as fallback for complex tasks
- âœ“ All runtimes expose HTTP APIs that the Python orchestrator calls

---

## 23.3 Cloud Fallback Strategy {#233-cloud-fallback-strategy}

**Prerequisites:** Section 5.1-5.2 (Models and Runtimes)  
**Related to:** Section 6.2 (Lead/Worker Pattern)  
**Implements:** Handling tasks too hard for local models  
**Read time:** ~4 minutes

**When local models aren't enough, fall back to powerful cloud APIsâ€”but do it strategically to minimize cost.**

---

### When to Use Cloud

| Use Cloud When | Why |
|---------------|-----|
| Local model fails/low confidence | Quality matters |
| Task requires 100K+ context | Local models limited to 4-32K |
| Complex multi-step reasoning | Cloud models more capable |
| User explicitly requests "best" | Preference for quality over speed |

| Stay Local When | Why |
|----------------|-----|
| Simple summarization | Local handles fine |
| Basic Q&A about document | Fast and free |
| Code completion | Real-time speed needed |
| Privacy-sensitive content | Data stays local |

---

### Cost-Aware Routing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTELLIGENT ROUTING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  USER REQUEST: "Write a marketing strategy"                 â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              COMPLEXITY ANALYSIS                     â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Length estimate: ~2000 words                       â”‚    â”‚
â”‚  â”‚  Reasoning required: High                           â”‚    â”‚
â”‚  â”‚  Domain knowledge: Marketing (general)              â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  âš¡ DECISION: Use Lead/Worker pattern               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚           â–¼                               â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ CLOUD (GPT-4)   â”‚            â”‚ LOCAL (Llama)   â”‚        â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚        â”‚
â”‚  â”‚ Create outline  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Write sections  â”‚        â”‚
â”‚  â”‚ and strategy    â”‚            â”‚ based on        â”‚        â”‚
â”‚  â”‚ framework       â”‚            â”‚ outline         â”‚        â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚        â”‚
â”‚  â”‚ Cost: ~$0.10    â”‚            â”‚ Cost: $0.00     â”‚        â”‚
â”‚  â”‚ (one-time)      â”‚            â”‚ (unlimited)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ Cloud APIs for planning and complex reasoning (paid but smart)
- âœ“ Local models for execution and bulk work (free)
- âœ“ Automatic fallback when local quality is insufficient
- âœ“ User can override to force local or cloud

---

## 23.4 Image Generation with ComfyUI {#234-image-generation-with-comfyui}

**Prerequisites:** Section 5.1 (Model Categories)  
**Related to:** Section 4.2 (Canvas)  
**Implements:** AI image generation  
**Read time:** ~5 minutes

**ComfyUI is a node-based tool for creating images with AI. Instead of just typing a prompt, you can build complex image processing pipelines.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **ComfyUI** | Visual tool for building AI image generation workflows | Our image generation backend |
| **Workflow** | A saved pipeline of image processing steps | Can be triggered programmatically |
| **Node** | One step in the pipeline (like "load model" or "apply style") | Building blocks of workflows |
| **Checkpoint** | A saved AI model file | SDXL base, custom fine-tunes |
| **ControlNet** | Guides image generation with poses, edges, etc. | Advanced control over output |

---

### Why ComfyUI?

â•â•â• CORE CONCEPT â•â•â•

> ComfyUI workflows are **saved as JSON** and can be **triggered via API**. This means:
> 1. Design complex pipelines visually
> 2. Save them as templates
> 3. Trigger from Handshake with different prompts
> 4. Receive generated images back

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMFYUI INTEGRATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  USER IN HANDSHAKE                                          â”‚
â”‚  "Generate a logo for my startup"                           â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PYTHON ORCHESTRATOR                     â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  1. Pick workflow: "logo_generation.json"           â”‚    â”‚
â”‚  â”‚  2. Insert prompt into workflow                     â”‚    â”‚
â”‚  â”‚  3. POST to ComfyUI API                             â”‚    â”‚
â”‚  â”‚  4. Poll for completion                             â”‚    â”‚
â”‚  â”‚  5. Retrieve generated image                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              COMFYUI (localhost:8188)                â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  [Load SDXL]â”€â”€â–¶[CLIP Encode]â”€â”€â–¶[KSampler]â”€â”€â–¶[Save] â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  IMAGE RETURNED + SAVED WITH METADATA                       â”‚
â”‚  (prompt, seed, settings stored in sidecar JSON)           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Pre-Built Workflows to Create

| Workflow | Description | Use Case |
|----------|-------------|----------|
| **txt2img_basic** | Simple text to image | Quick generations |
| **txt2img_quality** | High quality with refiner | Final outputs |
| **img2img** | Modify existing image | Variations |
| **inpaint** | Edit parts of image | Touch-ups |
| **upscale** | Increase resolution | Print-ready |

---

### Key Takeaways

- âœ“ **ComfyUI** runs as a separate service, controlled via API
- âœ“ Workflows are JSON files that can be version controlled
- âœ“ Generated images stored with full metadata for reproducibility
- âœ“ Can build progressively complex workflows over time

---

# 24. Multi-Agent Orchestration {#24-multi-agent-orchestration}

This section covers how multiple AI models coordinate to accomplish complex tasks.

---

## 24.1 Framework Comparison: AutoGen vs LangGraph vs CrewAI {#241-framework-comparison-autogen-vs-langgraph-vs-crewai}

**Prerequisites:** Section 2.4 (Multi-Model Orchestration)  
**Related to:** Section 5 (AI Model Strategy)  
**Implements:** Orchestration framework choice  
**Read time:** ~6 minutes

**Orchestration frameworks help coordinate multiple AI agents. Each framework has a different approach and strengths.**

---

### Framework Philosophies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THREE APPROACHES TO ORCHESTRATION               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  AUTOGEN (Microsoft)                                        â”‚
â”‚  Philosophy: Agents CONVERSE with each other                â”‚
â”‚                                                              â”‚
â”‚       Agent A â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Agent B                â”‚
â”‚          â”‚                              â”‚                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Agent C â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  Like: A meeting where experts discuss until done           â”‚
â”‚  Best for: Complex reasoning, human-in-loop                 â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  LANGGRAPH (LangChain)                                      â”‚
â”‚  Philosophy: Tasks flow through a GRAPH of steps            â”‚
â”‚                                                              â”‚
â”‚       [Start]â”€â”€â–¶[Plan]â”€â”€â–¶[Execute]â”€â”€â–¶[Review]â”€â”€â–¶[End]      â”‚
â”‚                    â”‚                    â”‚                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                         (if review fails)                   â”‚
â”‚                                                              â”‚
â”‚  Like: A flowchart where you define exactly what happens    â”‚
â”‚  Best for: Predictable workflows, complex conditionals      â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  CREWAI                                                     â”‚
â”‚  Philosophy: Agents have ROLES and work in SEQUENCE         â”‚
â”‚                                                              â”‚
â”‚       [Researcher]â”€â”€â–¶[Writer]â”€â”€â–¶[Editor]â”€â”€â–¶[Publisher]     â”‚
â”‚                                                              â”‚
â”‚  Like: An assembly line with specialists                    â”‚
â”‚  Best for: Simple, linear workflows                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Detailed Comparison

| Aspect | AutoGen | LangGraph | CrewAI |
|--------|---------|-----------|--------|
| **Learning Curve** | Medium | High | Low |
| **Flexibility** | High | Very High | Medium |
| **Debugging** | Conversation logs | Visual graph | Role inspection |
| **Human-in-Loop** | Excellent | Good | Limited |
| **Complex Branching** | Good | Excellent | Limited |
| **Setup Effort** | Medium | Higher | Low |
| **Documentation** | Good | Good | Growing |
| **Local-First** | Yes | Yes | Yes |

---

### Decision Point

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: Multi-agent orchestration frameworkâ”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ AutoGen - Conversational agents, Microsoft-backed        â”‚
â”‚   â€¢ LangGraph - Graph-based workflows, very flexible         â”‚
â”‚   â€¢ CrewAI - Simple role-based pipelines                    â”‚
â”‚                                                              â”‚
â”‚ Recommendation: START WITH AUTOGEN, consider LangGraph      â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ AutoGen balances power and approachability              â”‚
â”‚   â€¢ Good human-in-loop support (important for AI trust)     â”‚
â”‚   â€¢ Microsoft backing suggests long-term maintenance        â”‚
â”‚   â€¢ Can migrate to LangGraph if more control needed         â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ Less explicit flow control than LangGraph               â”‚
â”‚   â€¢ Conversation logging can be verbose                     â”‚
â”‚   â€¢ May need custom work for complex branching              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **AutoGen** recommended for initial development
- âœ“ **LangGraph** as alternative if explicit flow control needed
- âœ“ **CrewAI** too limited for complex Handshake workflows
- âœ“ All frameworks run locally with any LLM

---

## 24.2 The Lead/Worker Pattern {#242-the-leadworker-pattern}

**Prerequisites:** Section 6.1 (Framework Comparison)  
**Related to:** Section 5.3 (Cloud Fallback)  
**Implements:** Cost-effective multi-model approach  
**Read time:** ~4 minutes

**Use a powerful model to PLAN, then cheaper models to EXECUTE. This balances quality and cost.**

---

### The Pattern Explained

â•â•â• CORE CONCEPT â•â•â•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEAD/WORKER PATTERN                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  COMPLEX TASK: "Create a product launch plan with           â”‚
â”‚                 marketing copy and social media posts"      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              LEAD (GPT-4 Cloud)                      â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  "Here's the plan:                                  â”‚    â”‚
â”‚  â”‚   1. Executive summary (100 words)                  â”‚    â”‚
â”‚  â”‚   2. Target audience analysis                       â”‚    â”‚
â”‚  â”‚   3. Key messaging (3 bullet points)                â”‚    â”‚
â”‚  â”‚   4. Timeline with milestones                       â”‚    â”‚
â”‚  â”‚   5. Social posts: Twitter (3), LinkedIn (2)        â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚   Each section should follow format X..."           â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Cost: $0.15 (one complex reasoning call)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           WORKERS (Local Llama 3 13B)               â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Task 1: Write executive summary â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 2: Write audience analysis â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 3: Write key messaging â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 4: Create timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚  Task 5: Write social posts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Done        â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Cost: $0.00 (local, unlimited)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  TOTAL COST: ~$0.15 instead of ~$1.50+ if all cloud        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ Smart model plans, simple model executes
- âœ“ Reduces cloud API costs by 90%+
- âœ“ Local execution is fast and private
- âœ“ Fall back to cloud lead if local worker fails

---

## 24.3 Shared Context and Memory {#243-shared-context-and-memory}

**Prerequisites:** Section 6.1-6.2 (Orchestration basics)  
**Related to:** Section 3.3 (Data Architecture)  
**Implements:** How agents share information  
**Read time:** ~4 minutes

**Agents need to share information. A "shared memory" system ensures one agent's output is available to others.**

---

### Memory Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SHARED MEMORY SYSTEM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              SHARED CONTEXT STORE                    â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â€¢ Conversation history (all agents)                â”‚    â”‚
â”‚  â”‚  â€¢ Working documents (current task files)           â”‚    â”‚
â”‚  â”‚  â€¢ User preferences and context                     â”‚    â”‚
â”‚  â”‚  â€¢ Retrieved knowledge (RAG results)                â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚              â”‚              â”‚                      â”‚
â”‚         â–¼              â–¼              â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Planner  â”‚   â”‚ Writer   â”‚   â”‚ Reviewer â”‚                â”‚
â”‚  â”‚  Agent   â”‚   â”‚  Agent   â”‚   â”‚  Agent   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â”‚  STORAGE OPTIONS:                                           â”‚
â”‚  â€¢ File system (matches our data architecture)             â”‚
â”‚  â€¢ SQLite for structured queries                           â”‚
â”‚  â€¢ Vector store for semantic search (ChromaDB/FAISS)       â”‚
â”‚  â€¢ Redis/ZeroMQ for real-time passing                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ Agents share context through a central store
- âœ“ File-based storage aligns with overall architecture
- âœ“ Vector store enables semantic search over past interactions
- âœ“ Essential for coherent multi-step tasks

---

## 24.4 Task Routing and Fallback Logic {#244-task-routing-and-fallback-logic}

**Prerequisites:** Section 6.1-6.3  
**Related to:** Section 5 (AI Model Strategy)  
**Implements:** Intelligent model selection  
**Read time:** ~4 minutes

**The orchestrator must decide which model handles each task, and what to do if it fails.**

---

### Routing Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TASK ROUTING LOGIC                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  NEW TASK ARRIVES                                           â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it code-related? â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ Code Llama            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it image gen?    â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ SDXL/ComfyUI          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Is it complex       â”‚â”€â”€â”€â”€ Yes â”€â”€â–¶ Lead/Worker           â”‚
â”‚  â”‚ multi-step?         â”‚            (GPT-4 â†’ Local)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ Default             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Local LLM (Llama 3)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  IF ANY MODEL FAILS:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Check error type                                 â”‚    â”‚
â”‚  â”‚ 2. If quality issue â†’ retry with larger model      â”‚    â”‚
â”‚  â”‚ 3. If timeout â†’ retry with smaller model           â”‚    â”‚
â”‚  â”‚ 4. If persistent failure â†’ escalate to cloud       â”‚    â”‚
â”‚  â”‚ 5. Log everything for debugging                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ Route tasks based on type and complexity
- âœ“ Automatic fallback on failures
- âœ“ Comprehensive logging for debugging
- âœ“ User can override routing preferences

---

# 25. Collaboration and Sync {#25-collaboration-and-sync}

This section covers how Handshake enables multiple users and devices to work together.

---

## 25.1 Understanding CRDTs {#251-understanding-crdts}

**Prerequisites:** Section 2.2 (Local-First)  
**Related to:** Section 7.2 (Offline-First Architecture)  
**Implements:** Conflict-free collaboration  
**Read time:** ~5 minutes

**CRDTs are special data structures that allow multiple people to edit simultaneously without conflictsâ€”even while offline.**

---

### Jargon Glossary

| Term | Plain English | Why It Matters for Handshake |
|------|--------------|------------------------------|
| **CRDT** | Conflict-free Replicated Data Type - data that merges automatically | Enables real-time collaboration |
| **Yjs** | Most popular JavaScript CRDT library | Our likely choice for sync |
| **Automerge** | Alternative CRDT library | Fallback option |
| **Merge** | Combining two versions of a document | Happens automatically with CRDTs |
| **Operational Transform (OT)** | Older technique (Google Docs uses this) | CRDTs are newer and better for offline |

---

### How CRDTs Work (Simplified)

â•â•â• CORE CONCEPT â•â•â•

> Traditional documents: "Last write wins" (someone's work gets lost)
> 
> CRDT documents: "All writes merge" (everyone's work is preserved)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TRADITIONAL SYNC (CONFLICTS!)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Original: "Hello World"                                    â”‚
â”‚                                                              â”‚
â”‚  Alice (offline):  "Hello World!" (added !)                 â”‚
â”‚  Bob (offline):    "Hello Earth" (changed World)            â”‚
â”‚                                                              â”‚
â”‚  When both sync:                                            â”‚
â”‚  âŒ CONFLICT! Which version wins?                           â”‚
â”‚  â€¢ Keep Alice's? Bob loses his change.                      â”‚
â”‚  â€¢ Keep Bob's? Alice loses her change.                      â”‚
â”‚  â€¢ Show conflict dialog? Annoying.                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CRDT SYNC (NO CONFLICTS!)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Original: "Hello World"                                    â”‚
â”‚                                                              â”‚
â”‚  Alice (offline): Insert "!" at position 11                 â”‚
â”‚  Bob (offline):   Replace "World" with "Earth"              â”‚
â”‚                                                              â”‚
â”‚  When both sync:                                            â”‚
â”‚  âœ… CRDT merges both operations:                            â”‚
â”‚  Result: "Hello Earth!"                                     â”‚
â”‚                                                              â”‚
â”‚  Both changes preserved! No conflict dialog!                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Yjs: Our CRDT Choice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION POINT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What needs to be decided: CRDT implementation                â”‚
â”‚                                                              â”‚
â”‚ Options researched:                                          â”‚
â”‚   â€¢ Yjs - Most popular, used by many editors                â”‚
â”‚   â€¢ Automerge - Good, Rust implementation available         â”‚
â”‚   â€¢ Custom - Too much work                                   â”‚
â”‚                                                              â”‚
â”‚ Recommendation: YJS                                          â”‚
â”‚                                                              â”‚
â”‚ Rationale:                                                   â”‚
â”‚   â€¢ Tiptap (our editor) has Yjs integration built-in       â”‚
â”‚   â€¢ Large ecosystem and community                           â”‚
â”‚   â€¢ Works offline natively                                   â”‚
â”‚   â€¢ Can sync via any transport (WebSocket, WebRTC, file)   â”‚
â”‚                                                              â”‚
â”‚ Tradeoffs:                                                   â”‚
â”‚   â€¢ JavaScript-focused (need yrs for Rust interop)         â”‚
â”‚   â€¢ Learning curve for CRDT concepts                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **CRDTs automatically merge edits** without conflicts
- âœ“ **Yjs** is the recommended library
- âœ“ Works perfectly with offline-first architecture
- âœ“ Tiptap editor has built-in Yjs support

---

## 25.2 Offline-First Architecture {#252-offline-first-architecture}

**Prerequisites:** Section 7.1 (CRDTs)  
**Related to:** Section 3.3 (Data Architecture)  
**Implements:** Working without internet  
**Read time:** ~3 minutes

**Handshake works completely offline. Sync happens when you're online, but it's never required.**

---

### How Offline-First Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OFFLINE-FIRST FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  WORKING OFFLINE:                                           â”‚
â”‚  1. All data stored locally in files                       â”‚
â”‚  2. AI models run locally                                   â”‚
â”‚  3. Changes saved with CRDT state                          â”‚
â”‚  4. Everything works normally                               â”‚
â”‚                                                              â”‚
â”‚  WHEN ONLINE:                                               â”‚
â”‚  1. Check for remote changes                               â”‚
â”‚  2. CRDT automatically merges                              â”‚
â”‚  3. Push local changes to cloud (optional)                 â”‚
â”‚  4. Continue working                                        â”‚
â”‚                                                              â”‚
â”‚  SYNC IS OPTIONAL:                                          â”‚
â”‚  â€¢ Works forever with no account                           â”‚
â”‚  â€¢ Add sync when you want multi-device                     â”‚
â”‚  â€¢ Choose sync provider (Google Drive, custom server)      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ App is fully functional offline
- âœ“ Sync is optional, not required
- âœ“ CRDTs handle conflict-free merging
- âœ“ User chooses if/where to sync

---

## 25.3 Google Workspace Integration {#253-google-workspace-integration}

**Prerequisites:** Section 7.2 (Offline-First)  
**Related to:** Section 3.2 (Overall Architecture)  
**Implements:** Gmail, Drive, Calendar sync  
**Read time:** ~4 minutes

**Optionally sync with Google services: backup to Drive, import emails, show calendar events.**

---

### Integration Points

| Service | Integration | Priority |
|---------|-------------|----------|
| **Google Drive** | Backup workspace, sync files | [OPTIONAL] |
| **Gmail** | Import emails as documents | [OPTIONAL] |
| **Calendar** | Show events in calendar view | [OPTIONAL] |
| **Google Docs** | Export/import documents | [ADVANCED] |

---

### OAuth2 Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GOOGLE AUTH FLOW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. User clicks "Connect Google Account"                    â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  2. Opens system browser to Google login                    â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  3. User grants permissions (minimal scopes)                â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  4. Google redirects back to app with auth code             â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  5. App exchanges code for tokens                           â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  6. Tokens stored encrypted locally                         â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  7. App can now call Google APIs                            â”‚
â”‚                                                              â”‚
â”‚  SECURITY: Tokens never leave user's machine                â”‚
â”‚  PRIVACY: Minimal scopes requested                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ Google integration is **optional**
- âœ“ OAuth2 for secure authentication
- âœ“ Tokens stored locally and encrypted
- âœ“ Minimal permission scopes requested

---

# 26. Plugin and Extension System {#26-plugin-and-extension-system}

This section covers how to design Handshake as an extensible platform.

---

## 26.1 Plugin Architecture Patterns {#261-plugin-architecture-patterns}

**Prerequisites:** Section 3.2 (Overall Architecture)  
**Related to:** Section 8.2 (Security)  
**Implements:** Extensibility foundation  
**Read time:** ~5 minutes

**A good plugin system lets third parties (and you) extend the app without modifying core code.**

---

### Lessons from Reference Apps

Based on research of existing apps:

| App | Plugin Approach | Lesson for Handshake |
|-----|-----------------|---------------------|
| **Obsidian** | JS plugins in main process | Large ecosystem, some stability risks |
| **Joplin** | Sandboxed, separate process | Safer but more complex |
| **Logseq** | JS API, ClojureScript | Good API, some breaking changes |
| **VS Code** | Extension host process | Gold standard, but complex |

---

### Recommended Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PLUGIN ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PHASE 1 (MVP): Internal Extension Points                   â”‚
â”‚  â€¢ Define stable internal APIs                              â”‚
â”‚  â€¢ Build core features as "internal plugins"                â”‚
â”‚  â€¢ Establishes patterns for later                           â”‚
â”‚                                                              â”‚
â”‚  PHASE 2: User Scripts                                      â”‚
â”‚  â€¢ Allow simple automation scripts                          â”‚
â”‚  â€¢ Sandboxed JavaScript/Python execution                    â”‚
â”‚  â€¢ Limited API surface                                      â”‚
â”‚                                                              â”‚
â”‚  PHASE 3: Full Plugin System                                â”‚
â”‚  â€¢ Public plugin API                                        â”‚
â”‚  â€¢ Plugin marketplace                                        â”‚
â”‚  â€¢ Sandboxed execution (like Joplin)                        â”‚
â”‚  â€¢ Permission model                                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Extension Categories to Plan For

| Category | Examples | API Needed |
|----------|----------|-----------|
| **Custom Blocks** | New editor block types | Block registration, rendering |
| **AI Agents** | Specialized AI workflows | Agent API, model access |
| **Integrations** | Third-party services | HTTP, auth storage |
| **Views** | New database views | View registration, data access |
| **Themes** | Visual customization | CSS variables, style hooks |

---

### Key Takeaways

- âœ“ Design with extensibility in mind from day one
- âœ“ Build core features as internal "plugins" first
- âœ“ Full plugin system is Phase 3, not MVP
- âœ“ Learn from Obsidian's success and Joplin's security

---

## 26.2 Security and Sandboxing {#262-security-and-sandboxing}

**Prerequisites:** Section 8.1 (Plugin Architecture)  
**Related to:** Section 3.1 (Tauri Decision)  
**Implements:** Safe plugin execution  
**Read time:** ~4 minutes

**Plugins can be dangerous. Sandboxing restricts what they can do to prevent damage.**

---

### Security Principles

â•â•â• CORE CONCEPT â•â•â•

> **Principle of Least Privilege:** Plugins get only the permissions they need, nothing more.
>
> | Permission Level | Can Access |
> |-----------------|------------|
> | **Level 0** | Nothing (pure computation) |
> | **Level 1** | Read workspace data |
> | **Level 2** | Write workspace data |
> | **Level 3** | Network access |
> | **Level 4** | Full filesystem access |
> | **Level 5** | Execute system commands |
>
> Most plugins should only need Levels 1-2.

---

### Tauri's Security Advantage

Tauri provides "deny-by-default" security:
- Plugins must explicitly request each capability
- User approves permissions on install
- Cleaner than Electron's more permissive model

---

### Key Takeaways

- âœ“ Sandbox plugin execution
- âœ“ Explicit permission requests
- âœ“ User approval for sensitive permissions
- âœ“ Tauri's security model helps here

---

# 27. Reference Application Analysis {#27-reference-application-analysis}

This section summarizes lessons from analyzing similar open-source applications.

---

## 27.1 AppFlowy {#271-appflowy}

**Stack:** Flutter (Dart) + Rust backend  
**Data:** CRDT-based (yrs), RocksDB storage  
**Sync:** Offline-first CRDT via Supabase

**Key Insights:**
- âœ“ Flutter provides native performance and feel
- âœ“ Rust CRDT implementation is solid
- âš ï¸ Flutter limits JavaScript plugin ecosystem
- âš ï¸ Minimal plugin API currently

---

## 27.2 AFFiNE {#272-affine}

**Stack:** Electron + React/TypeScript  
**Data:** OctoBase (custom Rust CRDT)  
**Sync:** P2P CRDT, local-first

**Key Insights:**
- âœ“ "Everything is a block" model works well
- âœ“ Blocksuite component library is promising
- âš ï¸ Switched from Tauri to Electron (webview issues)
- âš ï¸ Performance issues with large documents
- âš ï¸ No mature plugin API yet

---

## 27.3 Obsidian {#273-obsidian}

**Stack:** Electron + TypeScript  
**Data:** Plain Markdown files  
**Sync:** Local vault with optional Obsidian Sync

**Key Insights:**
- âœ“ Thriving plugin ecosystem (hundreds of plugins)
- âœ“ Markdown files = portable, future-proof
- âœ“ Excellent community engagement
- âœ“ Proprietary but well-regarded
- âš ï¸ Some performance issues with huge vaults

---

## 27.4 Logseq {#274-logseq}

**Stack:** Electron + ClojureScript  
**Data:** Markdown/EDN files, SQLite  
**Sync:** Git/WebDAV/LiveSync options

**Key Insights:**
- âœ“ Mature JS plugin API
- âœ“ Bidirectional linking works well
- âš ï¸ Performance issues with large graphs/pages
- âš ï¸ Team added pagination to mitigate

---

## 27.5 Lessons Learned {#275-lessons-learned}

**Prerequisites:** Sections 9.1-9.4  
**Implements:** Design guidance from research  
**Read time:** ~4 minutes

---

### Patterns to Follow

| Pattern | Why It Works | Handshake Application |
|---------|--------------|----------------------|
| **File-based storage** | Portable, user-owned data | âœ“ Already planned |
| **Block-based editing** | Flexible, AI-friendly | âœ“ Using Tiptap/BlockNote |
| **CRDT sync** | Offline-first, conflict-free | âœ“ Using Yjs |
| **Plugin API early** | Builds ecosystem | Plan internal APIs from start |

---

### Patterns to Avoid

| Anti-Pattern | What Went Wrong | Handshake Mitigation |
|--------------|-----------------|---------------------|
| **Full doc re-render** | AFFiNE lag on keystroke | Virtualization, incremental updates |
| **Monolithic DB** | Joplin RAM bloat | File-based with SQLite index only |
| **No export path** | Athens shutdown orphaned users | Standard formats, export from day 1 |
| **Tauri webview issues** | AFFiNE switched to Electron | Minimal Tauri responsibilities, test early |

---

### Key Takeaways

- âœ“ Learn from others' mistakes before building
- âœ“ Performance at scale is a real concern
- âœ“ Export/migration paths are essential
- âœ“ Plugin ecosystems take years to build

---

# 28. Development Workflow {#28-development-workflow}

This section covers how to actually build Handshake efficiently.

---

## 28.1 Using AI Coding Assistants Effectively {#281-using-ai-coding-assistants-effectively}

**Prerequisites:** Section 2.3 (AI Models)  
**Related to:** Section 10.2 (Project Health)  
**Implements:** Development efficiency  
**Read time:** ~5 minutes

**The research documents provide a clear model for using AI assistants during development.**

---

### The Three-Layer Model

â•â•â• CORE CONCEPT â•â•â•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AI ASSISTANTS IN DEVELOPMENT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         GPT-4 / CLAUDE (Architects)                  â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ Feature specs and requirements                   â”‚    â”‚
â”‚  â”‚  â€¢ Architecture decisions                           â”‚    â”‚
â”‚  â”‚  â€¢ Trade-off analysis                               â”‚    â”‚
â”‚  â”‚  â€¢ Code review                                      â”‚    â”‚
â”‚  â”‚  â€¢ Debugging complex issues                         â”‚    â”‚
â”‚  â”‚  â€¢ Test strategy                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         CODEX / CODE MODELS (Implementers)          â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ Writing code from specs                          â”‚    â”‚
â”‚  â”‚  â€¢ Mechanical refactoring                           â”‚    â”‚
â”‚  â”‚  â€¢ Generating tests                                 â”‚    â”‚
â”‚  â”‚  â€¢ Writing boilerplate                              â”‚    â”‚
â”‚  â”‚  â€¢ Documentation comments                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         N8N / AUTOMATION (Operations)               â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  USE FOR:                                           â”‚    â”‚
â”‚  â”‚  â€¢ CI/CD workflows                                  â”‚    â”‚
â”‚  â”‚  â€¢ Health monitoring                                â”‚    â”‚
â”‚  â”‚  â€¢ Notifications                                    â”‚    â”‚
â”‚  â”‚  â€¢ External integrations                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### AI Development Workflow

| Phase | Use Generalist (GPT-4/Claude) | Use Code Model (Codex) |
|-------|------------------------------|------------------------|
| **Planning** | âœ“ Define specs, goals, non-goals | |
| **Architecture** | âœ“ Design systems, APIs | Scaffold structure |
| **Implementation** | Review PRs | âœ“ Write code from specs |
| **Testing** | Design test strategy | âœ“ Write test code |
| **Debugging** | âœ“ Analyze logs, hypothesize | Apply fixes |
| **Documentation** | âœ“ Write overviews | Docstrings, comments |

---

### Key Takeaways

- âœ“ **Generalists (GPT-4/Claude)** for thinking, **Code models** for doing
- âœ“ Always write specs before code
- âœ“ AI reviews AI-generated code (human oversight too)
- âœ“ n8n for DevOps automation

---

## 28.2 Project Health and Hygiene {#282-project-health-and-hygiene}

**Prerequisites:** Section 10.1 (AI Assistants)  
**Related to:** Section 10.3 (CI/CD)  
**Implements:** Maintainable codebase  
**Read time:** ~5 minutes

**A clean, consistent codebase is essentialâ€”especially when AI assistants help write code.**

---

### The Single Health Command

â•â•â• CORE CONCEPT â•â•â•

> **One command to rule them all:** A single command that validates the entire codebase.
>
> ```bash
> make check   # or: npm run check / python -m tools.health_check
> ```
>
> This command runs:
> 1. Linters (code style)
> 2. Type checking
> 3. Tests (fast subset)
> 4. Build verification
>
> **Why it matters:** Humans and AI both have ONE clear way to know if code is "good."

---

### Tool Stack

| Layer | Python (Backend) | TypeScript (Frontend) |
|-------|-----------------|----------------------|
| **Linting** | Ruff or flake8 | ESLint |
| **Formatting** | Black | Prettier |
| **Type Checking** | Pydantic, mypy | TypeScript strict |
| **Testing** | pytest | vitest or jest |
| **Import Sorting** | isort or Ruff | ESLint rules |

---

### Pre-Commit Hooks

```yaml
# .pre-commit-config.yaml (example)
repos:
  - repo: local
    hooks:
      - id: format-python
        name: Format Python (Black)
        entry: black
        language: system
        files: \.py$
      - id: lint-python
        name: Lint Python (Ruff)
        entry: ruff check
        language: system
        files: \.py$
```

ğŸ’¡ **Tip:** Pre-commit hooks catch issues before they reach CI, saving time and keeping history clean.

---

### Key Takeaways

- âœ“ **One health command** for all checks
- âœ“ Linters and formatters for consistency
- âœ“ Pre-commit hooks to catch issues early
- âœ“ Type annotations for AI and human safety

---

## 28.3 CI/CD and Testing Strategy {#283-cicd-and-testing-strategy}

**Prerequisites:** Section 10.2 (Project Health)  
**Related to:** Section 11 (Development Roadmap)  
**Implements:** Automated quality assurance  
**Read time:** ~4 minutes

**Continuous Integration ensures every code change is tested automatically.**

---

### CI Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CI PIPELINE (on every push)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. LINT                                                    â”‚
â”‚     â””â”€ Ruff, ESLint                                        â”‚
â”‚                                                              â”‚
â”‚  2. TYPE CHECK                                              â”‚
â”‚     â””â”€ mypy, TypeScript                                    â”‚
â”‚                                                              â”‚
â”‚  3. UNIT TESTS                                              â”‚
â”‚     â””â”€ pytest, vitest (fast tests only)                    â”‚
â”‚                                                              â”‚
â”‚  4. BUILD                                                   â”‚
â”‚     â””â”€ Frontend bundle, backend validation                 â”‚
â”‚                                                              â”‚
â”‚  IF ALL PASS â†’ âœ… Ready to merge                            â”‚
â”‚  IF ANY FAIL â†’ âŒ Block merge, fix issues                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Testing Pyramid

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   E2E     â”‚  Few, slow, high confidence
            â”‚   Tests   â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Integration   â”‚  Some, medium speed
         â”‚    Tests      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Unit Tests    â”‚  Many, fast, low coupling
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ CI runs on every push/PR
- âœ“ Failures block merges
- âœ“ Fast tests in CI, slow tests on schedule
- âœ“ Testing pyramid: many unit, some integration, few E2E

---

# 29. Development Roadmap {#29-development-roadmap}

This section provides a practical build order for Project Handshake.

---

## 29.1 Phase Overview {#291-phase-overview}

**Prerequisites:** All previous sections  
**Implements:** Project execution plan  
**Read time:** ~5 minutes

---

### The Phases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEVELOPMENT PHASES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PHASE 0: FOUNDATION (2-4 weeks)                            â”‚
â”‚  â”œâ”€ Set up monorepo structure                               â”‚
â”‚  â”œâ”€ Tauri + React scaffolding                               â”‚
â”‚  â”œâ”€ Python backend skeleton                                 â”‚
â”‚  â”œâ”€ Health check / CI pipeline                              â”‚
â”‚  â””â”€ Basic IPC between frontend and backend                  â”‚
â”‚                                                              â”‚
â”‚  PHASE 1: CORE EDITING (4-6 weeks)                          â”‚
â”‚  â”œâ”€ Block editor with Tiptap/BlockNote                      â”‚
â”‚  â”œâ”€ File-tree storage system                                â”‚
â”‚  â”œâ”€ Workspace navigator sidebar                             â”‚
â”‚  â””â”€ Basic CRUD operations                                   â”‚
â”‚                                                              â”‚
â”‚  PHASE 2: AI INTEGRATION (4-6 weeks)                        â”‚
â”‚  â”œâ”€ Ollama integration for local LLM                        â”‚
â”‚  â”œâ”€ Basic AI actions (summarize, write, translate)          â”‚
â”‚  â”œâ”€ Orchestrator setup (AutoGen or LangGraph)               â”‚
â”‚  â””â”€ Streaming responses to UI                               â”‚
â”‚                                                              â”‚
â”‚  PHASE 3: VISUAL TOOLS (3-4 weeks)                          â”‚
â”‚  â”œâ”€ Excalidraw canvas integration                           â”‚
â”‚  â”œâ”€ Basic spreadsheet with HyperFormula                     â”‚
â”‚  â””â”€ ComfyUI integration for images                          â”‚
â”‚                                                              â”‚
â”‚  PHASE 4: POLISH & SYNC (4+ weeks)                          â”‚
â”‚  â”œâ”€ Yjs CRDT integration                                    â”‚
â”‚  â”œâ”€ Optional Google Drive sync                              â”‚
â”‚  â”œâ”€ UI polish and performance                               â”‚
â”‚  â””â”€ Packaging and distribution                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 29.2 MVP Definition {#292-mvp-definition}

**Minimum Viable Product** = Phases 0-2 completed

### MVP Features

| Feature | Included in MVP |
|---------|-----------------|
| Document editor | âœ… |
| File-tree storage | âœ… |
| Basic AI (summarize, write) | âœ… |
| Local LLM via Ollama | âœ… |
| Canvas/whiteboard | âŒ (Phase 3) |
| Spreadsheets | âŒ (Phase 3) |
| Image generation | âŒ (Phase 3) |
| Multi-device sync | âŒ (Phase 4) |

---

## 29.3 Build Order and Dependencies {#293-build-order-and-dependencies}

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPENDENCY GRAPH                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   [Monorepo Setup]                                          â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼              â–¼              â–¼                     â”‚
â”‚   [Tauri Shell]  [Python Backend]  [CI Pipeline]           â”‚
â”‚         â”‚              â”‚              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                        â–¼                                     â”‚
â”‚              [Frontend-Backend IPC]                         â”‚
â”‚                        â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼              â–¼              â–¼                     â”‚
â”‚   [Block Editor]  [File Storage]  [Ollama Integration]      â”‚
â”‚         â”‚              â”‚              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                        â–¼                                     â”‚
â”‚              [AI Actions in Editor]                         â”‚
â”‚                        â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼              â–¼              â–¼                     â”‚
â”‚     [Canvas]    [Spreadsheet]    [ComfyUI]                  â”‚
â”‚         â”‚              â”‚              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                        â–¼                                     â”‚
â”‚              [Yjs Sync / Polish]                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Takeaways

- âœ“ **Phase 0** must be solid before adding features
- âœ“ **MVP is achievable in ~12 weeks** with focused effort
- âœ“ Build foundational pieces first (IPC, storage, CI)
- âœ“ AI integration comes after basic editing works

---

# 30. Risk Assessment {#30-risk-assessment}

**Prerequisites:** All previous sections  
**Implements:** Risk awareness and mitigation  
**Read time:** ~4 minutes

---

### Risk Matrix

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **Tauri webview issues** | Medium | High | Minimal Tauri role; test early on all platforms |
| **Local model performance** | Medium | Medium | Cloud fallback; smaller model options |
| **Complexity overwhelm** | High | High | Strict MVP scope; phases; hire help |
| **CRDT learning curve** | Medium | Medium | Use Yjs (proven); start with single-user |
| **Plugin security** | Low | High | Delay plugins; learn from Joplin model |
| **Scope creep** | High | High | Written MVP definition; say no to extras |

---

### Complexity Ratings

| Component | Complexity | Notes |
|-----------|------------|-------|
| Tauri setup | âš ï¸ Medium | Some Rust knowledge needed |
| Block editor | âš ï¸ Medium | Tiptap helps a lot |
| AI orchestration | âš ï¸âš ï¸ High | Multi-model coordination is complex |
| Canvas | âš ï¸ Medium | Excalidraw does heavy lifting |
| Spreadsheets | âš ï¸ Medium | HyperFormula helps |
| CRDT sync | âš ï¸âš ï¸ High | Conceptually challenging |
| ComfyUI integration | âš ï¸ Medium | API-based, manageable |
| Plugin system | âš ï¸âš ï¸ High | Defer to post-MVP |

---

# 31. Technology Stack Summary {#31-technology-stack-summary}

**Complete list of technologies mentioned across all research documents.**

---

### Core Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Desktop Shell** | Tauri | Cross-platform wrapper |
| **Frontend** | React + TypeScript | User interface |
| **Backend** | Python (FastAPI) | API server, orchestration |
| **AI Runtime** | Ollama, ComfyUI | Model execution |
| **Storage** | File system + SQLite | Data persistence |
| **Sync** | Yjs (CRDT) | Collaboration |

---

### Frontend Libraries

| Library | Purpose |
|---------|---------|
| Tiptap / BlockNote | Block-based editor |
| Excalidraw | Canvas/whiteboard |
| HyperFormula | Spreadsheet formulas |
| Wolf-Table | Spreadsheet UI |
| React Table / AG Grid | Data grid views |
| React Beautiful DnD | Drag and drop |

---

### Backend Libraries

| Library | Purpose |
|---------|---------|
| FastAPI | HTTP API server |
| AutoGen or LangGraph | Agent orchestration |
| Ollama API | Local LLM access |
| ComfyUI API | Image generation |
| Pydantic | Data validation |
| SQLAlchemy | SQLite access |

---

### AI Models

| Model | Purpose | Size |
|-------|---------|------|
| Llama 3 13B | General text | ~14GB |
| Code Llama 13B | Code generation | ~14GB |
| Mistral 7B | Fast responses | ~8GB |
| SDXL 1.0 | Image generation | ~10GB |

---

### DevOps Tools

| Tool | Purpose |
|------|---------|
| GitHub Actions | CI/CD |
| Ruff, Black, isort | Python linting/formatting |
| ESLint, Prettier | TypeScript linting/formatting |
| pytest | Python testing |
| vitest | TypeScript testing |
| n8n (optional) | Workflow automation |

---

# 32. Consolidated Glossary {#32-consolidated-glossary}

**Alphabetical list of all technical terms defined in this document.**

| Term | Definition |
|------|------------|
| **Agent** | An AI model configured for a specific role with the ability to take actions |
| **API** | Application Programming Interface - how programs communicate with each other |
| **AutoGen** | Microsoft's multi-agent conversation framework |
| **Block-Based Editor** | Editor where content is made of stackable blocks instead of continuous text |
| **Chromium** | Open-source browser engine that Chrome is built on |
| **ComfyUI** | Node-based visual tool for Stable Diffusion image generation |
| **CRDT** | Conflict-free Replicated Data Type - enables automatic merge of concurrent edits |
| **Desktop Shell** | Program that wraps web code to run as a native desktop application |
| **Electron** | Popular desktop shell that bundles Chromium and Node.js |
| **GGUF** | File format for quantized AI models |
| **HyperFormula** | Open-source spreadsheet formula engine |
| **IPC** | Inter-Process Communication - how different parts of an app talk to each other |
| **LangGraph** | LangChain's graph-based agent orchestration framework |
| **Lead/Worker Pattern** | Smart model plans, simpler models execute |
| **LLM** | Large Language Model - AI trained on text to understand and generate language |
| **Local-First** | Architecture where data lives primarily on user's device |
| **Monorepo** | Single repository containing multiple related projects |
| **OAuth2** | Standard protocol for secure third-party authorization |
| **Ollama** | Easy-to-use local LLM runner |
| **Orchestrator** | Code that coordinates multiple AI models to work together |
| **Parameters** | The "knobs" inside an AI model (more = smarter but heavier) |
| **Quantization** | Shrinking AI models to use less memory |
| **REST API** | Common style for web APIs using HTTP methods |
| **SDXL** | Stable Diffusion XL - high-quality image generation model |
| **Sidecar File** | Small metadata file that accompanies a main file |
| **SQLite** | Lightweight database contained in a single file |
| **Tauri** | Lightweight desktop shell using Rust and system webview |
| **Tiptap** | Extensible rich text editor framework |
| **VRAM** | Video RAM - memory on graphics card where AI models run |
| **WebSocket** | Protocol for real-time, two-way communication |
| **Yjs** | Popular JavaScript CRDT library |

---

# 33. Open Questions and Next Steps {#33-open-questions-and-next-steps}

**Things the research doesn't fully answer that need further investigation.**

---

### Unresolved Questions

| Question | Why It Matters | Suggested Action |
|----------|---------------|------------------|
| **Exact Tauri version?** | v1 vs v2 have API differences | Check latest stable, test early |
| **Python bundling strategy?** | How to package Python with Tauri | Research PyInstaller + Tauri sidecar |
| **Model download UX?** | How do users get 10GB+ models? | Design in-app download + progress UI |
| **License audit** | Some libraries have complex licenses | Full audit before production |
| **Performance benchmarks** | Real numbers on target hardware | Build prototype, measure |

---

### Research Gaps

The documents **don't cover**:
- Mobile versions (iOS/Android)
- Web version (browser-only)
- Enterprise features (SSO, audit logs)
- Monetization strategy
- Analytics/telemetry approach
- Accessibility (a11y) requirements

---

### Immediate Next Steps

1. **Set up monorepo** with Tauri + React + Python structure
2. **Validate Tauri** on Windows, Mac, Linux
3. **Prototype IPC** between React and Python
4. **Test Ollama** integration
5. **Build health check** command

---

# 34. Sources Referenced {#34-sources-referenced}

This document consolidates research from the following source documents:

1. **Handshake_Project.pdf** (9 pages)
   - Core specification: multi-model orchestration, UI frameworks, Google API integration, ComfyUI, architecture overview

2. **Model_Strategy_and_Tooling_Guide.pdf** (4 pages)
   - AI assistant usage strategy, Codex vs GPT-4/Claude roles, n8n evaluation

3. **Reference_App_Deep_Dive_Local-First_Open_Workspace_Tools.pdf** (7 pages)
   - Technical analysis of AppFlowy, AFFiNE, Anytype, Logseq, Obsidian, Joplin

4. **Tauri_Electron_Decision.pdf** (4 pages)
   - Framework comparison, consensus from multiple AI advisors recommending Tauri

5. **Project_Health_Hygiene_Guide.pdf** (7 pages)
   - Codebase standards, testing, CI/CD, logging, AI-friendly practices

6. **Development_Roadmap_Draft.pdf** (7 pages)
   - Phase planning, implementation order, testing strategy, deployment

7. **Notion_vs_Milanote_vs_Excel_Feature_Comparison.pdf** (4 pages)
   - Target app analysis, orchestration framework comparison, local model recommendations

---

## Document End

**Total estimated read time:** ~90 minutes for complete document

**For quick reference:**
- [Executive Summary](#19-executive-summary) - 5 min overview
- [Technology Stack Summary](#31-technology-stack-summary) - Quick reference
- [Development Roadmap](#29-development-roadmap) - What to build when

---

*This document was compiled on November 29, 2025 from 7 research documents totaling ~42 pages.*
